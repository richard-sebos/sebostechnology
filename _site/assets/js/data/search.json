[
  
  {
    "title": "üîê Restricting SSH Access with a Limited User Account",
    "url": "/posts/Restricted-Access/",
    "categories": "Linux Security, SSH Hardening, System Administration, Server Security, User Management",
    "tags": "SSH security, restricted SSH user, rbash Linux, secure SSH login, Linux sysadmin tips, SSH hardening practices, limiting SSH access, two-account SSH model, secure Linux configuration, SSH restricted shell",
    "date": "2025-05-25 06:11:00 -0600",
    





    
    "snippet": "Securing SSH access is one of the most important steps a Linux administrator can take to harden a system. SSH is used daily by sysadmins, but if not configured carefully, it can expose powerful adm...",
    "content": "Securing SSH access is one of the most important steps a Linux administrator can take to harden a system. SSH is used daily by sysadmins, but if not configured carefully, it can expose powerful admin accounts to unnecessary risk‚Äîespecially when used from remote laptops or devices that might be lost or compromised.In this article, we explore a simple but powerful concept: using a restricted user for SSH logins and switching to your administrative account only after establishing a secure connection. This practice adds another layer of protection to your systems and can help reduce your attack surface significantly.üìö Table of Contents  Why Use Two User Accounts for SSH?  Setting Up a Restricted User with rbash  Restricting Executable Commands  Is This Overkill or Just Smart Security?  Final Thoughtsüßë‚Äçüíª Why Use Two User Accounts for SSH?I recently started configuring my Linux systems with two separate user accounts:  A restricted account named richard  An admin account named admin_richardThe richard account is limited to its home directory and a minimal set of commands. It can log in via SSH, but has no admin privileges. On the other hand, admin_richard has full sudo rights‚Äîbut is explicitly blocked from logging in over SSH.This model protects the administrative account from direct remote access and makes it much harder for an attacker to gain privileged access, even if an SSH key or laptop is compromised.Let‚Äôs walk through how to set it up.üõ°Ô∏è Setting Up a Restricted User with rbashMany Linux distributions include rbash (restricted Bash), which limits what a user can do with their shell.Step 1: Check for rbashwhich rbashIf it‚Äôs not available, create a symlink to Bash:which bash     # typically /usr/bin/bashsudo ln -s /usr/bin/bash /usr/bin/rbashEven though it points to the same binary, Linux enforces restricted behavior when invoked as rbash.Step 2: Set rbash as the User Shellsudo usermod -s /usr/bin/rbash richardNow, when richard logs in, their shell will be restricted. They won‚Äôt be able to change directories out of their home, use cd, set environment variables, or execute arbitrary commands.üß∞ Restricting Executable CommandsNext, let‚Äôs limit which commands richard can use by creating a custom bin directory.Step 1: Create and Lock Down a Custom Command Directorysudo mkdir /home/richard/.binsudo chown root:root /home/richard/.binThis ensures richard can‚Äôt add new commands.Step 2: Symlink Only Safe Commandssudo ln -s /bin/ls /home/richard/.bin/lssudo ln -s /bin/su /home/richard/.bin/susudo ln -s /bin/clear /home/richard/.bin/clearThese links expose only the commands you choose.Step 3: Set a Safe PATH and File PermissionsEdit .bashrc:sudo nano /home/richard/.bashrcAdd:export PATH=$HOME/.binumask 077Then lock down the file:sudo chown root:richard /home/richard/.bashrcThis setup ensures that even if someone uploads a malicious file via SCP, it won‚Äôt be executable‚Äîand richard won‚Äôt have access to anything outside .bin.ü§î Is This Overkill or Just Smart Security?Security often feels like overkill‚Äîuntil it‚Äôs not.By creating this restricted user workflow, you‚Äôre building another layer of defense. Combined with tools like SSH key authentication, Fail2Ban, and 2FA, this approach:  Limits exposure of your admin account  Slows down attackers  Encourages better compartmentalization of privilegesCybersecurity isn‚Äôt about one perfect solution‚Äîit‚Äôs about stacking defenses so that even if one layer is breached, others remain intact.‚úÖ Final ThoughtsSetting up a restricted SSH user may feel like extra work up front, but it pays dividends in security. You minimize the risk of exposing your admin credentials and give yourself time to respond in the event of a breach attempt.In the next article, I‚Äôll show how to block admin users from logging in via SSH altogether, making your privileged accounts even safer.Need Linux expertise? I help businesses streamline servers, secure infrastructure, and automate workflows. Whether you‚Äôre troubleshooting, optimizing, or building from scratch‚ÄîI‚Äôve got you covered.üì¨ Drop a comment or email me to collaborate. For more tutorials, tools, and insights, visit sebostechnology.com."
  },
  
  {
    "title": "üõ°Ô∏è How Secure Are Your Linux Files? Access Control Demystified",
    "url": "/posts/DAC-ACL-MAC/",
    "categories": "Linux access control, Linux security, system permissions, Linux auditing",
    "tags": "DAC, ACLs, MAC, AppArmor, SELinux, Auditd",
    "date": "2025-05-21 06:11:00 -0600",
    





    
    "snippet": "IntroductionAt a high level, Linux file permissions seem simple. You use ls -l to view them, and tools like chmod and chown to change who can read, write, or execute a file. This basic model‚Äîknown ...",
    "content": "IntroductionAt a high level, Linux file permissions seem simple. You use ls -l to view them, and tools like chmod and chown to change who can read, write, or execute a file. This basic model‚Äîknown as Discretionary Access Control (DAC)‚Äîis where most users start.But Linux security goes much deeper.Beyond traditional permissions, there are advanced access control mechanisms designed for more granular and robust security. In this guide, we explore three key models:  Discretionary Access Control (DAC)  Access Control Lists (ACLs)  Mandatory Access Control (MAC)We‚Äôll also examine the tools that help implement and monitor these models‚ÄîAppArmor, SELinux, and Auditd‚Äîto give you a high-level understanding of how they work together to secure your system.Table of Contents  Introduction  Discretionary Access Control (DAC)  Access Control Lists (ACLs)      Mandatory Access Control (MAC)          AppArmor      SELinux        Auditd: Monitoring Access Controls  Comparison Table  Conclusion1. Discretionary Access Control (DAC)OverviewDAC is the traditional Unix/Linux permission model, where file owners determine access rights.Key Features  Ownership: Each file/directory has an owner and group.  Permissions: Read (r), write (w), and execute (x) permissions for owner, group, and others.Examplels -l file.txt-rw-r--r-- 1 alice users 1024 May 21 10:00 file.txtIn this example:  Alice: Read and write permissions.  Users group: Read permission.  Others: Read permission.Pros and Cons  ‚úÖ Pros: Simple and straightforward.  ‚ùå Cons: Limited granularity; potential for misconfigurations.2. Access Control Lists (ACLs)OverviewACLs provide more granular permissions beyond the traditional owner/group/others model.Key Features  Fine-Grained Control: Assign specific permissions to individual users or groups.  Flexibility: Ideal for collaborative environments.Examplesetfacl -m u:bob:rw file.txtgetfacl file.txtThis grants read and write permissions to user Bob on file.txt.Pros and Cons  ‚úÖ Pros: Enhanced flexibility; precise control.  ‚ùå Cons: Can become complex to manage.3. Mandatory Access Control (MAC)OverviewMAC enforces system-wide policies that users cannot override, providing robust security.Key Features  System-Enforced Policies: Access decisions are based on predefined rules.  Enhanced Security: Limits the potential impact of compromised accounts or applications.AppArmorAppArmor uses path-based profiles to restrict program capabilities.Example Profile/usr/sbin/nginx {  /var/www/** r,  /etc/nginx/** r,  /etc/shadow r,}This profile restricts nginx to read-only access on specified directories.SELinuxSELinux employs label-based policies, assigning security contexts to files and processes.Examplels -Z /var/www/html/index.html-rw-r--r--. root root unconfined_u:object_r:httpd_sys_content_t:s0 index.htmlThe security context ensures only authorized processes can access the file.AppArmor vs. SELinux            Feature      AppArmor      SELinux                  Policy Type      Path-based      Label-based              Ease of Use      Easier to configure      More complex, granular control              Default in      Ubuntu, SUSE      RHEL, Fedora, CentOS              Configuration      /etc/apparmor.d/      /etc/selinux/, semanage tools      Auditd: Monitoring Access ControlsOverviewAuditd is the Linux auditing system, logging access attempts and policy violations.Key Features  Comprehensive Logging: Tracks access events, denials, and policy breaches.  Integration: Works seamlessly with AppArmor and SELinux.Example LogsAppArmor Denialaudit[1234]: apparmor=\"DENIED\" operation=\"open\" profile=\"/usr/sbin/nginx\" name=\"/etc/shadow\"SELinux Denialtype=AVC msg=audit(1623046567.583:107): avc:  denied  { read } for  pid=1327 comm=\"nginx\" name=\"shadow\"Setting Audit RulesTo monitor access to /etc/passwd:auditctl -w /etc/passwd -p wa -k passwd_watchRetrieve logs with:ausearch -k passwd_watchComparison Table            Feature      DAC      ACLs      MAC (AppArmor/SELinux)                  Control Level      User-defined      User-defined with exceptions      System-enforced              Granularity      Basic      Fine-grained      Very fine-grained              User Modifiable      Yes      Yes      No              Complexity      Low      Medium      High              Audit Capabilities      Limited      Limited      Extensive (with Auditd)      ConclusionImplementing robust access controls is vital for Linux system security:  DAC: Suitable for simple permission models.  ACLs: Offer enhanced flexibility for complex environments.  MAC: Provide stringent, system-enforced security policies.  Auditd: Essential for monitoring and auditing access events.By understanding and appropriately applying these mechanisms, administrators can significantly enhance the security posture of their Linux systems."
  },
  
  {
    "title": "üõ°Ô∏è Complete Guide to Fail2Ban - Protect Your SSH Server from Brute Force Attacks",
    "url": "/posts/Fail2Ban/",
    "categories": "Linux Security, Server Hardening, SSH Security, Fail2Ban, DevOps Tutorials, Cybersecurity Best, Practices",
    "tags": "SSH Security, Fail2Ban Configuration, Prevent Brute Force Attacks, Linux Server Hardening, SSH 2FA, SSH Key Authentication, Secure SSH Login, Fail2Ban SSH Setup, Cybersecuritym Linux Security Best Practices",
    "date": "2025-05-18 06:11:00 -0600",
    





    
    "snippet": "üìù IntroductionSecuring your SSH access is a fundamental step in hardening any Linux server. While using SSH Key-Based Authentication and enabling Two-Factor Authentication (2FA) significantly stren...",
    "content": "üìù IntroductionSecuring your SSH access is a fundamental step in hardening any Linux server. While using SSH Key-Based Authentication and enabling Two-Factor Authentication (2FA) significantly strengthen login security, these methods alone don‚Äôt prevent brute force attacks.üìö Table of Contents  Introduction  Why Brute Force Attacks Are Still a Threat  What is Fail2Ban?      How to Set Up Fail2Ban for SSH          File 1: Global Defaults ‚Äì /etc/fail2ban/jail.local      File 2: SSH-Specific Settings ‚Äì /etc/fail2ban/jail.d/sshd.local        Testing Fail2Ban Protection      Conclusion and Next Steps  Brute force attempts can lead to:  üõ°Ô∏è Security Risks  üìà Performance and Resource Impacts  üì° Network Disruptions  üìö Compliance and Audit ViolationsEven with firewalls and IP restrictions using AllowUsers, compromised devices within your network can still launch brute force attacks. This is where Fail2Ban comes in.üö® Why Brute Force Attacks Are Still a ThreatYou might think, ‚ÄúI have SSH keys and 2FA enabled; isn‚Äôt that enough?‚Äù Unfortunately, the answer is no. While these measures prevent unauthorized access, they don‚Äôt stop repeated failed login attempts that can:  Consume system resources  Fill up logs, affecting audit clarity  Trigger compliance violations  Potentially exploit zero-day vulnerabilitiesFail2Ban actively prevents these attacks by blocking malicious IP addresses before they can become a bigger problem.üîé What is Fail2Ban?Fail2Ban is a powerful intrusion prevention system that monitors log files and reacts to suspicious activity, such as repeated failed authentication attempts. While we‚Äôre focusing on SSH in this guide, Fail2Ban also supports services like Apache, NGINX, and Sendmail.‚úÖ Common Fail2Ban Actions:  Apply temporary or permanent bans  Block IP addresses using firewall rules  Modify IP sets for advanced firewall management  Add routes to blackhole malicious traffic  Send email notifications to administrators  Execute custom scripts  Log and alert without banning (monitoring mode)Fail2Ban‚Äôs actions are highly customizable, allowing you to tailor protections based on your specific security requirements.üõ†Ô∏è How to Set Up Fail2Ban for SSHFail2Ban configuration primarily revolves around its jail files. The jail.conf file contains default settings, but it‚Äôs best practice to use jail.local and service-specific .local files for custom configurations.üì¶ File 1: Global Defaults ‚Äì /etc/fail2ban/jail.local[DEFAULT]# ========================# Fail2Ban Global Settings# ========================# IP addresses or networks to ignore (never ban). # Add your trusted subnets and management IPs here.ignoreip = 127.0.0.1/8 ::1 192.168.178.0/24  # Duration for which an IP is banned after exceeding maxretry attempts.# Format examples: s (seconds), m (minutes), h (hours), d (days)bantime = 1h  # Time window within which maxretry failures must occur to trigger a ban.findtime = 10m  # Number of failed attempts before an IP gets banned.maxretry = 3  # Backend used to read logs. # 'systemd' is recommended for modern systems using journalctl.backend = systemd  # Control DNS usage in logs and actions. # Options: yes | warn | no # 'warn' tries to resolve but continues if it fails.usedns = warn  # ===========================# Ban Action Configuration# ===========================# Action to take when a rule is triggered.# %(action_mwl)s: #  - Ban the IP#  - Send an email with whois info and relevant logsaction = %(action_mwl)süì¶ File 2: SSH-Specific Settings ‚Äì /etc/fail2ban/jail.d/sshd.local[sshd]# ==========================# SSH Jail Configuration# ==========================# Enable the SSH jail to monitor and protect against brute-force attacks.enabled = true  # Port Fail2Ban should monitor for SSH connections.# If you run SSH on a custom port, replace 'ssh' with the actual port number (e.g., 2222).port = ssh  # Filter definition to use. # 'sshd' refers to the default filter that matches common SSH authentication failures.filter = sshd  # Log file location. # '%(sshd_log)s' uses the default value set by the system, typically /var/log/auth.log or journalctl.logpath = %(sshd_log)s  # Backend for reading logs.# 'systemd' is recommended if your system uses journalctl for logging.backend = systemd  # ==========================# SSH-Specific Overrides# ==========================# Time window to evaluate failed login attempts.# If 'maxretry' failures occur within this time, the IP will be banned.findtime = 5m  # Number of failed attempts allowed before triggering a ban.maxretry = 4  üí° Tip: You can apply the same structure for other services like nginx.local or apache.local under /etc/fail2ban/jail.d/ to keep configurations clean and organized.Fail2Ban processes configuration files in the following order:  /etc/fail2ban/jail.conf (Defaults ‚Äì Do Not Modify)  /etc/fail2ban/jail.local (Global Overrides)  All .local files in /etc/fail2ban/jail.d/ (Service-Specific Settings)üß© Testing Fail2Ban ProtectionAfter setting up Fail2Ban, it‚Äôs crucial to validate that it correctly detects and mitigates brute force attempts.Here‚Äôs a simple script to simulate failed SSH logins:#!/bin/bash# ===============================# Fail2Ban SSH Ban Trigger Script# ===============================# This script intentionally generates failed SSH login attempts # to test if Fail2Ban properly detects and blocks brute-force attacks.# -------------------------------# Target Configuration# -------------------------------TARGET_HOST=\"192.168.178.13\"    # IP address or hostname of the target server to testFAKE_USER=\"invaliduser\"         # Non-existent username to force authentication failureATTEMPTS=5                      # Number of failed attempts (match or exceed Fail2Ban 'maxretry' setting)echo \"Triggering Fail2Ban by attempting to SSH as user '$FAKE_USER' to $TARGET_HOST\"# -------------------------------# Brute-Force Simulation Loop# -------------------------------for i in $(seq 1 $ATTEMPTS); do    echo \"Attempt $i...\"    ssh \\        -o PreferredAuthentications=password \\    # Force password authentication (skip public key)        -o PubkeyAuthentication=no \\              # Disable public key authentication entirely        -o StrictHostKeyChecking=no \\              # Avoid host key verification prompts        -o ConnectTimeout=5 \\                      # Limit each connection attempt to 5 seconds        \"$FAKE_USER@$TARGET_HOST\" exit             # Attempt to connect and immediately exit if successful (won't be)done# -------------------------------# Final Status Message# -------------------------------echo \"Done. Check Fail2Ban status on the target server using:\"echo \"  sudo fail2ban-client status sshd\"Check the Fail2Ban status:sudo fail2ban-client status sshdExample output:Status for the jail: sshd|- Filter|  |- Currently failed:\t1|  |- Total failed:\t5|  `- Journal matches:\t_SYSTEMD_UNIT=sshd.service + _COMM=sshd`- Actions   |- Currently banned:\t1   |- Total banned:\t1   `- Banned IP list:\t192.168.178.11To manually unblock an IP before the ban expires:sudo fail2ban-client set sshd unbanip 192.168.178.11‚úÖ Conclusion and Next StepsSSH is a critical protocol for remote Linux server management but also remains a primary target for attackers. By combining Fail2Ban with 2FA, SSH Key Authentication, and IP Restrictions, you create a multi-layered defense that significantly reduces the risk of compromise.üîê New to SSH Security? Start Here!Kick off your SSH hardening journey with Your First Steps to a Hardened SSH Server. Learn why securing sshd_config is critical and how to avoid common security pitfalls.Drop a comment or reach out‚Äîwe‚Äôre here to help. For more content like this, tools, and walkthroughs, visit my site at Sebos Technology."
  },
  
  {
    "title": "üõ°Ô∏è Securing ROS2 Robotic Projects with Auditd- A Practical Guide",
    "url": "/posts/ROS2-Auditd/",
    "categories": "Robotics, ROS2 Install Series, Security, Robotics Security, Linux System Hardening, ROS2 Development, Cybersecurity Best Practices, Open-Source Security Tools, Industrial IoT Security, DevSecOps for Robotics",
    "tags": "ROS2 Security, Auditd Linux, Robotics Cybersecurity, Secure ROS Communications, Linux Auditing Tools, ROS2 Hardening, AppArmor and Auditd, Network Monitoring in Robotics, Robot Security Frameworks, SROS2 Configuration, ROS2 Development Security, Linux System Monitoring, ROS2 Build Security, Suricata IDS, Auditd Rules Examples, ROS2",
    "date": "2025-05-17 06:11:00 -0600",
    





    
    "snippet": "In our ongoing journey to secure ROS2-based robotic projects, we‚Äôve already implemented several foundational security measures. We started with a fresh installation, configured SROS2 for secure ROS...",
    "content": "In our ongoing journey to secure ROS2-based robotic projects, we‚Äôve already implemented several foundational security measures. We started with a fresh installation, configured SROS2 for secure ROS communications, enabled AppArmor and Auditd, set up a firewall, and deployed Suricata for network monitoring.While AppArmor, Auditd, and Suricata all help define rules and profiles to protect and monitor a system, this article focuses on diving deeper into Auditd‚Äîa powerful auditing framework that doesn‚Äôt just monitor but provides invaluable insights into system activities.üìö Table of Contents  What Is Auditd?  Why Use Auditd in Robotics Projects?  Auditd and Robots: Real-World Use Cases  How Auditd Works: Practical Examples  Monitoring ROS2 Code and Development Environments  Reviewing Audit Logs Efficiently  Conclusionüîç What Is Auditd?auditd is a Linux security auditing daemon that monitors and records critical system events, including user activities, process executions, file system accesses, and some network-related actions.  ‚ö†Ô∏è Important: Auditd does not prevent events‚Äîit records and reports them for later review.In a robotics context, where systems comprise various interconnected devices‚Äîfrom cameras and sensors to motor controllers‚Äîit‚Äôs challenging to define strict access controls without disrupting functionality. Auditd helps bridge this gap by providing detailed reports on interactions across device boundaries, allowing teams to make informed security decisions.üìå Key Capabilities of Auditd:  Monitors file system actions (read, write, execute, attribute changes).  Tracks user activities (logins, privilege escalations, command executions).  Captures process actions (process creation, exec calls).  Observes network-related system calls (e.g., connect(), bind()), but is not a network traffic analysis tool like Suricata.  Maintains detailed logs in /var/log/audit/audit.log.  Integrates with ausearch, aureport, and auditctl for efficient analysis and rule management.ü§ñ Why Use Auditd in Robotics Projects?Security debates often center around two approaches:  Deny Access: Safer but may hinder development and business operations.  Monitor and Audit: Allows freedom during development but requires vigilance.Auditd strikes a balance by enabling passive monitoring during development. Rules can be created to monitor who and what accesses sensitive areas. Then, as the project moves into production, these passive rules can transition into enforced policies using tools like AppArmor, SELinux, or Suricata.This approach is particularly useful for robotics, where interconnected devices and unpredictable interactions make strict deny policies impractical during early stages.ü§ñ Auditd and Robots: Real-World Use CasesIn robotics, multiple subsystems interact seamlessly‚Äîcameras send data to AI processors, controllers issue motor commands, and various sensors provide environmental feedback. But how do you ensure those communications are legitimate?Did a wheel control signal originate from the robot‚Äôs controller or a compromised driver?By integrating auditd during the development phase, you can monitor process events and establish behavioral baselines. These baselines help you define acceptable interactions, which can later be enforced to block anything outside of expected behavior.‚öôÔ∏è How Auditd Works: Practical ExamplesLet‚Äôs create a simple auditd rule to monitor when colcon, the ROS2 build tool, is executed.-w /usr/bin/colcon -p x -k colcon_execüìñ Rule Breakdown            Parameter      Meaning                  -w      Watch the specified file or directory (/usr/bin/colcon).              -p x      Monitor execute permissions (triggers when the file is run).              -k colcon_exec      Assigns the log entry a custom key colcon_exec for easier searching.      üìù Monitoring ROS2 Code and Development EnvironmentsAuditd can also help secure your codebase. Here‚Äôs how you can monitor attempts to access ROS2 project files by unauthorized users:########################################################### üöÄ ROS Environment and Security Audit Rules for Auditd ############################################################ -----------------------------------------------------------------------------# üîπ Monitor execution of critical ROS-related commands to detect build and #    dependency management activity.# ------------------------------------------------------------------------------w /usr/bin/colcon       -p x   -k colcon_exec      # Watch execution of 'colcon' (ROS workspace build tool)-w /usr/bin/rosdep       -p x   -k rosdep_exec      # Watch execution of 'rosdep' (ROS dependency manager)# -----------------------------------------------------------------------------# üîπ Monitor changes and execution within the main ROS installation and #    system-wide ROS configuration directories.# ------------------------------------------------------------------------------w /opt/ros               -p x    -k ros_exec       # Detect execution of binaries/scripts from the core ROS installation-w /etc/ros2              -p wa   -k ros_conf       # Monitor changes (write/attribute) to ROS2 configuration files# -----------------------------------------------------------------------------# üîπ Monitor changes to the user environment configuration, specifically for #    modifications that may alter ROS environment variables.# ------------------------------------------------------------------------------w /home/rosbot/.bashrc   -p wa   -k ros_env        # Monitor user '.bashrc' for changes that might affect environment variables# -----------------------------------------------------------------------------# üîπ Monitor ROS workspace activities to detect execution of workspace scripts #    and modifications to source code before builds.# ------------------------------------------------------------------------------w /home/rosbot/ros2_ws        -p x    -k ros_ws_exec  # Detect execution of scripts/binaries in the workspace root-w /home/rosbot/ros2_ws/src    -p wa   -k ros_src      # Monitor changes to source code files (write/attribute changes)########################################################### ‚úÖ End of Audit Rules##########################################################  üõ°Ô∏è Note: Auditd doesn‚Äôt block actions‚Äîit logs them for your review.üìÇ Reviewing Audit Logs EfficientlyTo quickly search audit logs for specific keys:ausearch -k ros_code_access -k colcon_execFor a cleaner, more organized report, use the following script:üìÑ Script: audit_report.sh(See full script here)This script loops through auditd keys  and formats them for easy reading. It‚Äôs a great tool for ongoing monitoring or post-incident reviews.üìå Usage Examples:      Group by Audit Key:    ./audit_ros_tracker.sh      It will create a file /var/log/audit_ros_events.csv\"2025-05-17 12:08:53\",\"richard\",\" \"/usr/bin/python3\" \"/usr/bin/colcon\" \"--help\"\",colcon_exec\"2025-05-17 12:08:53\",\"richard\",\" \"/usr/bin/python3\" \"/usr/bin/rosdep\" \"--help\"\",rosdep_exec\"2025-05-17 12:08:53\",\"richard\",\" \"/bin/bash\" \"/opt/ros/trigger.sh\"\",ros_exec\"2025-05-17 12:08:53\",\"richard\",\" \"/bin/bash\" \"/home/rosbot/ros2_ws/trigger_ws.sh\"\",ros_ws_exeThis script can be run manually or scheduled via systemd or cron for regular reporting.‚úÖ ConclusionIn complex systems like robotics, security isn‚Äôt just about locking things down‚Äîit‚Äôs about understanding how your systems behave and identifying unusual activity before it becomes a problem.By integrating Auditd into your ROS2 robotic projects early in the development lifecycle, you empower your teams to gather critical security insights without disrupting innovation. And while this article focused on robotics, the same principles and scripts apply seamlessly to general Linux environments and applications.Looking to learn more about ROS2 security, SROS2 node permissions, or robotic system hardening? Bookmark this series and follow along as we secure each layer of our Linux-based robotic system.For more content like this, tools, and walkthroughs, visit my site at Sebos Technology."
  },
  
  {
    "title": "üîê How to Set Up Multi-Factor Authentication (MFA) on Ubuntu for SSH",
    "url": "/posts/MFA/",
    "categories": "Github, Bash, Automation",
    "tags": "Automation, BashScripting, DevOps, Git",
    "date": "2025-05-10 11:31:00 -0600",
    





    
    "snippet": "How to Set Up Multi-Factor Authentication (MFA) on Ubuntu for SSHSecuring your Linux servers is more critical than ever, and one of the simplest ways to dramatically improve login security is by en...",
    "content": "How to Set Up Multi-Factor Authentication (MFA) on Ubuntu for SSHSecuring your Linux servers is more critical than ever, and one of the simplest ways to dramatically improve login security is by enabling Multi-Factor Authentication (MFA). This guide walks you through setting up MFA on a fresh Ubuntu 24.04 installation using Google Authenticator, ensuring your SSH access is protected against unauthorized access.üìö Table of Contents  What is Multi-Factor Authentication (MFA)?      Why Use MFA for SSH?          A Brief History of Authentication      Why MFA Makes Sense for SSH Today            Setting Up MFA for SSH on Ubuntu          Step 1: Install Google Authenticator      Step 2: Link User Accounts to the Authenticator App      Step 3: Configure SSH to Use MFA        Final ThoughtsWhat is Multi-Factor Authentication (MFA)?Multi-Factor Authentication (MFA) adds a critical layer of security to the login process by requiring users to provide an additional verification factor beyond just a password. Typically, this involves a Time-based One-Time Password (TOTP) generated by an app such as Google Authenticator or Microsoft Authenticator on a personal device.The server and the authenticator app share a secret key and generate synchronized time-based tokens. When a user attempts to log in, the system prompts for a token generated by the authenticator app, ensuring that only someone with both the correct password (or SSH key) and the physical device can gain access.My personal journey with MFA began over 25 years ago, when dedicated hardware tokens were required to securely access corporate VPNs from home. Today, MFA is an essential security measure used across industries‚Äîfrom protecting work accounts to securing personal banking applications.Why Use MFA for SSH?A Brief History of AuthenticationIn the early days of computing, password-based authentication was considered sufficient. This was mainly because the computing power required to crack passwords through brute-force or dictionary attacks was expensive and not widely available.As technology progressed and computing resources became more powerful and accessible, these attacks became both practical and increasingly common. This evolution exposed the inherent weaknesses of password-only security.To address this, token-based authentication systems emerged in the mid-1980s. These early systems required users to possess a physical device that generated time-sensitive codes‚Äîan early form of what we now recognize as Multi-Factor Authentication (MFA).Why MFA Makes Sense for SSH TodayModern security best practices strongly recommend implementing MFA to protect critical systems like Linux servers accessed over SSH. Fortunately, integrating MFA with SSH is both simple and highly effective.  ‚úÖ Flexible Authentication Options: Combine MFA with either password-based authentication, SSH key-based authentication, or both.  ‚úÖ User-Specific Policies: SSH is versatile enough to apply different authentication methods for different users based on their needs.  ‚úÖ Stronger Security: Even if a password or SSH key is compromised, MFA ensures an attacker cannot log in without the additional verification factor.Implementing MFA on SSH should be considered a baseline security standard, not just an optional enhancement.Setting Up MFA for SSH on UbuntuThis guide assumes you are starting with a fresh Ubuntu 24.04 installation and have already configured SSH key-based authentication.Step 1: Install Google AuthenticatorFirst, ensure your system is up to date and install the Google Authenticator PAM module.# Update and upgrade the systemsudo apt update &amp;&amp; sudo apt upgrade -y# Install Google Authenticator PAM modulesudo apt install libpam-google-authenticator -yStep 2: Link User Accounts to the Authenticator AppNext, log in as the user account you want to secure with MFA and run the Google Authenticator setup:google-authenticatorFollow the prompts to complete the setup:  A QR code will be displayed. Scan it using an authenticator app (Google Authenticator, Microsoft Authenticator, etc.) on your mobile device.  The app will start generating time-based one-time passwords (TOTPs).  Backup codes will also be provided. Store them securely in case you lose access to your device.Step 3: Configure SSH to Use MFANow, update your SSH and PAM configurations to enforce MFA.1. Edit PAM ConfigurationOpen the PAM configuration for SSH:sudo nano /etc/pam.d/sshdAdd the following line to enable Google Authenticator:# Google Authenticator MFAauth required pam_google_authenticator.soTo disable password-based authentication through PAM, comment out or remove the following line:# @include common-auth  Note:PAM (Pluggable Authentication Modules) is a flexible framework used on Linux systems to integrate various authentication methods. It allows services like SSH to support additional authentication mechanisms, including MFA.2. Edit SSH Daemon ConfigurationOpen the SSH daemon configuration file:sudo nano /etc/ssh/sshd_configUpdate the following parameters to enforce MFA with public key authentication:# Require public key and MFAAuthenticationMethods publickey,keyboard-interactive# Enable public key authenticationPubkeyAuthentication yes# Enable PAM for MFAUsePAM yesChallengeResponseAuthentication yes# Disable password-based authenticationPasswordAuthentication noFinally, restart the SSH service to apply the changes:sudo systemctl restart ssh  Tip:Keep your current SSH session open and test the new configuration in a separate terminal. This ensures that if there‚Äôs a misconfiguration, you won‚Äôt be locked out of the server.Final ThoughtsEnabling MFA for SSH is one of the easiest and most effective ways to harden your server against unauthorized access. With minimal configuration changes, you can significantly improve your security posture by requiring both something the user knows (a password or SSH key) and something the user has (the MFA device).In today‚Äôs threat landscape, enabling MFA is no longer just a best practice‚Äîit‚Äôs a necessity.üîê New to SSH Security? Start Here!Kick off your SSH hardening journey with Your First Steps to a Hardened SSH Server. Learn why securing sshd_config is critical and how to avoid common security pitfalls.Drop a comment or reach out‚Äîwe‚Äôre here to help.  For more content like this, tools, and walkthroughs, visit my site at Sebos Technology."
  },
  
  {
    "title": "üîê Securing ROS 2 Robots - Network Intrusion Detection with Suricata",
    "url": "/posts/Suricata/",
    "categories": "Robotics, ROS2 Install Series, Security",
    "tags": "ros2, robotics, cybersecurity, linux",
    "date": "2025-05-07 06:11:00 -0600",
    





    
    "snippet": "As robots become more deeply integrated into logistics, healthcare, research, and everyday life, they also become more attractive targets for cyber threats. Network security in robotics is no longe...",
    "content": "As robots become more deeply integrated into logistics, healthcare, research, and everyday life, they also become more attractive targets for cyber threats. Network security in robotics is no longer optional‚Äîespecially when those systems rely on distributed middleware like ROS 2 (Robot Operating System 2).In this article, part of our broader ROS 2 robot hardening series, we focus on using Suricata, a high-performance open-source intrusion detection system (IDS), to monitor and alert on suspicious network traffic in real time.  üîé This guide covers installing Suricata on an Ubuntu 24.04-based ROS 2 robot, integrating it into an automated main.sh install script, and validating the setup.üìö Table of Contents  Why Use Suricata for Robotic Network Security?      Suricata Deployment Modes for Robots          ‚úÖ Host-Based Monitoring      ‚úÖ Inline IPS Mode      ‚úÖ Passive Mode with SPAN/TAP            Installing Suricata via main.sh Integration          suricata_setup Function        Configuring Suricata for Your Network Interface  Testing Suricata with ICMP Traffic  Summary and Next StepsWhy Use Suricata for Robotic Network Security?Suricata provides deep packet inspection and real-time alerting for potentially malicious activity across:  Ethernet, IP, TCP/UDP, and application-layer protocols  ROS 2 DDS traffic, which typically operates over UDP ports 7400‚Äì7600  Common attack patterns, detected via customizable rule signaturesBy adding Suricata to a robotic system, you gain:  ‚úÖ Visibility into all traffic flows (including internal ROS 2 communication)  ‚úÖ Early warnings for unauthorized access or malformed packets  ‚úÖ A foundation for Zero Trust network security, even on public or mobile connectionsSuricata Deployment Modes for RobotsSuricata supports multiple deployment models depending on your network architecture:‚úÖ Host-Based MonitoringRuns directly on the robot and inspects packets via the system‚Äôs network interface.  üß∞ Best for: Isolated devices, portable robots, and standalone systems.‚úÖ Inline IPS ModeSits between network segments to actively block malicious traffic (intrusion prevention system mode).  üß∞ Best for: Robots behind dedicated gateways or firewalls.‚úÖ Passive Mode with SPAN/TAPConnects to a mirror or SPAN port on a switch to passively monitor all subnet traffic.  üß∞ Best for: Labs, testing environments, and security operations centers.Installing Suricata via main.sh IntegrationIn this series, we use a modular approach to secure ROS 2 systems. Suricata is installed via the suricata_setup function, called by the centralized main.sh installer. This makes security setup consistent and scriptable across all robot deployments.üîß suricata_setup Function#!/bin/bashsource ./common.shsuricata_setup() {    apt update &amp;&amp; apt upgrade    apt install -y suricata    # Add basic detection rules    echo 'alert udp any any -&gt; any 7400:7600 (msg:\"ROS2 DDS UDP Traffic Detected\"; sid:100001;)' | sudo tee -a /etc/suricata/rules/local.rules    echo 'alert icmp any any -&gt; any any (msg:\"ICMP test detected\"; sid:1000001; rev:1;)' | sudo tee -a /etc/suricata/rules/local.rules    # Ensure Suricata loads the custom rule file    echo 'include: local.rules' &gt;&gt; /etc/suricata/suricata.yaml    systemctl enable --now suricata}You can plug this function directly into your main.sh or call it as a modular step in a larger installation sequence.üîÑ Configuring Suricata for Your Network InterfaceSuricata needs to monitor the correct network interface. Identify yours:ip a | grep UPSample result:2: ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; ...Update /etc/suricata/suricata.yaml:af-packet:  - interface: ens3  # Replace 'ens3' with your active network interfaceUpdate the rule path configuration:default-rule-path: /etc/suricata/rulesrule-files:  - local.rulesMake sure there are no conflicting includes:# Comment out default includes if needed# include: some-other-rules.yamlRestart Suricata to apply the changes:sudo systemctl restart suricataüß™ Testing Suricata with ICMP TrafficYou can validate your setup with a basic ping test:Terminal 1:ping 8.8.8.8Terminal 2:sudo tail -f /var/log/suricata/fast.logExpected output:[**] [1:1000001:1] ICMP test detected [**] {ICMP} 10.0.2.15:8 -&gt; 8.8.8.8:0This confirms Suricata is actively monitoring and logging traffic based on your rules.üõ°Ô∏è Summary and Next StepsWith Suricata installed and configured, your robot now has:‚úÖ Real-time intrusion detection‚úÖ Visibility into ROS 2 DDS traffic‚úÖ A scalable security foundation for Zero Trust deploymentsYou‚Äôre now protected against common threats that might slip past firewalls or go undetected at the system level.  üîú Next in the series: We‚Äôll explore how to add reporting and alerting, so your robot can notify you when suspicious behavior occurs‚Äîfurther closing the loop on real-time defense.Looking to learn more about ROS2 security, SROS2 node permissions, or robotic system hardening? Bookmark this series and follow along as we secure each layer of our Linux-based robotic system.For more content like this, tools, and walkthroughs, visit my site at Sebos Technology."
  },
  
  {
    "title": "üîê Mastering SSH Key-Based Authentication",
    "url": "/posts/SSH-Key-Based/",
    "categories": "Github, Bash, Automation",
    "tags": "Automation, BashScripting, DevOps, Git",
    "date": "2025-05-03 10:11:00 -0600",
    





    
    "snippet": "Mastering SSH Key-Based Authentication: Secure Passwordless Login for Linux and WindowsWhen managing remote servers, SSH (Secure Shell) is the standard for encrypted and secure communication. Howev...",
    "content": "Mastering SSH Key-Based Authentication: Secure Passwordless Login for Linux and WindowsWhen managing remote servers, SSH (Secure Shell) is the standard for encrypted and secure communication. However, relying on passwords for SSH access leaves your systems vulnerable to:  Credential stuffing  Phishing attacks  Keylogging and malware  Man-in-the-middle (MitM) attacks  Password reuse threats  Brute-force dictionary attacksIf you‚Äôre looking for a safer, faster, and more reliable way to access your servers, SSH key-based authentication is the solution. In this guide, we‚Äôll show you how to set up passwordless SSH login for Linux and Windows (via PuTTY) and share tips to boost your SSH security posture.Why Use SSH Keys?SSH keys are a pair of cryptographic files‚Äîa private key and a public key‚Äîthat authenticate users without a password. This offers several advantages:  Enhanced security (especially with stronger keys like 4096-bit RSA or modern Ed25519)  Improved convenience (no passwords to remember or type repeatedly)  Resilience against common attack vectorsWhen you connect to a server, your client offers the public key. If the server recognizes it and verifies the corresponding private key, access is granted‚Äîno password required.Let‚Äôs walk through setting this up step-by-step.1. How to Generate SSH KeysOn Linux (OpenSSH)  Open your terminal.  Generate your SSH key pair:    ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"              -t rsa: Specifies the algorithm (RSA).      -b 4096: Sets key length for stronger encryption.      -C: Optional comment for identifying the key.        Press Enter to accept the default file location (~/.ssh/id_rsa).  Choose an optional passphrase for added protection.  Verify your keys:    ls -l ~/.ssh/id_rsa*        You‚Äôll see:          id_rsa (your private key ‚Äî keep it safe!)      id_rsa.pub (your public key ‚Äî upload this to servers)        Upload your public key to the server:    ssh-copy-id user@server_name        This adds the key to the server‚Äôs ~/.ssh/authorized_keys file.  On Windows (Using PuTTYgen)  Download PuTTY and open PuTTYgen.  Choose:          Key type: RSA      Number of bits: 4096        Click Generate and move your mouse to create entropy.  Save your:          Private key (id_rsa.ppk) securely (e.g., C:\\Users\\your_username\\ssh)      Public key by copying it from the PuTTYgen window        SSH into your server using a password, then:    echo \"your-copied-public-key\" &gt;&gt; ~/.ssh/authorized_keys      2. Configuring SSH ClientsLinux: ~/.ssh/configSimplify server access by creating an SSH config file:vi ~/.ssh/configAdd:Host your_server_alias    HostName your.server.com    User your_username    Port 22    IdentityFile ~/.ssh/id_rsaNow connect with:ssh your_server_aliasWindows: PuTTY  Open PuTTY.  In the Session tab:          Host Name: your server‚Äôs IP or hostname      Port: 22        Go to Connection &gt; SSH &gt; Auth and browse to your .ppk private key.  (Optional) Save the session for quick access later.3. Testing Your SSH Connection  Linux:     ssh your_server_alias        If configured correctly, you‚Äôll connect without a password prompt.    Windows (PuTTY):          Load your saved session and click Open.      4. More SSH Features to ExploreSSH does more than log you in‚Äîhere are a few powerful extras:Run remote commands:ssh user@server \"uptime\"Secure file transfers with SCP:  Upload:    scp file.txt user@server:/remote/path/        Download:    scp user@server:/remote/path/file.txt .      Create SSH tunnels (port forwarding):ssh -L 8080:example.com:80 user@serverUse an SSH agent (Linux):eval \"$(ssh-agent -s)\"ssh-add ~/.ssh/id_rsa5. Troubleshooting SSH Key Issues  ‚ÄúPermission denied‚Äù errors? Ensure correct file permissions:     chmod 700 ~/.ssh chmod 600 ~/.ssh/authorized_keys        PuTTY not connecting?          Make sure you‚Äôre using the .ppk format      Check PuTTY‚Äôs Event Log for specific errors      6. Bonus Security Tips for SSH HardeningWant enterprise-grade security? Consider:  Use Ed25519 keys instead of RSA:    ssh-keygen -t ed25519 -C \"your_email@example.com\"        Disable password authentication:Edit /etc/ssh/sshd_config:    PasswordAuthentication noChallengeResponseAuthentication no        Then:    sudo systemctl restart sshd        Restrict SSH access to specific users:    AllowUsers your_username        Install Fail2Ban or SSHGuard for intrusion detection and brute-force protection.Final ThoughtsSSH key authentication is an essential skill for anyone working with servers‚Äîfrom home labs to production environments. It significantly improves security, eliminates password fatigue, and opens the door to more advanced remote workflows.Take the time to set it up properly‚Äîyour systems (and your future self) will be better protected for it.Have questions or want to dive deeper into SSH agent forwarding, key rotation strategies, or enterprise hardening? Drop a comment or reach out‚Äîwe‚Äôre here to help."
  },
  
  {
    "title": "Securing ROS2 Nodes with SROS2",
    "url": "/posts/SROS2-Setup/",
    "categories": "Robotics, ROS2 Install Series, Security",
    "tags": "ros2, robotics, cybersecurity, linux",
    "date": "2025-04-30 04:00:00 -0600",
    





    
    "snippet": "In our previous post, we configured the firewall and granted $PROGRAMMER_LAPTOP_IP access to the robot via SSH. But controlling a robot involves more than just external connections‚Äîa robot is a com...",
    "content": "In our previous post, we configured the firewall and granted $PROGRAMMER_LAPTOP_IP access to the robot via SSH. But controlling a robot involves more than just external connections‚Äîa robot is a complex ecosystem of internal components, including a base controller, cameras, scanners, and movement modules. These components (ROS2 nodes) must communicate securely with each other.So, how do we secure internal communication between ROS2 nodes?üìö Table of Contents  Why Node-to-Node Security Matters  What Is SROS2?  SROS2 Setup Script Overview  Step-by-Step: sros2_setup() Bash Function          Install OpenSSL      Switch to ROS2 User      Initialize Keystore and Keys      Define Node Permissions      Generate Signed Permissions File        Security Context Summary  Next Steps: Intrusion Detection with SuricataWhy Node-to-Node Security MattersIn robotic systems powered by ROS2, nodes constantly exchange data‚Äîsome of it sensitive, such as movement commands or camera feeds. Without encryption and access control, this data is vulnerable to tampering or eavesdropping, especially in networked environments like factories or research labs.That‚Äôs where SROS2 comes in.What Is SROS2?SROS2 (Secure ROS2) extends ROS2 with security mechanisms built on the DDS-Security standard. It uses OpenSSL to encrypt node communication and restrict access based on signed permissions.When configured, SROS2 creates a keystore directory containing:  A Certificate Authority (CA)  Each node‚Äôs:          Public certificate      Private key      Signed permissions file      This setup ensures only authorized nodes can participate in the ROS2 ecosystem.SROS2 Setup Script OverviewTo streamline setup, we created a script: sros2_setup(). This Bash function initializes a keystore and configures a node (base_controler) for secure communication.It assumes:  A working ROS2 installation  ROS2 user and workspace set in an external script (common.sh)Step-by-Step: sros2_setup() Bash Function1. Install OpenSSLapt install -y opensslüîß Ensures the required cryptographic tools are available for SROS2 key generation.2. Switch to ROS2 Usersudo -u \"$ROS_USER\" bash -c '...'üèÉ Executes setup in the context of the non-root ROS2 user.Inside the block:a. Source the ROS2 Environmentsource /opt/ros/$ROS_DISTRO/setup.bashüîó Loads the ROS2 environment variables.b. Create the Keystore Directorymkdir -p $ROS_WS/sros2_keystorec. Initialize the Keystore and Create Keysros2 security create_keystore $ROS_WS/sros2_keystoreros2 security create_key $ROS_WS/sros2_keystore base_controlerüîê Generates a secure identity for the base_controler node.4. Define Node PermissionsCreate a file with the following XML content:&lt;permissions&gt;  &lt;grant name=\"base_controler_grant\" subject_name=\"CN=base_controler\"&gt;    &lt;validity&gt;      &lt;not_before&gt;2025-01-01T00:00:00&lt;/not_before&gt;      &lt;not_after&gt;2027-01-01T00:00:00&lt;/not_after&gt;    &lt;/validity&gt;    &lt;allow rule=\"ALLOW\"&gt;      &lt;domains&gt;        &lt;id&gt;0&lt;/id&gt;      &lt;/domains&gt;      &lt;topics&gt;        &lt;topic&gt;*&lt;/topic&gt;      &lt;/topics&gt;      &lt;partitions&gt;        &lt;partition&gt;*&lt;/partition&gt;      &lt;/partitions&gt;    &lt;/allow&gt;  &lt;/grant&gt;&lt;/permissions&gt;üìú This permission file grants the base_controler node access to all topics in domain ID 0.5. Generate Signed Permissions Fileros2 security create_permission $ROS_WS/sros2_keystore base_controler‚úçÔ∏è Signs the permissions file using the CA to make it valid and enforceable.Security Context SummaryWith this function, we now have:‚úÖ A keystore initialized for SROS2‚úÖ A certificate and key pair for the base_controler node‚úÖ A signed permission file allowing communication‚úÖ End-to-end encryption for internal robot node communicationCombined with previous security steps‚ÄîAppArmor, Auditd, and firewall configuration‚Äîwe‚Äôre building a layered security model for our robot.See code here‚ÄîNext Steps: Intrusion Detection with SuricataNow that encrypted communication is set up within the robot‚Äôs internal architecture, the next step is to detect anomalies or intrusions at the network level. In our next article, we‚Äôll integrate Suricata, an open-source intrusion detection system (IDS), to monitor traffic and alert on suspicious behavior.Stay tuned üëÄLooking to learn more about ROS2 security, SROS2 node permissions, or robotic system hardening? Bookmark this series and follow along as we secure each layer of our Linux-based robotic system.For more content like this, tools, and walkthroughs, visit my site at Sebos Technology."
  },
  
  {
    "title": "Streamlining SSH Key Management",
    "url": "/posts/SSH-AllowUsers/",
    "categories": "SSH, AllowUsers",
    "tags": "SSH, servers, cybersecurity",
    "date": "2025-04-27 17:39:00 -0600",
    





    
    "snippet": "Securing SSH access is a crucial step in hardening your Linux servers. In our previous discussion, we highlighted the importance of SSH as a secure communication protocol and introduced the sshd_co...",
    "content": "Securing SSH access is a crucial step in hardening your Linux servers. In our previous discussion, we highlighted the importance of SSH as a secure communication protocol and introduced the sshd_config file, the primary configuration point for the SSH daemon.In this article, we‚Äôll take it a step further by exploring methods to restrict SSH access to specific users and IP addresses. We‚Äôll cover three key techniques: TCP Wrappers, AllowUsers/AllowGroups directives, and IP restrictions within sshd_config. These methods provide layered security controls, helping ensure that only authorized users from designated sources can access your server.Table of Contents  What are TCP Wrappers?          Example: Allowing a Specific IP        AllowUsers and AllowGroups Directives  IP Restrictions in sshd_config  ConclusionWhat are TCP Wrappers?TCP Wrappers are a host-based access control system that can restrict network services based on IP addresses or hostnames. Although considered legacy technology, TCP Wrappers may still be present on some systems and can serve as an additional layer of security. They operate using two configuration files:  /etc/hosts.allow: Specifies which hosts are permitted to access specific services.  /etc/hosts.deny: Specifies which hosts are denied access.Example: Allowing a Specific IPIf you want to allow SSH access only from a specific IP address, such as 192.168.1.100, you can configure the following:/etc/hosts.allow:sshd: 192.168.1.100This explicitly permits SSH connections from 192.168.1.100./etc/hosts.deny:sshd: ALLThis denies SSH connections from all other sources.  Note: While TCP Wrappers provide basic access control, modern systems often rely on firewalls for similar functionality. For example, with firewalld, you can achieve the same restriction:firewall-cmd --permanent --add-rich-rule='rule family=\"ipv4\" source address=\"192.168.1.100\" service name=\"ssh\" accept'firewall-cmd --reloadUsing firewalls is generally preferred for flexibility and scalability, but knowing TCP Wrappers can be helpful in environments where they are still in use.AllowUsers and AllowGroups DirectivesThe sshd_config file governs SSH behavior through various directives, two of which‚ÄîAllowUsers and AllowGroups‚Äîare particularly useful for access control.  AllowUsers: Specifies which user accounts are allowed to access the server via SSH.  AllowGroups: Specifies which user groups are allowed SSH access.If both directives are used, a user must meet both conditions‚Äîthey must be listed in AllowUsers and belong to one of the groups specified in AllowGroups‚Äîto successfully connect via SSH.Example: Enterprise Group ControlIn an enterprise environment, it‚Äôs common to create a dedicated SSH access group, such as ssh_users, and use AllowGroups to control access:AllowGroups ssh_usersIf your Linux server is integrated with Active Directory, you can manage SSH access centrally by creating an Active Directory group and referencing it in AllowGroups. This way, only users in that AD group can SSH into the server, simplifying user management.IP Restrictions in sshd_configFor more granular control, you can combine user-based restrictions with IP-based conditions directly in the sshd_config file. This is where the Match Address block comes into play.The Match directive applies configuration settings only when specific conditions are met, such as a user‚Äôs IP address.Example: Restricting a User by IPTo allow only the user richard to SSH from the IP 192.168.1.100, add the following to sshd_config:Match Address 192.168.1.100    AllowUsers richardThis ensures that only richard can SSH from that specific IP. All other users or connections from other IPs will be denied.Alternatively, SSH supports inline IP restrictions within the AllowUsers directive:AllowUsers richard@192.168.1.100However, using the Match Address block is generally preferred for reliability and clarity, especially when managing complex configurations.  Caution: When working with conditional blocks in sshd_config, be careful to structure your directives correctly. Misconfiguration can lock you out of SSH access!ConclusionSecuring SSH access is a multi-layered process that goes beyond simply setting up keys or passwords. By using TCP Wrappers, AllowUsers/AllowGroups directives, and IP restrictions, you can significantly reduce the attack surface of your Linux server.While TCP Wrappers offer basic, legacy controls, modern solutions like firewalld provide more robust options. Combining user and group-based restrictions with IP-level controls in sshd_config gives you the flexibility to tailor access precisely to your environment‚Äôs needs.Remember, defense in depth is key. Use these techniques together to build a solid security posture for your SSH services.If you have questions, suggestions, or topics you‚Äôd love to see covered, drop a comment below. Let‚Äôs make robotics not just exciting and innovative‚Äîbut secure as well.For more content like this, tools, and walkthroughs, visit my site at Sebos Technology."
  },
  
  {
    "title": "Robot Security with ROS2 and UFW",
    "url": "/posts/Firewall-ROS2/",
    "categories": "Robotics, ROS2 Install Series, Security",
    "tags": "ros2, robotics, cybersecurity, linux",
    "date": "2025-04-27 04:00:00 -0600",
    





    
    "snippet": "üß† Introduction: Why Robot Security MattersTo me, robots are more than machines‚Äîthey‚Äôre a reflection of human creativity and ingenuity. Whether designed to explore new worlds, ease daily burdens, en...",
    "content": "üß† Introduction: Why Robot Security MattersTo me, robots are more than machines‚Äîthey‚Äôre a reflection of human creativity and ingenuity. Whether designed to explore new worlds, ease daily burdens, entertain, or alleviate suffering, robots are becoming deeply embedded in our lives. But to do any of this, they must connect with the outside world.Table of Contents  Introduction: Why Robot Security Matters  Robots Need Internet‚ÄîAnd That‚Äôs a Risk  Firewalls and Robots: Not Just for Servers  Why Robots Aren‚Äôt Just Fancy Servers  Using UFW for ROS 2: A Secure Setup  Temporarily Allowing System Updates  Best Practices Recap  Conclusion: Building Safer, Smarter RobotsThat connection is a double-edged sword. In an age filled with malware, hackers, and increasing cyber threats, are we truly prepared for a future where robots outnumber people?üåê Robots Need Internet‚ÄîAnd That‚Äôs a RiskCan a robot exist in isolation? In theory, yes‚Äîbut most real-world robots need a communication channel to receive updates, new tasks, or telemetry feedback. Whether it‚Äôs via Wi-Fi, Ethernet, USB, or a console port, connectivity is vital.Firewalls help manage and control that connectivity. They protect a robot‚Äôs network interfaces‚Äîboth wired and wireless‚Äîby filtering traffic. But traditional firewall models (like ‚Äúallow all outbound, block all unsolicited inbound‚Äù) may not be sufficient for mobile, autonomous, and connected robots.üß± Firewalls and Robots: Not Just for ServersTypical servers live in protected data centers with layers of physical and network security: climate-controlled rooms, multiple firewalls, monitoring tools, and intrusion detection systems. But robots don‚Äôt have that luxury.Robots are mobile and autonomous. They need to bring their security with them. And for those built with resource-constrained hardware, adding security features like full endpoint protection or network segmentation isn‚Äôt always feasible.ü§ñ Why Robots Aren‚Äôt Just Fancy ServersSure, robots are cooler than servers. But from a networking standpoint, servers live in predictable environments. Robots live in the wild.  Servers have external security layers.  Robots must rely on built-in protections.  Many robots run on lightweight hardware and OS distributions that don‚Äôt include hardened firewall settings by default.That‚Äôs why configuring a Linux firewall on your ROS 2 robot is not optional‚Äîit‚Äôs essential.üî• Using UFW for ROS 2: A Secure SetupLinux firewalls like UFW (Uncomplicated Firewall) and firewalld are commonly used to manage rules and enforce security policies. For this guide, we‚Äôll use UFW, as it‚Äôs simple and widely supported.Sample UFW Script for ROS 2 Robotssource ./common.shufw_setup() {    ufw default deny incoming    ufw default deny outgoing    # Allow ROS node traffic out    for ip in \"${ROS_NODE_IPS[@]}\"; do        for port in {7400..7600}; do            ufw allow out to \"$ip\" port \"$port\" proto udp        done    done    # Allow SSH from development laptop only    ufw allow in from \"$PROGRAMMER_LAPTOP_IP\" to any port 22 proto tcp    ufw allow out to \"$PROGRAMMER_LAPTOP_IP\"    echo 'y' | ufw enable}This setup ensures:  No unsolicited inbound or arbitrary outbound traffic  ROS 2 communications via UDP to trusted nodes only  SSH access from a specific IP (your programming laptop)üì¶ Temporarily Allowing System UpdatesTo keep your robot secure and up to date, you may want to temporarily open up outbound access for package updates:ufw_allow_updates() {    ufw allow out to any port 53 proto udp    ufw allow out 80/tcp    ufw allow out 443/tcp}ufw_deny_updates() {    ufw delete allow out 53 proto udp    ufw delete allow out 80/tcp    ufw delete allow out 443/tcp}This way, you can install updates or patches, and then immediately lock it back down.‚úÖ Best Practices Recap            Security Practice      Why It Matters                  deny incoming, deny outgoing      Locks down everything by default              Restrict by IP &amp; port      Only allow what is explicitly trusted              Temporary rules for updates      Reduces open surface while staying up to date              Avoid ufw allow 22/tcp      Prevents global SSH access              Use ufw enable      Don‚Äôt forget to actually turn the firewall on      üß† Conclusion: Building Safer, Smarter RobotsIn the world of robotics, connectivity is both a feature and a liability. Firewalls like UFW give us a lightweight, flexible way to protect our robots without overloading their systems. As robots become more autonomous and network-aware, the need for proper firewall configurations becomes critical‚Äînot optional.By using smart defaults, scoping access, and managing updates securely, we make sure that our robots are not just useful‚Ä¶ but trustworthy.If you have questions, suggestions, or topics you‚Äôd love to see covered, drop a comment below. Let‚Äôs make robotics not just exciting and innovative‚Äîbut secure as well.For more content like this, tools, and walkthroughs, visit my site at Sebos Technology."
  },
  
  {
    "title": "Setting Up a Secure ROS 2 System Part 4 - AppArmor and Auditd",
    "url": "/posts/Armor-Audit/",
    "categories": "Robotics, ROS2 Install Series, Security",
    "tags": "ros2, robotics, cybersecurity, linux",
    "date": "2025-04-25 04:00:00 -0600",
    





    
    "snippet": "When it comes to securing your ROS 2 environment, tools like AppArmor, SELinux, and Auditd are often overlooked‚Äîbut they‚Äôre incredibly powerful. Like traditional file permissions and firewalls, the...",
    "content": "When it comes to securing your ROS 2 environment, tools like AppArmor, SELinux, and Auditd are often overlooked‚Äîbut they‚Äôre incredibly powerful. Like traditional file permissions and firewalls, these Linux security modules help control and monitor which users and processes have access to specific resources. More than just a defense layer, they also provide detailed audit logs when access is attempted or denied, making them essential for robotics applications where software interfaces with the real world.Table of Contents  Introduction to Linux Security Tools  Understanding SELinux vs AppArmor  Getting Started with Auditd  How AppArmor, SELinux, and Auditd Work Together  Setting Up AppArmor and Auditd on Ubuntu for ROS 2          Enable and Start AppArmor      Create Placeholder ROS 2 AppArmor Profile      Define Auditd Rules for ROS 2 Components      Apply Auditd Rules and Enable Logging        What‚Äôs Next?Understanding SELinux vs AppArmorBoth SELinux and AppArmor are implementations of Mandatory Access Control (MAC)‚Äîa security framework built directly into the Linux kernel. This gives them a low-level ability to enforce strict policies and limit system access, reducing the risk of compromise.  SELinux is commonly used in Red Hat-based distributions like CentOS and Fedora.  AppArmor is more often found on Debian-based systems, including Ubuntu‚Äîwhich is where ROS 2 is frequently installed.Typically, systems run either SELinux or AppArmor, not both. For ROS 2 on Ubuntu, we‚Äôll focus on AppArmor.Getting Started with AuditdAuditd is the Linux auditing daemon. It collects logs related to system activity, including access events from SELinux and AppArmor. You can use these logs to build or refine security policies‚Äîtightening access to only what‚Äôs necessary for your robot‚Äôs functionality.With Auditd, you can track execution of binaries, changes to configuration files, and access attempts‚Äîcritical information when developing in secure robotics or industrial control systems (ICS) environments.How AppArmor, SELinux, and Auditd Work TogetherThe workflow typically looks like this:  Start with AppArmor (or SELinux) in permissive mode to log but not block activity.  Use Auditd to monitor system events and understand what your robot software actually needs.  Based on this data, create or adjust security policies.  Once confident, switch to enforcing mode to actively block unauthorized actions.This iterative approach ensures security policies are accurate without disrupting development.Setting Up AppArmor and Auditd on Ubuntu for ROS 2Since ROS 2 is usually deployed on Ubuntu, we‚Äôll walk through enabling AppArmor and Auditd in this context. This setup lays the foundation for secure ROS 2 development and gives visibility into how your software interacts with the system.1. Enable and Start AppArmorsystemctl enable apparmorsystemctl start apparmorThis enables AppArmor to start on boot and begins enforcing any existing profiles.2. Create Placeholder ROS 2 AppArmor Profilemkdir -p /etc/apparmor.d/ros2/echo \"# Placeholder ROS2 AppArmor profile\" &gt; /etc/apparmor.d/ros2/ros2-defaultWhile this profile doesn‚Äôt enforce anything yet, it sets the stage for adding real rules as development progresses.3. Define Auditd Rules for ROS 2 Componentscat &lt;&lt;EOF &gt; /etc/audit/rules.d/ros2.rules-w /opt/ros -p x -k ros_exec-w /home/$ROS_USER/ros2_ws -p x -k ros_ws_exec-w /home/$ROS_USER/ros2_ws/src -p wa -k ros_src-w /etc/ros2 -p wa -k ros_conf-w /usr/bin/colcon -p x -k colcon_exec-w /usr/bin/rosdep -p x -k rosdep_exec-w /home/$ROS_USER/.bashrc -p wa -k ros_envEOFThese rules log:  Executions of binaries like ROS, colcon, and rosdep  Changes to source files, configs, and shell environmentUse $ROS_USER as your development user or replace it directly.4. Apply Auditd Rules and Enable Loggingaugenrules --loadsystemctl enable auditdsystemctl start auditdThis applies your audit rules and ensures Auditd starts automatically.What‚Äôs Next?At this point, you have:  A fresh Ubuntu installation with ROS 2  A dedicated ROS user and the environment  Auditd and AppArmor installed and configuredNext up in the series, we‚Äôll explore how to configure firewall rules to further protect your robot‚Äôs communication channels and surface interfaces.If you have questions, suggestions, or topics you‚Äôd love to see covered, drop a comment below. Let‚Äôs make robotics not just exciting and innovative‚Äîbut secure as well."
  },
  
  {
    "title": "Setting Up a Secure ROS 2 System - Part 3 Installing ROS 2 ",
    "url": "/posts/Install-ROS2/",
    "categories": "Robotics, ROS2 Install Series, Security",
    "tags": "ros2, robotics, cybersecurity, linux",
    "date": "2025-04-18 11:58:00 -0600",
    





    
    "snippet": "Setting up a reliable and efficient ROS 2 development environment is a key step in any robotics or automation project. In our previous post, we covered how to:  Update a fresh Ubuntu installation. ...",
    "content": "Setting up a reliable and efficient ROS 2 development environment is a key step in any robotics or automation project. In our previous post, we covered how to:  Update a fresh Ubuntu installation.  Create and configure a dedicated ROS 2 user.  Prepare the system for installing ROS 2.Now, we‚Äôll continue that journey by walking through the ros2_install.sh script. This script automates the installation of ROS 2 base components and supporting tools to streamline your development setup.  ‚úÖ Note: These steps are tailored for Ubuntu 24.04 (Noble Numbat) systems.üìö Table of Contents  Overview of the ros2_install.sh Script  Step-by-Step Breakdown          Installing Prerequisite Packages      Adding the ROS 2 Repository      Installing ROS 2 Base and Tools      Initializing rosdep      Setting Up the ROS 2 Environment        Next Steps  SEO TagsOverview of the ros2_install.sh ScriptThe ros2_install.sh script is designed to automate the key steps needed to install ROS 2 on a clean Ubuntu system. It handles:  Installing all necessary system dependencies.  Adding the official ROS 2 package repository and its GPG key.  Installing core ROS 2 packages along with useful developer tools.  Initializing rosdep, which handles ROS dependency management.This script greatly simplifies what would otherwise be a tedious manual installation process.Step-by-Step BreakdownInstalling Prerequisite Packagesapt install -y curl gnupg2 lsb-release software-properties-commonThis command installs essential tools needed to add third-party APT repositories:  curl: Downloads content from URLs (used to retrieve the GPG key).  gnupg2: Manages GPG keys to verify repository authenticity.  lsb-release: Outputs distribution codename (e.g., focal, jammy) for dynamic configuration.  software-properties-common: Allows managing additional APT sources.Adding the ROS 2 RepositoryImport the GPG Key:curl -sSL \"$ROS_KEY_URL\" -o /etc/apt/trusted.gpg.d/ros.ascThis command downloads the ROS 2 repository‚Äôs public GPG key and stores it in the system‚Äôs trusted keyring, ensuring secure installation of ROS packages.Add the Repository:echo \"deb [arch=amd64 signed-by=/etc/apt/trusted.gpg.d/ros.asc] $ROS_REPO_URL $(lsb_release -cs) main\" &gt; /etc/apt/sources.list.d/ros2.listThis command adds the ROS 2 repository to the system‚Äôs APT sources. It automatically uses the appropriate Ubuntu codename (e.g., jammy) to ensure compatibility.Update APT Cache:apt updateThis refreshes the package index so the newly added ROS 2 repository is recognized.Installing ROS 2 Base and Toolsapt install -y ros-$ROS_DISTRO-ros-base python3-rosdep python3-colcon-common-extensions python3-argcomplete colconThis installs the ROS 2 base system and essential development tools:  ros-$ROS_DISTRO-ros-base: The minimal installation of ROS 2.  python3-rosdep: Tool to resolve and install ROS package dependencies.  colcon, python3-colcon-common-extensions: ROS 2‚Äôs recommended build system.  python3-argcomplete: Adds command-line autocompletion for ROS CLI tools.Initializing rosdepOne-Time Initialization (if needed):[ -f /etc/ros/rosdep/sources.list.d/20-default.list ] || rosdep initThis checks whether rosdep has already been initialized. If not, it runs the initialization process.Update rosdep Definitions:rosdep updateDownloads the latest package dependency definitions so that rosdep can install required system packages for ROS nodes.Setting Up the ROS 2 EnvironmentSource the Environment Manually:source /opt/ros/&lt;distro&gt;/setup.bashThis sets up necessary environment variables (like ROS_PACKAGE_PATH) for ROS 2 in the current terminal session.Make it Persistent for a User:grep -q \"source /opt/ros/$ROS_DISTRO/setup.bash\" /home/$ROS_USER/.bashrc \\|| echo \"source /opt/ros/$ROS_DISTRO/setup.bash\" &gt;&gt; /home/$ROS_USER/.bashrcThis command ensures the ROS 2 environment is automatically sourced every time the user opens a new terminal. It checks for an existing entry to avoid duplicates.Next StepsAt this point, your system should be fully prepared for ROS 2 development. Here‚Äôs what we‚Äôve achieved so far:  A freshly installed and updated Ubuntu system.  A dedicated user account configured for ROS 2.  ROS 2 base system and build tools installed and ready to use.See code hereIn the next post, we‚Äôll focus on securing the ROS 2 environment using AppArmor and Auditd, adding an extra layer of protection to your robotics platform.If you have questions, suggestions, or topics you‚Äôd love to see covered, drop a comment below. Let‚Äôs make robotics not just exciting and innovative‚Äîbut secure as well.For more content like this, tools, and walkthroughs, visit my site at Sebos Technology."
  },
  
  {
    "title": "Installing ROS 2 on Ubuntu with `ros2_install.sh`",
    "url": "/posts/Installing-ROS2/",
    "categories": "Robotics, ROS2 Install Series, Security",
    "tags": "ros2, robotics, cybersecurity, linux",
    "date": "2025-04-09 11:58:00 -0600",
    





    
    "snippet": "Setting up a reliable and efficient ROS 2 development environment is a key step in any robotics or automation project. In our previous post, we covered how to:  Update a fresh Ubuntu installation. ...",
    "content": "Setting up a reliable and efficient ROS 2 development environment is a key step in any robotics or automation project. In our previous post, we covered how to:  Update a fresh Ubuntu installation.  Create and configure a dedicated ROS 2 user.  Prepare the system for installing ROS 2.Now, we‚Äôll continue that journey by walking through the ros2_install.sh script. This script automates the installation of ROS 2 base components and supporting tools to streamline your development setup.  ‚úÖ Note: These steps are tailored for Ubuntu 20.04 (Focal) and Ubuntu 22.04 (Jammy) systems.üìö Table of Contents  Overview of the ros2_install.sh Script  Step-by-Step Breakdown          Installing Prerequisite Packages      Adding the ROS 2 Repository      Installing ROS 2 Base and Tools      Initializing rosdep      Setting Up the ROS 2 Environment        Next Steps  SEO TagsOverview of the ros2_install.sh ScriptThe ros2_install.sh script is designed to automate the key steps needed to install ROS 2 on a clean Ubuntu system. It handles:  Installing all necessary system dependencies.  Adding the official ROS 2 package repository and its GPG key.  Installing core ROS 2 packages along with useful developer tools.  Initializing rosdep, which handles ROS dependency management.This script greatly simplifies what would otherwise be a tedious manual installation process.Step-by-Step BreakdownInstalling Prerequisite Packagesapt install -y curl gnupg2 lsb-release software-properties-commonThis command installs essential tools needed to add third-party APT repositories:  curl: Downloads content from URLs (used to retrieve the GPG key).  gnupg2: Manages GPG keys to verify repository authenticity.  lsb-release: Outputs distribution codename (e.g., focal, jammy) for dynamic configuration.  software-properties-common: Allows managing additional APT sources.Adding the ROS 2 RepositoryImport the GPG Key:curl -sSL \"$ROS_KEY_URL\" -o /etc/apt/trusted.gpg.d/ros.ascThis command downloads the ROS 2 repository‚Äôs public GPG key and stores it in the system‚Äôs trusted keyring, ensuring secure installation of ROS packages.Add the Repository:echo \"deb [arch=amd64 signed-by=/etc/apt/trusted.gpg.d/ros.asc] $ROS_REPO_URL $(lsb_release -cs) main\" &gt; /etc/apt/sources.list.d/ros2.listThis command adds the ROS 2 repository to the system‚Äôs APT sources. It automatically uses the appropriate Ubuntu codename (e.g., jammy) to ensure compatibility.Update APT Cache:apt updateThis refreshes the package index so the newly added ROS 2 repository is recognized.Installing ROS 2 Base and Toolsapt install -y ros-$ROS_DISTRO-ros-base python3-rosdep python3-colcon-common-extensions python3-argcomplete colconThis installs the ROS 2 base system and essential development tools:  ros-$ROS_DISTRO-ros-base: The minimal installation of ROS 2.  python3-rosdep: Tool to resolve and install ROS package dependencies.  colcon, python3-colcon-common-extensions: ROS 2‚Äôs recommended build system.  python3-argcomplete: Adds command-line autocompletion for ROS CLI tools.Initializing rosdepOne-Time Initialization (if needed):[ -f /etc/ros/rosdep/sources.list.d/20-default.list ] || rosdep initThis checks whether rosdep has already been initialized. If not, it runs the initialization process.Update rosdep Definitions:rosdep updateDownloads the latest package dependency definitions so that rosdep can install required system packages for ROS nodes.Setting Up the ROS 2 EnvironmentSource the Environment Manually:source /opt/ros/&lt;distro&gt;/setup.bashThis sets up necessary environment variables (like ROS_PACKAGE_PATH) for ROS 2 in the current terminal session.Make it Persistent for a User:grep -q \"source /opt/ros/$ROS_DISTRO/setup.bash\" /home/$ROS_USER/.bashrc \\|| echo \"source /opt/ros/$ROS_DISTRO/setup.bash\" &gt;&gt; /home/$ROS_USER/.bashrcThis command ensures the ROS 2 environment is automatically sourced every time the user opens a new terminal. It checks for an existing entry to avoid duplicates.Next StepsAt this point, your system should be fully prepared for ROS 2 development. Here‚Äôs what we‚Äôve achieved so far:  A freshly installed and updated Ubuntu system.  A dedicated user account configured for ROS 2.  ROS 2 base system and build tools installed and ready to use.In the next post, we‚Äôll focus on securing the ROS 2 environment using AppArmor and Auditd, adding an extra layer of protection to your robotics platform."
  },
  
  {
    "title": "üõ°Ô∏è Setting Up a Secure ROS 2 System Part 1 ‚Äì Updating Ubuntu and Creating a ROS User",
    "url": "/posts/ROS2_Build-Part1/",
    "categories": "Robotics, ROS2 Install Series, Security",
    "tags": "ros2, robotics, cybersecurity, linux",
    "date": "2025-04-09 07:06:00 -0600",
    





    
    "snippet": "IntroductionWelcome to the first post in my series on setting up a secure ROS 2 system on Ubuntu. In this part, we‚Äôre going to lay the groundwork by updating the operating system and setting up a d...",
    "content": "IntroductionWelcome to the first post in my series on setting up a secure ROS 2 system on Ubuntu. In this part, we‚Äôre going to lay the groundwork by updating the operating system and setting up a dedicated user for ROS 2. These initial steps might seem routine, but they‚Äôre crucial for building a system that‚Äôs stable, maintainable, and secure.Table of Contents  Introduction  Modular Bash Script for Setup  Making the Script Re-Runnable  Setting the Timezone, Updating Ubuntu, and Creating the ROS User  Next Steps  SEO KeywordsTo make the process repeatable and reliable, I‚Äôm using a modular Bash script. Throughout this series, I‚Äôll be adding functionality to this script so that by the end, you‚Äôll be able to start from a fresh Ubuntu install and fully configure your ROS 2 system with automation and security best practices baked in.Let‚Äôs dive in and start crafting the setup script.Modular Bash Script for SetupTo keep things organized, I‚Äôve broken the script into logical modules. This makes it easier to maintain, debug, and extend. At the core, there‚Äôs a common.sh script where shared variables and functions live. Here‚Äôs an overview of what‚Äôs included:#!/bin/bash# Shared configuration and utility functionsTIMEZONE=\"UTC\"                           # System timezoneROS_DISTRO=\"jazzy\"                       # ROS 2 distributionROS_USER=\"rosbot\"                        # Local ROS userROS_WS=\"/home/$ROS_USER/ros2_ws\"        # Development workspace# ROS repository and keyROS_KEY_URL=\"https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc\"ROS_REPO_URL=\"http://packages.ros.org/ros2/ubuntu\"# IPs of other ROS nodes in the systemROS_NODE_IPS=(\"192.168.1.10\" \"192.168.1.11\")This file will be sourced by each module, ensuring all scripts have access to shared settings and configuration values.Making the Script Re-RunnableOne important design goal is that the script should be safe to re-run. Whether you‚Äôre tweaking things during development or reinstalling on a new version of Ubuntu, being able to resume where you left off is essential.Each task in the script is wrapped in a run_step function, which checks whether that step has already completed. If it has, it skips it. If not, it runs and logs the result.STATE_FILE=\"/var/log/ros2_setup_state\"log_step() { echo \"$1\" &gt;&gt; \"$STATE_FILE\"; }has_run() { grep -q \"$1\" \"$STATE_FILE\" 2&gt;/dev/null; }abort_on_failure() { echo \"[-] ERROR: $1\"; exit 1; }run_step() {    local STEP_NAME=\"$1\"    shift    if ! has_run \"$STEP_NAME\"; then        echo \"[*] Running: $STEP_NAME...\"        \"$@\" || abort_on_failure \"$STEP_NAME failed\"        log_step \"$STEP_NAME\"    else        echo \"[‚úì] Skipping: $STEP_NAME (already completed)\"    fi}This approach ensures robustness and resilience as you build out more complex setups.Setting the Timezone, Updating Ubuntu, and Creating the ROS UserThe first actual setup script we‚Äôll use is system_setup.sh. This assumes you‚Äôre starting from a clean Ubuntu install and performs the following:  Sets the system timezone  Updates and upgrades all system packages  Installs commonly needed software tools  Creates a dedicated user for running ROS 2Here‚Äôs the relevant snippet:#!/bin/bashsource ./common.shtimezone_and_update() {    timedatectl set-timezone \"$TIMEZONE\"    apt update    apt full-upgrade -y    apt install -y curl gnupg2 lsb-release software-properties-common ufw apparmor apparmor-utils auditd vim nano}And for setting up the user:create_ros_user() {    id -u \"$ROS_USER\" &amp;&gt;/dev/null || (        adduser --disabled-password --gecos '' \"$ROS_USER\"        usermod -aG sudo \"$ROS_USER\"    )}With these two functions, we begin the process of hardening the base system and preparing a clean, controlled environment for running ROS 2.While these steps might seem a bit mundane, they are foundational. A secure system is built in layers, and that starts with good hygiene: consistent configuration, up-to-date software, and properly scoped user permissions.Next StepsIn the next post, we‚Äôll continue building the script to install ROS 2 itself, configure the environment, and begin applying security tools such as firewalls and intrusion detection systems.Security doesn‚Äôt come from one magic tool ‚Äî it‚Äôs a layered approach that begins at the OS level. With the base system updated and a dedicated user in place, we‚Äôre ready to move on to more exciting territory.Stay tuned!"
  },
  
  {
    "title": "Hardening Your Robot Project from the Start",
    "url": "/posts/ROS2_Build/",
    "categories": "Robotics, ROS2 Install Series, Security",
    "tags": "ros2, robotics, cybersecurity, linux",
    "date": "2025-04-08 04:00:00 -0600",
    





    
    "snippet": "As robotics becomes more accessible and developers take the leap into building their own intelligent machines, the importance of security cannot be overstated. Whether you‚Äôre tinkering with your fi...",
    "content": "As robotics becomes more accessible and developers take the leap into building their own intelligent machines, the importance of security cannot be overstated. Whether you‚Äôre tinkering with your first robot at home or developing a prototype for industrial use, it‚Äôs critical to think beyond just getting ROS2 installed and your nodes communicating.Welcome to the first post in a new series focused on securing your ROS2 environment ‚Äî starting from the ground up, at the Linux OS level. This guide is crafted to walk you through practical and effective steps to immediately improve the security posture of your robot projects. Our goal is to empower builders and developers to safeguard their work from common attack vectors without getting lost in complexity.üõ°Ô∏è Why Create This SeriesThere‚Äôs no shortage of great tutorials out there that walk you through setting up ROS2, spinning up topics, or launching nodes. But once you have ROS2 up and running, what comes next? How do you ensure your system isn‚Äôt just functional, but secure?Robots are complex systems with internal hardware communication and often external network access. These connections‚Äîif left unsecured‚Äîcan become points of vulnerability. Especially when you‚Äôre dealing with expensive hardware or devices that interact with the real world, protecting your infrastructure becomes essential.This series aims to fill that gap‚Äîbridging the world of ROS2 functionality with practical cybersecurity strategies.üîß What This Series CoversIn this series, we‚Äôll walk through key steps to build a more secure ROS2 setup from the OS level and beyond:  ‚úÖ Post-install system updates  üë§ Creating a dedicated ROS user  ü§ñ Installing ROS2 securely  üîê Configuring AppArmor and Auditd  üåê Setting up basic firewall rules  üß∞ Enabling SROS2 for secure communication  üîé Installing and configuring Suricata for network intrusion detection  üìÑ Creating a baseline security report  üóìÔ∏è Automating periodic security checks with scheduled reportsEach topic will have its own deep-dive post, so you can follow along step by step or jump to the parts that are most relevant to your setup.üöÄ Next StepsStay tuned for the first hands-on guide: ‚ÄúPost-Install Hardening for Linux on a ROS2 Host‚Äù, where we‚Äôll cover system updates, user creation, and setting up your environment securely from the beginning.You can follow this series here and subscribe via RSS for updates when new parts are released.üß© Closing ThoughtsSecuring robotics projects might seem daunting at first, but just a few well-placed security practices can make a world of difference. Whether you‚Äôre a hobbyist, educator, or professional, adopting a ‚Äúsecurity-first‚Äù mindset from the beginning of your ROS2 journey will help protect both your hardware and your data.If you have questions, suggestions, or topics you‚Äôd love to see covered, drop a comment below or reach out via GitHub Discussions. Let‚Äôs make robotics not just exciting and innovative‚Äîbut secure as well."
  },
  
  {
    "title": "Running Linux in QEMU -  How to Build a VM from an ISO",
    "url": "/posts/QEMU-ISO/",
    "categories": "QEMU, Virtualization, Cybersecurity, Sysadmin",
    "tags": "linux, virtualization, sysadmin, devops",
    "date": "2025-04-05 05:07:00 -0600",
    





    
    "snippet": "Running Linux in QEMU for Testing on IBM PowerPC (or x86)I recently had the chance to work on an IBM Power 6 system running Linux‚Äîa first for me. While I‚Äôve spent years working with x86 and ARM sys...",
    "content": "Running Linux in QEMU for Testing on IBM PowerPC (or x86)I recently had the chance to work on an IBM Power 6 system running Linux‚Äîa first for me. While I‚Äôve spent years working with x86 and ARM systems, PowerPC architecture was uncharted territory until now.The need came up during a project involving a critical application where we had to validate compatibility with Power architecture. Rather than using the actual hardware (and tying it up), I fired up a QEMU virtual machine on my MacBook Pro M3 to simulate the environment.Surprisingly, it worked beautifully: fast to build, simple to configure, and perfect for quick app validation.  üí° Pro Tip: This tutorial uses qemu-system-x86_64 for demo purposes, but the same steps apply for PowerPC‚Äîjust swap in qemu-system-ppc.Absolutely! Here‚Äôs a Table of Contents you can include at the top of your Dev.to article. It‚Äôs formatted for Markdown and uses anchor-style links that are compatible with Dev.to‚Äôs automatic heading linking.üìö Table of Contents  Introduction  üîç Why Build a Custom QCOW2 Image?  üõ† Step 1: Create a Blank QCOW2 Disk  üöÄ Step 2: Boot the Ubuntu ISO with Your QCOW2 Disk          Breakdown of the Command        üîÅ Step 3: Reboot into the Installed System  üß† Final Thoughts  üîó ResourcesIf you‚Äôre a developer or systems engineer looking to test Linux on different architectures or just need a portable virtual machine for experimentation, this guide will walk you through creating a QEMU VM from an ISO file and a blank QCOW2 disk.üîç Why Build a Custom QCOW2 Image?Sure, many Linux distributions offer prebuilt QCOW2 cloud images, but sometimes you need more control.For instance:  You‚Äôre installing a specialized stack like ROS 2 on Ubuntu.  You want to simulate a different CPU architecture.  You‚Äôre building a portable development lab on your laptop.Whatever the case, starting from an ISO gives you a flexible and consistent VM environment.üõ† Step 1: Create a Blank QCOW2 DiskStart by creating a 40GB disk in QCOW2 format:qemu-img create -f qcow2 ubuntu-ros2.qcow2 40GWhy QCOW2?QCOW2 is a smart choice for virtualization:  Supports snapshots  Saves disk space through compression  Lightweight and portableüöÄ Step 2: Boot the Ubuntu ISO with Your QCOW2 DiskNow, boot your ISO using QEMU and attach the new disk. This command launches a VM with:  4 GB RAM  2 vCPUs  Boot from ISO  Port forwarding from host port 2222 to VM SSH port 22qemu-system-x86_64 \\  -m 4G \\  -smp 2 \\  -boot d \\  -cdrom ubuntu-22.04.iso \\  -drive file=ubuntu-ros2.qcow2,format=qcow2 \\  -netdev user,id=net0,hostfwd=tcp::2222-:22 \\  -device e1000,netdev=net0 \\  -display default,show-cursor=onBreakdown of the Command            Flag      Purpose                  -m 4G      Allocates 4GB of memory              -smp 2      Uses 2 CPU cores              -boot d      Boots from CD-ROM (ISO)              -cdrom      Path to your Ubuntu ISO              -drive      Your blank QCOW2 disk              -netdev / -device      Sets up networking + SSH forwarding              -display      Shows the VM window with a visible cursor      Once the Ubuntu installer boots, walk through the installation and select your new disk.üîÅ Step 3: Reboot into the Installed SystemAfter the install is complete, power down the VM. Now restart it, this time booting from the QCOW2 disk directly:qemu-system-x86_64 \\  -m 4G \\  -smp 2 \\  -drive file=ubuntu-ros2.qcow2,format=qcow2 \\  -netdev user,id=net0,hostfwd=tcp::2222-:22 \\  -device e1000,netdev=net0 \\  -display default,show-cursor=on  üîí Security Tip: If you‚Äôre planning to SSH into the VM, set up your public SSH key and secure your user account.üß† Final ThoughtsWhether you‚Äôre testing for PowerPC compatibility, building custom environments for embedded systems, or just geeking out with virtualization, QEMU is an incredibly versatile tool.It gives you a high degree of flexibility:  No need for dedicated servers  Easily simulate alternate architectures  Keep your workflows portable and reproducibleWith just a few commands, you‚Äôve set up a clean, isolated Linux environment that‚Äôs ready for development, debugging, or deployment testing.üîó Resources  QEMU Documentation  Using QEMU with PowerPCHave questions about Linux or virtualization? Drop a comment or reach out‚Äîalways happy to chat!"
  },
  
  {
    "title": "How to Use QEMU to Run Linux VMs in Minutes",
    "url": "/posts/QEMU-Linux/",
    "categories": "linux, virtualization, sysadmin, devops",
    "tags": "linux, virtualization, sysadmin, devops",
    "date": "2025-04-01 08:39:00 -0600",
    





    
    "snippet": "If you read my last post, you‚Äôll remember we explored QEMU from a high-level perspective. Today, let‚Äôs roll up our sleeves and dive into how you can quickly get a Linux VM up and running using QEMU...",
    "content": "If you read my last post, you‚Äôll remember we explored QEMU from a high-level perspective. Today, let‚Äôs roll up our sleeves and dive into how you can quickly get a Linux VM up and running using QEMU. We‚Äôll cover both graphical and console-based virtual machines, using Kali Linux and Fedora Server as examples. Once QEMU is installed, you can have a fully functional Linux VM within minutes.  ‚úÖ Install QEMU if you haven‚Äôt already.Table of Contents  Running Kali Linux with GUI in QEMU  QEMU Command Breakdown (Kali)  Running Fedora Server in Console Mode  QEMU Command Breakdown (Fedora)  Why Use QCOW2 Images?  Final ThoughtsRunning Kali Linux with GUI in QEMUKali Linux is one of the most well-known distributions in the cybersecurity world‚Äîloved by blue teamers, pen-testers, and hackers alike. While you might be familiar with the live ISO, there‚Äôs also a maintained QEMU-ready version of Kali that makes spinning up a VM super easy.Once you‚Äôve downloaded the QEMU .qcow2 image of Kali, you can launch it with a few simple options. Here‚Äôs a typical configuration I use for GUI-based Kali setups:  4 GB RAM  2 virtual CPUs  Virtual hard drive (QCOW2)  NAT networking  Virtio networking for better performance  Graphical window with visible cursor  Custom name: ‚ÄúKali VM‚ÄùThis setup is ideal for local penetration testing labs or blue team sandboxing.QEMU Command Breakdown (Kali)Run Kali Linux GUI:qemu-system-x86_64 \\  -m 4096 \\  -smp 2 \\  -hda kali-linux-2025.1a-qemu-amd64.qcow2 \\  -cpu max \\  -display default,show-cursor=on \\  -device virtio-net,netdev=net0 -netdev user,id=net0 \\  -name \"Kali VM\"Explanation of Options:  qemu-system-x86_64: Launch QEMU for 64-bit x86 systems.  -m 4096: Assigns 4 GB of RAM.  -smp 2: Enables 2 virtual CPU cores.  -hda: Mounts the Kali .qcow2 disk image as the primary drive.  -cpu max: Uses all available host CPU features for better guest performance.  -display default,show-cursor=on: Opens a graphical window with visible mouse cursor.  -device virtio-net,netdev=net0: Adds a fast virtual NIC using virtio.  -netdev user,id=net0: Enables NAT (user-mode networking).  -name: Gives the VM a name for identification.üì• Download Kali Linux QEMU imageRunning Fedora Server in Console ModeWhile GUIs are great, sometimes you just want to get straight into the terminal‚Äîespecially on servers. Running QEMU in console mode is resource-efficient and better suited for headless setups or SSH-based management.Fedora offers pre-built .qcow2 images that work perfectly with this style of virtualization. Here‚Äôs a minimal setup I use:  4 GB RAM  2 CPU cores  Max CPU feature exposure  QCOW2 virtual drive  No graphical interface  Console and QEMU monitor output to terminalQEMU Command Breakdown (Fedora)Run Fedora Server Console:qemu-system-x86_64 \\  -m 4096 \\  -smp 2 \\  -cpu max \\  -hda Fedora-Server-KVM-41-1.4.x86_64.qcow2 \\  -nographic \\  -serial mon:stdioExplanation of Options:      qemu-system-x86_64: Launches QEMU with 64-bit x86 emulation.        -m 4096: Allocates 4 GB of RAM to the VM.        -smp 2: Uses 2 virtual CPUs.        -cpu max: Enables all CPU features available to the host for optimal performance in the guest.        -hda Fedora-Server-KVM-41-1.4.x86_64.qcow2: Uses this QCOW2 disk image (Fedora Server) as the main virtual hard drive.        -nographic: Disables the graphical display (no window will pop up), and instead routes VM output to the terminal. Ideal for server environments or SSH-only VMs.        -serial mon:stdio: Redirects the serial console and QEMU monitor to your terminal (STDIO), so you interact with the VM as if it were a headless physical server via serial console.  üì• Download Fedora Server QEMU imageWhy Use QCOW2 Images?The .qcow2 format is incredibly convenient‚Äîit‚Äôs a pre-installed, ready-to-boot Linux environment in a single file. Unlike Live ISOs, changes to the system are persistent, which is ideal for testing and development.  ‚ö†Ô∏è Note: These images might include default user credentials. Make sure to change the password or, better yet, create your own user immediately after boot.QCOW2 images are perfect for:  Rapid Linux prototyping  Security testing environments  Script and automation testing  Isolated lab setupsFinal ThoughtsWhether you‚Äôre prototyping, testing scripts, or building a home lab, QEMU with ready-made QCOW2 images is a powerful and fast way to get Linux up and running. In just a few minutes, you can launch a GUI-driven Kali instance or a lean Fedora Server terminal‚Äîall without touching VirtualBox or VMware.  If you could get any Linux distro running in just a couple of minutes, what would you use it for?Have questions about Linux or virtualization? Drop a comment or reach out‚Äîalways happy to chat!"
  },
  
  {
    "title": "Embracing Secure Remote Access with PuTTY SSH Key",
    "url": "/posts/Putty/",
    "categories": "linux, virtualization, sysadmin, devops",
    "tags": "linux, virtualization, sysadmin, devops",
    "date": "2025-03-31 08:39:00 -0600",
    





    
    "snippet": "Embracing Secure Remote Access with PuTTY SSH KeysIn today‚Äôs digital landscape, secure remote access is crucial for managing servers and networks efficiently. Enter PuTTY and SSH keys‚Äîa powerful du...",
    "content": "Embracing Secure Remote Access with PuTTY SSH KeysIn today‚Äôs digital landscape, secure remote access is crucial for managing servers and networks efficiently. Enter PuTTY and SSH keys‚Äîa powerful duo enhancing security and ease of access.What is PuTTY?PuTTY is a free, open-source tool widely used on Windows to establish SSH or Telnet connections. Developed by Simon Tatham, it‚Äôs known for its robust security features and user-friendly interface, making remote server access seamless and secure.Who Uses It?Primarily, system administrators, developers, and IT professionals leverage PuTTY for accessing Linux or Unix servers. Essential for anyone needing to manage remote systems securely and efficiently.How to Generate Keys with PuTTYgen  Download and Install PuTTYgen: Obtain it from the official website.  Launch PuTTYgen: Open the tool to generate your keys.  Generate Keys: Click ‚ÄúGenerate‚Äù and move your mouse across the screen to create a secure key pair.  Save Your Keys: Store both private (puttyPrivateKey.pem) and public (id_rsa.pub) keys securely.  Optional: Save passphrase: Enhance security by protecting your private key with a passphrase.Using PuTTY SSH Keys  Configure PuTTY: Open the tool, navigate to ‚ÄúSSH &gt; Auth.‚Äù  Browse Private Key: Select your saved private key file.  Connect Securely: Use the session settings to connect and enjoy secure access without entering a password repeatedly.Why Use SSH Keys?SSH keys offer superior security over passwords by providing cryptographic authentication. They eliminate the need for shared passwords, reducing risks of unauthorized access and enhancing protection against brute-force attacks.Closing ThoughtsIncorporating PuTTY SSH keys into your remote access toolkit is a vital step towards robust security. By following these steps, you can ensure secure and efficient management of your servers. Remember to adhere to best practices, such as safeguarding private keys, to maintain the highest level of security in your remote operations."
  },
  
  {
    "title": "QEMU - Lightweight Virtualization for the Command Line",
    "url": "/posts/QEMU-Lightweight/",
    "categories": "linux, virtualization, sysadmin, devops",
    "tags": "linux, virtualization, sysadmin, devops",
    "date": "2025-03-31 04:10:00 -0600",
    





    
    "snippet": "These days, virtualization has never been easier. Whether you‚Äôre using enterprise-grade solutions like Proxmox or VMware ESXi, or desktop-friendly platforms like VirtualBox or Hyper-V, spinning up ...",
    "content": "These days, virtualization has never been easier. Whether you‚Äôre using enterprise-grade solutions like Proxmox or VMware ESXi, or desktop-friendly platforms like VirtualBox or Hyper-V, spinning up virtual machines is more accessible than ever.But for command-line lovers like myself, there‚Äôs something special about using tools that don‚Äôt require a GUI. Call it a personal motto: the hard way, or no way at all. That‚Äôs where QEMU comes in.For over a decade, I‚Äôve used QEMU to create quick, reliable VMs for short-term projects, directly from the terminal.üìö Table of Contents  üîπ Introduction  ‚öôÔ∏è What is QEMU?  üí° Why Use QEMU?  üßæ Wrapping Up‚ÄîWhat is QEMU?QEMU (Quick Emulator) is a flexible, open-source hypervisor that uses software emulation to run operating systems for a variety of hardware architectures.  Want to run an ARM-based OS on an x86 machine? QEMU can do that.  Need to test out a PowerPC distro or legacy OS? It‚Äôs got you covered.  It supports both headless operation and graphical interfaces.  VMs are stored in disk image formats like qcow2, which are easy to back up, copy, or migrate between machines.  Need compatibility with another format? QEMU includes tools to convert between image types.Whether you‚Äôre emulating different hardware or just need a fast local VM for testing, QEMU offers unmatched flexibility‚Äîright from your command line.Why Use QEMU?You might be wondering‚Äîwhy bother with QEMU when I already have a Proxmox server or access to cloud-based VMs?For me, QEMU is all about speed, portability, and simplicity:  I can create a VM, close my laptop, toss it in my backpack, and pick up exactly where I left off.  Once the VM is up, no constant network connection is needed.  It uses fewer resources than full-featured hypervisors.  Backing up or moving VMs is as simple as copying a few files.  Many distros, like Kali Linux, even provide ready-to-use qcow2 images, which can be running in minutes.Recently, I used QEMU to run a Red Hat Enterprise Linux 8 (RHEL8) PowerPC image and an x86_64 OpenBSD VM on an Apple Silicon MacBook Pro (M3/M4)‚Äîno extra hardware needed.QEMU also powers many of the hypervisors we use today behind the scenes. It‚Äôs been around a long time, and its reliability and versatility keep it relevant in modern workflows.Wrapping UpQEMU is an incredibly powerful tool for developers, sysadmins, and tinkerers alike. Whether you‚Äôre running quick test environments or exploring new OS architectures, it‚Äôs a great addition to your toolbox.I‚Äôll be diving deeper into how to set up and optimize QEMU environments in future posts. Stay tuned!Need help with Linux or virtualization? Feel free to reach out‚ÄîI‚Äôm always happy to chat."
  },
  
  {
    "title": "Stop Losing Code - Automate Your GitHub Backups with Bash Scripts",
    "url": "/posts/AutoGithub/",
    "categories": "Github, Bash, Automation, DevOps Tools, Code Management",
    "tags": "Automation, BashScripting, DevOps, Git, Git Automation, ProductivityHacks",
    "date": "2025-03-10 06:32:00 -0600",
    





    
    "snippet": "Are you tired of losing track of your projects and code scattered across multiple machines? You‚Äôre not alone. I used to have this bad habit of never checking in my code, which left me hunting down ...",
    "content": "Are you tired of losing track of your projects and code scattered across multiple machines? You‚Äôre not alone. I used to have this bad habit of never checking in my code, which left me hunting down files across old laptops and virtual machines. That changed when I automated my GitHub workflow with simple Bash scripts. Here‚Äôs how you can automate GitHub backups and secure your code effortlessly.üöÄ Table of Contents  Why You Should Automate Code Backups  Automating GitHub Repository Setup  Securing GitHub Authentication via SSH and API Tokens  Automating Code Sync and Git Push  Final Thoughts and Next StepsüìÇ Why You Should Automate Code BackupsManual backups fail because‚Ä¶ well, we‚Äôre human. Automating this process ensures that your projects are always safe, versioned, and recoverable‚Äîeven if your machine crashes tomorrow.üí° Pro Tip: Automating GitHub repository creation and code check-ins not only saves time but also improves your DevOps hygiene.üîß Automating GitHub Repository SetupWhile working on my Ethical Hacking Robot project, I realized my code wasn‚Äôt backed up anywhere. This led me to create a Bash script that automatically links new projects to GitHub repositories using a configuration file.üóÇÔ∏è Configuration Variables (setup_config.sh):  USERNAME ‚Äì Linux user running the project  GITHUB_REPO ‚Äì Target GitHub repository URL  GIT_USER_NAME ‚Äì GitHub account name  GIT_USER_EMAIL ‚Äì GitHub email  GITHUB_API_TOKEN ‚Äì GitHub API token (retrieved securely at runtime)üìÑ Sample Script:#!/bin/bashsource setup_config.shLOGFILE=\"/home/$USERNAME/setup.log\"exec &gt; &gt;(tee -a \"$LOGFILE\") 2&gt;&amp;1bash setup_github.sh \"$USERNAME\" \"$GITHUB_REPO\" \"$GIT_USER_NAME\" \"$GIT_USER_EMAIL\" \"$GITHUB_API_TOKEN\"üîê Securing GitHub Authentication via SSH and API TokensSecurity first! Instead of hardcoding sensitive credentials, I store the GitHub API token in a secure file. The script then generates SSH keys and configures GitHub authentication automatically.sudo -u $USERNAME ssh-keygen -t rsa -b 4096 -C \"$USERNAME@$(hostname)\" -f $USER_HOME/.ssh/id_rsa -N \"\"echo \"Host github.com    User git    IdentityFile ~/.ssh/id_rsa    StrictHostKeyChecking no\" | sudo -u $USERNAME tee $USER_HOME/.ssh/config &gt; /dev/nullüí° Quick Security Tip: Always restrict permissions on sensitive files like .github_token using chmod 600.üì¶ Automating Code Sync and Git PushNow that authentication is set, it‚Äôs time to sync your local code with GitHub automatically.üìÅ Define Source and Destination Paths:SRC_ROS2=\"/home/ros2_dev/ros2_ws/src/\"SRC_NON_ROS=\"/home/ros2_dev/non_ros_code/\"DEST_GITHUB=\"/home/ros2_dev/github/dev/\"üîÑ Sync Code Using Rsync:rsync -av --exclude='build/' --exclude='install/' --exclude='log/' \"$SRC_ROS2\" \"$DEST_GITHUB/src/\"rsync -av \"$SRC_NON_ROS\" \"$DEST_GITHUB/non_ros_code/\"üì§ Automate Git Commit and Push:git add dev/git commit -m \"Automated backup on $(date)\"git push origin mainüí° Pro Tip: Schedule this script using cron for fully automated daily or hourly backups.üìÖ Final Thoughts and Next StepsThis workflow has saved me countless hours and secured my codebase across multiple projects. But it‚Äôs still a work in progress. Next, I plan to:  Replace hardcoded paths with dynamic variables.  Add full error handling and logging.  Publish a fully polished version of these scripts as a public GitHub repository.üëâ Check out the full working scripts here!If you found this helpful, share it with your network or drop a comment below! What automation hacks are you using to improve your workflow?"
  },
  
  {
    "title": "Securing a Prankster Robot - Linux Security Strategies to Prevent Rogue AI",
    "url": "/posts/PranksterRobot/",
    "categories": "Linux, Robotics, Sysadmin, Devops",
    "tags": "linux, robotics, ros2, devops",
    "date": "2025-03-01 14:39:00 -0600",
    





    
    "snippet": "IntroductionImagine a world where robots entertain us at theme parks, birthday parties, or public events. Now, imagine one of these robots is a mischievous clown robot designed to perform pranks‚Äîli...",
    "content": "IntroductionImagine a world where robots entertain us at theme parks, birthday parties, or public events. Now, imagine one of these robots is a mischievous clown robot designed to perform pranks‚Äîlike blowing bubbles at unsuspecting guests.While it sounds fun, what happens if the clown robot goes rogue and starts pranking everyone uncontrollably? What if its bubble gun malfunctions and sprays foam at VIP guests or clogs an important event stage?This article outlines a security architecture leveraging Linux security policies, encryption, and strict access controls to prevent unintended behavior while ensuring the system operates as expected.Table of Contents  The Clown Robot Setup  Securing the Important Modules  Securing the Code          Protecting the Prank Safety Protocol      Restricting Execution with systemd and sudoers        Friend and Foe Logic Security  The Camera Logic: Isolation &amp; Security  Bubble Gun Security: Validating Prank Requests  Overall Security Strategy  Final Thoughts: Preventing an AI TakeoverThe Clown Robot SetupOur clown robot has the following capabilities:  Facial Recognition ‚Äì Scans faces to identify people.  Friend/Foe Database ‚Äì Determines if a person is a ‚Äúprank-friendly‚Äù guest or someone who should not be pranked.  Prank Safety Protocol ‚Äì Governs whether the bubble gun can activate.  Bubble Gun Module ‚Äì Fires bubbles when permitted.Operational Flow  The robot searches its environment and scans faces.  The Friend/Foe logic determines if a person is a prank-friendly guest or someone who should not be pranked.  If prank permission is granted, the Prank Safety Protocol generates a prank token and wraps it in an encrypted authorization token before sending it to the Bubble Gun.  If prank permission is denied, the robot moves on to the next guest.Now, let‚Äôs explore how to prevent unauthorized control and ensure that the clown robot does not start spraying bubbles at everyone uncontrollably.Securing the Important ModulesEach module must be isolated and protected with strict security measures:  The Bubble Gun ‚Äì Can only fire when properly authorized.  The Prank Safety Protocol ‚Äì Controls when the gun can fire and ensures safety.  Friend and Foe Logic ‚Äì Determines valid prank targets but cannot directly fire the gun.  Camera System ‚Äì Captures images but has no control over firing.Securing the CodeProtecting the Prank Safety Protocol  The prank safety protocol code is stored in a secure, read-only directory.  The protocol only allows one bubble blast at a time before automatically engaging the safety again.  Only the root user can modify the prank safety protocol.Restricting Execution with systemd and sudoersTo ensure that only authorized components can interact with the prank safety protocol:  A dedicated systemd service manages the prank safety protocol.  A new user bubble_safety is created with restricted permissions.  A sudoers rule allows bubble_safety to run the service‚Äîbut nothing else.  The bubble_safety user cannot modify code, ensuring that even if compromised, it cannot alter the prank logic.Friend and Foe Logic Security  Runs as a separate service that listens for image processing requests.  It does not have access to the camera system, preventing manipulation of images.  If a guest is prank-friendly, it sends an encrypted request to the prank safety protocol.  The prank safety protocol returns an encrypted prank token.  The Friend/Foe system never sees the actual prank token, preventing it from spoofing commands.Extra Security Layer: Wrapping the Prank TokenTo prevent interception or misuse of the prank token:  The Prank Safety Protocol generates a prank token.  It encrypts the token and then wraps it in a separate encrypted authorization token.  The Bubble Gun must decrypt the authorization token first to extract the prank token.Since only the Bubble Gun can decrypt the authorization token, this prevents other components from manipulating the token.The Camera Logic: Isolation &amp; Security  A separate service tracks movement and captures images.  No access to the prank safety protocol or gun‚Äîit simply forwards images.  Does not process Friend/Foe logic, ensuring it cannot manipulate decisions.Bubble Gun Security: Validating Prank Requests  The bubble gun listens for prank tokens from both the prank safety protocol and Friend/Foe logic.  To fire, the gun must verify:          Sender Identity ‚Äì Encrypted sender ID validation.      Token Expiry ‚Äì Tokens expire after a short time.      Token Matching ‚Äì The Friend/Foe token must match the Prank Safety Protocol token.      Authorization Token Validation ‚Äì The prank safety protocol‚Äôs encrypted token must be decrypted before extracting the prank token.      This multi-step validation ensures that no single system can authorize a prank on its own.Overall Security Strategy1. Camera System‚úÖ Captures images but cannot access the gun or prank safety protocol.‚úÖ Does not process Friend/Foe logic, preventing manipulation.2. Friend and Foe Logic‚úÖ Cannot access the camera, ensuring image integrity.‚úÖ Can communicate with the gun but lacks the full prank key.‚úÖ Cannot modify the prank safety protocol or reuse expired tokens.3. Prank Safety Protocol‚úÖ Has access to the gun but only generates a wrapped authorization token.‚úÖ Cannot forge Friend/Foe verification tokens.‚úÖ Runs as a protected systemd service with restricted execution.4. Bubble Gun‚úÖ Only fires when it receives a valid decrypted prank token.‚úÖ Validates both the prank token and the authorization token.‚úÖ Cannot be activated without explicit authorization.Final Thoughts: Preventing an AI TakeoverTo further harden security, implementing SELinux or AppArmor can restrict processes to their intended permissions.  SELinux (Security-Enhanced Linux) can define mandatory access control (MAC) policies to prevent unauthorized access to critical files and services.  AppArmor can restrict each service to a predefined set of operations, ensuring that even if compromised, they cannot execute arbitrary commands.By combining system isolation, encryption, token-based authentication, and kernel-level security policies, we ensure that the clown robot remains under strict control‚Äîa fun entertainer, not a rogue prankster.üîê Security in robotics isn‚Äôt just about preventing rogue AI‚Äîit‚Äôs about ensuring systems operate as designed, without unintended consequences."
  },
  
  {
    "title": "Enhancing Code Reusability in Robotics - A Modular WiFi Scanner with ROS2 & Systemd",
    "url": "/posts/ROS2-Reusability/",
    "categories": "Linux, Robotics, Sysadmin, Devops",
    "tags": "linux, robotics, ros2, devops",
    "date": "2025-02-26 14:39:00 -0600",
    





    
    "snippet": "When exploring robotics and automation, it‚Äôs easy to find demonstration code that blends system-generated logic with user-defined logic. While these demos are great for learning, they often lack th...",
    "content": "When exploring robotics and automation, it‚Äôs easy to find demonstration code that blends system-generated logic with user-defined logic. While these demos are great for learning, they often lack the structured approach needed for real-world applications. As someone with a background in coding, the concept of Separation of Duty has been ingrained in me. This principle ensures that different components of a system remain modular, making them easier to maintain, extend, and reuse.In this article, we‚Äôll apply this principle by designing a WiFi Scanner that:  Scans for available WiFi networks  Saves results to a database  Provides multiple ways to execute the scan (direct execution, systemd automation, and ROS2 integration)    Table of Contents    Code Reusability: A Practical Approach with ROS2, Systemd, and Python          Table of Contents      Understanding Code Reusability in ROS2      Building a WiFi Scanner in Python      Automating with Systemd Timers and Services                  Systemd Service          Systemd Timer                    Integrating with ROS2                  Why Use ROS2?          Creating a ROS2 Topic                    Conclusion: The Power of Separation of Duty      By the end, we‚Äôll have a single Python-based WiFi scanner that can be accessed in multiple ways while keeping the core logic separate.Understanding Code Reusability in ROS2Code reusability means writing code in a way that it can be used in different parts of a system without modification. In ROS2, this means keeping system-level and user-level logic separate, making the code easier to maintain and extend.Let‚Äôs focus on creating a reusable core and exposing it through multiple interfaces. This approach allows:  Direct execution for quick testing  Automation with Systemd timers and services  Integration with ROS2 Topics for robotic applicationsLet‚Äôs begin by developing our core functionality in Python.Building a WiFi Scanner in PythonTo test our separation of duties, we first create a Python script that:  Scans for available WiFi networks  Saves the results to a database  Provides a script to trigger the scanHere‚Äôs how we can directly execute the scanner:python3 -m save_wifi_scan.pyfind code hereThis allows system users  ad hoc WiFi scans as needed. With this foundation in place, let‚Äôs explore how to automate this process using systemd.Automating with Systemd Timers and ServicesInstead of manually running the script, we can create a systemd service to execute it automatically at regular intervals.Systemd ServiceThe service acts as a wrapper around our Python script:# wifi_scan.service[Unit]Description=WiFi Scanner ServiceAfter=network.target[Service]Type=simpleUser=rootWorkingDirectory=/opt/robot/wifi_scannerExecStart=python3 /opt/robot/wifi_scanner/save_wifi_scan.pyRestart=alwaysRestartSec=10Systemd TimerTo execute the service at fixed intervals, we configure a timer:# wifi_scan.timer[Unit]Description=Run WiFi Scanner every 30 seconds[Timer]OnBootSec=10OnUnitActiveSec=30Unit=wifi_scan.service[Install]WantedBy=timers.targetNow, enabling the timer ensures the scan runs automatically every 30 seconds:systemctl enable wifi_scan.timersystemctl start wifi_scan.timerWith this setup, WiFi scanning is fully automated without requiring manual execution. However, what if we want real-time access to this data in a robotic system?Integrating with ROS2To extend our reusable WiFi scanner, we can publish scan results to a ROS2 Topic. This enables other components in a robotic system to access WiFi data dynamically.Why Use ROS2?Publishing WiFi data in ROS2 allows:  Checking if a target network is in range before connecting  Mapping WiFi signal strength to a robot‚Äôs location  Building a WiFi coverage map of the environmentCreating a ROS2 TopicTo integrate our Python WiFi scanner with ROS2, we modify our code:import syssys.path.append('/opt/robot/wifi_scanner/scanner')from scanner import WiFiScanner  # Assuming your scanner script is saved as scanner.pydef scan_and_publish(self):    networks = WiFiScanner.scan()    # Code to publish networks to a ROS2 topicfind code hereAfter rebuilding the ROS workspace, which publishes WiFi scan results to a ROS2 topic called /wifi_scanner, making them available for other robotic components.To test the topic:ros2 topic echo /wifi_scannerExample output:data: '[{\"bssid\": \"XX:XX:XX:XX:XX:XX\", \"ssid\": \"NetworkName\", \"channel\": 1, \"signal_strength\": -60}]'Learn to build ROS2 Topic hereWith this setup, our WiFi scanner is now accessible in three different ways:  Direct execution for immediate results  Automated systemd timers for scheduled scans  ROS2 Topic publishing for real-time integrationConclusion: The Power of Separation of DutyBy structuring our WiFi scanner to separate core functionality from execution methods, we‚Äôve created a highly reusable and flexible system. This approach:  Makes debugging and maintenance easier  Allows multiple ways to interact with the same core logic  Enables seamless integration into Linux and ROS2 environmentsNow, think about your own projects‚Äîwhat services could benefit from a Separation of Duty approach? Whether you‚Äôre working with robotics, system administration, or automation, designing modular and reusable components will save time and effort in the long run.What other tasks in your system could be reused more effectively? üöÄ"
  },
  
  {
    "title": "Building an Ethical Hacking Robot with ROS2 - WiFi Scanner Implementation",
    "url": "/posts/ROS2-WiFi-Scanner/",
    "categories": "Linux, Robotics, Sysadmin, Hack",
    "tags": "linux, robotics, ros2, hacking",
    "date": "2025-02-23 16:05:00 -0600",
    





    
    "snippet": "One of the key software technologies needed for my Ethical Hacking Robot is ROS2 (Robot Operating System 2). ROS2 is a powerful middleware framework that allows developers to create modular softwar...",
    "content": "One of the key software technologies needed for my Ethical Hacking Robot is ROS2 (Robot Operating System 2). ROS2 is a powerful middleware framework that allows developers to create modular software components that integrate with hardware. It has been widely adopted by hobbyists, enterprises, and government agencies for robotics development.Table of Contents  Introduction  How Does ROS2 Work?  WiFi Scanner: The First Step  Publishing WiFi Networks as a ROS2 Topic          Setting Up the ROS2 Workspace      Creating the WiFi Scanner Node      Defining the Entry Point in setup.py        Building and Running the ROS2 WiFi Scanner  Practical Use Cases: Why a WiFi Scanning Robot?  What Would You Use a WiFi-Scanning Robot For?  Next StepsYou can include this at the beginning of your article to give readers an easy way to navigate through the sections. If you plan to publish it online with Markdown support, the clickable links will work automatically. üòäHow Does ROS2 Work?ROS2 is installed as an application on an existing operating system, such as Ubuntu. It provides a structured environment to build robotic applications using:  Topics ‚Äì Real-time data streaming using a publish-subscribe model.  Services ‚Äì One-time request-response calls.  Actions ‚Äì Long-running tasks with feedback.  Parameters ‚Äì Configuration values that can be modified at runtime.  Lifecycle Nodes ‚Äì Ensuring a safe startup and shutdown of components.  TF (Transforms) ‚Äì Managing robot coordinate frames.  Logging ‚Äì Debugging and monitoring system performance.These components work together to perform robotic actions. For example, a navigation action might use a topic to provide location data while interacting with a wheel control service to move the robot.So, where do we start after installing ROS2? Let‚Äôs begin by developing a WiFi Scanner‚Äîa crucial feature for an ethical hacking robot.WiFi Scanner: The First StepSince my larger project involves creating a hacking robot, the first step was developing a WiFi scanning tool. I wrote a Python script that:  Scans for available WiFi SSIDs (network names).  Stores them in a database for future analysis.  Can be triggered manually, through a systemd timer, or as a ROS2 Topic.In the future, I envision the robot combining GPS and WiFi scanning to map out WiFi networks along with their signal strength.Publishing WiFi Networks as a ROS2 TopicFor the robot to be aware of active WiFi networks, I created a ROS2 Topic to publish available networks in real-time.Setting Up the ROS2 WorkspaceFirst, create a ROS2 workspace and package for the WiFi scanner:# Create the workspacemkdir -p /opt/robot_ros/wifi/srccd /opt/robot_ros/wifi/src# Create a ROS2 packageros2 pkg create --build-type ament_python wifi_scannerThis will generate the following directory structure:src‚îî‚îÄ‚îÄ wifi_scanner    ‚îú‚îÄ‚îÄ resource    ‚îú‚îÄ‚îÄ test    ‚îî‚îÄ‚îÄ wifi_scannerCreating the WiFi Scanner NodeInside src/wifi_scanner/wifi_scanner/, I implemented a ROS2 node that scans and publishes WiFi networks every 5 seconds.import sysimport rclpyfrom rclpy.node import Nodefrom std_msgs.msg import Stringimport json# Import the scanning codesys.path.append('/opt/robot/wifi_scanner/scanner')from scanner import WiFiScanner  # Assuming your scanner script is named wifi_scanner.pyclass WiFiScannerNode(Node):    def __init__(self):        super().__init__('wifi_scanner_node')        self.publisher_ = self.create_publisher(String, 'wifi_scanner', 5)        self.timer = self.create_timer(5.0, self.scan_and_publish)  # Scan every 5 seconds        self.get_logger().info(\"WiFi Scanner Node has been started.\")    def scan_and_publish(self):        networks = WiFiScanner.scan()        networks_data = [network.__dict__ for network in networks]        msg = String()        msg.data = json.dumps(networks_data)        self.publisher_.publish(msg)        self.get_logger().info(f\"Published {len(networks)} WiFi networks.\")def main(args=None):    rclpy.init(args=args)    node = WiFiScannerNode()    rclpy.spin(node)    node.destroy_node()    rclpy.shutdown()if __name__ == '__main__':    main()Find out more about WiFiScanner codehereDefining the Entry Point in setup.pyTo let ROS2 know which script to execute, update the setup.py file inside src/wifi_scanner/:Before:entry_points={    'console_scripts': [    ],},After:entry_points={    'console_scripts': [        'wifi_node = wifi_scanner.wifi_node:main',    ],},Building and Running the ROS2 WiFi ScannerNow that the code is in place, let‚Äôs build the package:# Navigate back to the ROS workspacecd /opt/robot_ros/wifi# Build the packagecolcon build --packages-select wifi_scannersource install/setup.bashTo start the WiFi scanner node:ros2 run wifi_scanner wifi_nodeTo verify that the topic is being published, list all available ROS2 topics:ros2 topic listYou should see:/wifi_scannerThe topic name /wifi_scanner is defined in the Python script:self.publisher_ = self.create_publisher(String, 'wifi_scanner', 10)To test the topic, use:ros2 topic echo /wifi_scannerExample output:data: '[{\"bssid\": \"XX:XX:XX:XX:XX:XX\", \"ssid\": \"Network1\", \"channel\": 1, \"rate\": \"54 Mb/s\", \"bars\": 5, \"secure\": \"WPA2\"}]'Practical Use Cases: Why a WiFi Scanning Robot?From my experience working with warehouse environments, one major issue is WiFi dead zones‚Äîareas where connectivity drops. A robot with a WiFi scanner could:  Traverse a warehouse and map WiFi coverage at different bin locations.  Identify weak signal areas and optimize access point placement.  Assist IT teams in troubleshooting connectivity issues.Beyond warehouses, other potential applications include:  Security Audits ‚Äì Identifying rogue access points in an office environment.  Smart Cities ‚Äì Mapping public WiFi coverage areas.  Disaster Recovery ‚Äì Deploying a robot to find working networks in disaster-stricken areas.Next StepsNow that we have a working WiFi scanner, the next steps include:  Integrating GPS data to map WiFi networks geographically.  Enhancing security analysis to detect open vs. secured networks.  Automating WiFi signal strength monitoring for network optimization.    What Would You Use a WiFi-Scanning Robot For?  This is just the beginning of building a fully autonomous hacking robot. Stay tuned for more updates! üöÄI‚Äôd love to hear your thoughts! How would you use a robot that can scan for WiFi networks?"
  },
  
  {
    "title": "Building a ROS2-Based Autonomous Cybersecurity Robot for Ethical Hacking",
    "url": "/posts/Cybersecuity-Robot/",
    "categories": "Linux, Robotics, Sysadmin, Hack",
    "tags": "robotics, ethical hacking, ROS2, ZeroTrust",
    "date": "2025-02-14 06:43:00 -0600",
    





    
    "snippet": "IntroductionCybersecurity and robotics are two rapidly evolving fields, yet their intersection remains largely unexplored. As robots become more advanced and widely used in industries such as healt...",
    "content": "IntroductionCybersecurity and robotics are two rapidly evolving fields, yet their intersection remains largely unexplored. As robots become more advanced and widely used in industries such as healthcare, manufacturing, and logistics, their security risks also increase. Many robotic systems today lack proper authentication, encryption, and access control mechanisms, making them vulnerable to cyber threats. This project aims to address those challenges by developing a ROS2-based autonomous robot designed specifically for ethical hacking and cybersecurity research.Table of Contents  Introduction  Why I Started This Project  What I‚Äôm Building  How Security is Integrated  How You Can Use This Project  Join Me on This JourneyWhy I Started This ProjectMy interest in both robotics and cybersecurity led me to identify a major gap‚Äîmost robots are built for functionality, not security. While ROS2 offers improvements over its predecessor, it still has vulnerabilities that attackers can exploit. I wanted to create a project that not only demonstrates secure robotic design but also serves as a penetration testing tool for evaluating real-world robotic threats. By building this robot, I hope to provide a platform that researchers and ethical hackers can use to explore cyber-physical security challenges and develop better defenses for robotic systems.What I‚Äôm BuildingThis project is a ROS2-based autonomous cybersecurity robot designed for ethical hacking and penetration testing. It consists of multiple ROS2 nodes, each responsible for different functions such as perception, navigation, and network security testing. The robot operates under Zero Trust Security principles, meaning that every component must authenticate itself before communication occurs. Unlike traditional robots that prioritize movement and task execution, this robot is designed to actively identify, test, and secure vulnerabilities in robotic systems.How Security is IntegratedTo ensure this robot can be used for both ethical hacking and real-world security applications, several security layers are integrated into its design:  Identity Verification: Every ROS2 node must authenticate itself before communicating with others, preventing unauthorized access.  End-to-End Encryption: ROS2 messages are protected using SROS2 and DDS security, ensuring secure data transmission between nodes.  Access Control: Only authorized users and processes can interact with the system, limiting potential attack vectors.  Threat Detection: AI-powered anomaly detection is used to monitor system behavior in real time, identifying potential cyber threats.These features allow the robot to simulate attacks, detect vulnerabilities, and test countermeasures, making it a valuable tool for robotic security research and penetration testing.How You Can Use This ProjectIf you‚Äôre an ethical hacker, security researcher, or robotics developer, this project can serve as a real-world testing platform for securing autonomous systems. As robots become more integrated into industrial automation, autonomous vehicles, and IoT environments, the need for robust cybersecurity grows. This project provides insights into securing robotic communications, detecting cyber threats, and implementing Zero Trust Security models.Additionally, I plan to share code, tutorials, and case studies to help others apply these security practices to their own projects. Whether you‚Äôre interested in robotic security, ROS2 hacking, or cyber-physical penetration testing, this project will offer valuable resources and insights.Join Me on This JourneyThis is just the beginning of an exciting journey into robotics and cybersecurity. I will be documenting every step, from initial development to testing and deployment. Along the way, I‚Äôll tackle challenges, experiment with new security models, and refine the robot‚Äôs capabilities.If you‚Äôre working on ROS2 security, ethical hacking, or robotic penetration testing, I‚Äôd love to collaborate and exchange ideas. Let‚Äôs work together to build a safer, more secure robotic future! üöÄ"
  },
  
  {
    "title": "Automate Port Knocking with Dynamic Port Rotation for Secure SSH Access",
    "url": "/posts/Auto-Port-Knocking/",
    "categories": "SSH, Auth Keys",
    "tags": "DevOps, CyberSecurity, SSH, EthicalHacking",
    "date": "2025-02-09 06:03:00 -0600",
    





    
    "snippet": "IntroductionNow that you have Port Knocking configured, let‚Äôs take it a step further! Instead of using a static knocking sequence, we will automatically rotate knock ports daily using a systemd tim...",
    "content": "IntroductionNow that you have Port Knocking configured, let‚Äôs take it a step further! Instead of using a static knocking sequence, we will automatically rotate knock ports daily using a systemd timer and service.This enhances security by making it nearly impossible for attackers to guess the correct knock sequence.Table of Contents  Introduction  Creating the Port Rotation Script  Setting Up a Systemd Timer          Create the Service File      Create the Timer File        Enabling the Timer  Client-Side Implementation  Enhancing Security with Additional Measures  Conclusionüìå Read Part 1: Setting Up Port Knocking for Secure SSH AccessPart of the Ethical Hacking Robot Project1. Creating the Port Rotation ScriptCreate /usr/local/bin/update_knockd_ports.sh and add the following:#!/bin/bash# Define pathsKNOCKD_MAIN_CONF=\"/etc/knockd.conf\"PORT_FILE=\"/etc/knockd_ports\"# Generate 3 unique random ports (2000-65000)NEW_PORTS=$(shuf -i 2000-65000 -n 3 | tr '\\n' ',' | sed 's/,$//')# Store new portsecho \"$NEW_PORTS\" &gt; \"$PORT_FILE\"# Update knockd configurationcat &lt;&lt;EOF &gt; \"$KNOCKD_MAIN_CONF\"[options]    UseSyslog[openSSH]    sequence = $NEW_PORTS    seq_timeout = 5    command     = /sbin/iptables -I INPUT -s %IP% -p tcp --dport 22 -j ACCEPT    tcpflags    = syn[closeSSH]    sequence    = $(echo $NEW_PORTS | awk '{print $3\",\"$2\",\"$1}')    seq_timeout = 5    command     = /sbin/iptables -D INPUT -s %IP% -p tcp --dport 22 -j ACCEPT    tcpflags    = synEOF# Restart knockd to apply changessystemctl restart knockd# Secure fileschmod 600 \"$PORT_FILE\" \"$KNOCKD_MAIN_CONF\"scp \"$PORT_FILE\" user@client-ip:~/.echo \"Knockd ports updated: $NEW_PORTS\"2. Setting Up a Systemd TimerCreate the Service File/etc/systemd/system/knockd-rotate.service[Unit]Description=Rotate Knockd PortsAfter=network.target[Service]ExecStart=/usr/local/bin/update_knockd_ports.shCreate the Timer File/etc/systemd/system/knockd-rotate.timer[Unit]Description=Schedule Knockd Port Rotation[Timer]OnCalendar=*-*-* 00:00:00Persistent=true[Install]WantedBy=timers.target3. Enabling the Timersudo systemctl daemon-reloadsudo systemctl enable --now knockd-rotate.timerCheck if the timer is running:systemctl list-timers --all4. Client-Side ImplementationTo automate knocking from the client side, we can create a script, /usr/local/bin/knock_server.sh, that takes either lock or unlock as an argument to control SSH access.Client Script: /usr/local/bin/knock_server.sh#!/bin/bashKNOCK_FILE=\"$HOME/knockd_ports\"KNOCK_SERVER=\"192.168.178.18\"  # Change to your server's hostname or IPSSH_PORT=22  # SSH port to check# Read and convert the knock sequence (replace commas with spaces)KNOCK_SEQUENCE=$(cat \"$KNOCK_FILE\" | tr ',' ' ')# Determine the actioncase \"$1\" in    unlock)        if is_ssh_open; then            echo \"SSH is already unlocked on $KNOCK_SERVER. No need to knock.\"        else            echo \"Knocking to unlock on $KNOCK_SERVER with sequence: $KNOCK_SEQUENCE\"            knock -v \"$KNOCK_SERVER\" $KNOCK_SEQUENCE        fi        ;;    lock)        REVERSED_SEQUENCE=$(echo \"$KNOCK_SEQUENCE\" | awk '{for(i=NF; i&gt;0; i--) printf $i\" \"; print \"\"}')        echo \"Knocking to lock on $KNOCK_SERVER with sequence: $REVERSED_SEQUENCE\"        knock -v \"$KNOCK_SERVER\" $REVERSED_SEQUENCE        ;;    *)        echo \"Error: Invalid state. Use 'unlock' or 'lock'.\"        exit 1        ;;esacThis script:‚úÖ Reads the latest knock sequence from ~/knockd_ports (which the server updates)‚úÖ Unlocks SSH access with the correct port knock sequence‚úÖ Locks SSH access with the reverse sequenceUsage:./knock_server.sh unlock  # Open SSH access  ./knock_server.sh lock    # Close SSH access  5. Enhancing Security with Additional MeasuresWhile Port Knocking is a powerful security feature, it should be combined with other security measures for a truly hardened SSH setup:  SSH Key Authentication ‚Äì Disable password-based logins and use SSH keys.  Fail2Ban ‚Äì Prevent brute-force attacks by banning repeated failed login attempts.  Multi-Factor Authentication (MFA) ‚Äì Add an extra layer of security for SSH logins.Code and config files available hereConclusionBy automating Port Knocking, you‚Äôve created an ultra-secure SSH access system that dynamically changes its lock combination daily!üöÄ Next Steps: Experiment with different automation intervals and explore additional SSH hardening techniques!"
  },
  
  {
    "title": "SSH Security Boost - Implementing Port Knocking to Block Unauthorized Access",
    "url": "/posts/Port-Knocking/",
    "categories": "SSH, Auth Keys",
    "tags": "DevOps, CyberSecurity, SSH, EthicalHacking",
    "date": "2025-02-08 17:42:00 -0600",
    





    
    "snippet": "IntroductionSecuring SSH access is critical for home lab users and new system administrators looking to protect their remote servers. One effective way to enhance security is Port Knocking, a techn...",
    "content": "IntroductionSecuring SSH access is critical for home lab users and new system administrators looking to protect their remote servers. One effective way to enhance security is Port Knocking, a technique that keeps SSH access hidden until a predefined sequence of connection attempts (or ‚Äúknocks‚Äù) is made on specific ports. When the correct sequence is detected, the firewall dynamically allows SSH access.Table of Contents  Introduction  Understanding Port Knocking  Installing and Configuring knockd          Step 1: Install knockd      Step 2: Edit the knockd Configuration File      Step 3: Enable knockd on Startup        Adjusting Firewall Rules          Allow Established Connections      Block SSH by Default        Enabling and Starting knockd with systemctl          Reload Systemd Daemon      Enable knockd to Start at Boot      Start the knockd Service      Verify knockd Status        Testing Port Knocking  Next Steps: Automating Port Knockingüìå Read Part 2: Automating Port Knocking with Dynamic Port RotationPart of the Ethical Hacking Robot ProjectBy the end of this tutorial, you‚Äôll have a fully functional Port Knocking setup, ensuring that your SSH server remains hidden from unauthorized access.1. Understanding Port KnockingBy default, your SSH service listens on port 22, which makes it an easy target for brute-force attacks and port scanning. With Port Knocking, your SSH port remains closed unless a specific sequence of connection attempts is made on predefined ports. Once the correct sequence is received, the firewall temporarily opens SSH access for the client.2. Installing and Configuring knockdStep 1: Install knockdFor Debian/Ubuntu, install knockd with:sudo apt update &amp;&amp; sudo apt install knockd -yFor CentOS/RHEL, use:sudo yum install knock -yStep 2: Edit the knockd Configuration FileModify /etc/knockd.conf to define the knocking sequence and the commands to open or close SSH access:### Open SSH Access  [openSSH]      sequence = 60842,31027,56118      seq_timeout = 5      command     = /sbin/iptables -I INPUT -s %IP% -p tcp --dport 22 -j ACCEPT      tcpflags    = syn  ### Close SSH Access  [closeSSH]      sequence    = 56118,31027,60842      seq_timeout = 5      command     = /sbin/iptables -D INPUT -s %IP% -p tcp --dport 22 -j ACCEPT      tcpflags    = syn  üí° Tip: You can modify the sequence values to any ports of your choice for additional security.Step 3: Enable knockd on StartupEdit /etc/default/knockd to ensure the service runs on boot:START_KNOCKD=1  KNOCKD_OPTS=\"-i ens18\"üí° Tip: Use ip a to find your network interface if unsure.3. Adjusting Firewall RulesBefore enabling Port Knocking, modify your iptables rules:‚úÖ Allow Established ConnectionsTo prevent active SSH sessions from being interrupted:sudo iptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT‚ùå Block SSH by DefaultUntil the correct knock sequence is received, block all SSH traffic:sudo iptables -A INPUT -p tcp --dport 22 -j REJECT4. Enabling and Starting knockd with systemctlReload Systemd Daemonsudo systemctl daemon-reloadEnable knockd to Start at Bootsudo systemctl enable knockdStart the knockd Servicesudo systemctl start knockdVerify knockd Statussudo systemctl status knockdIf successful, you should see ‚Äúactive (running)‚Äù. üöÄ5. Testing Port KnockingFrom your client machine, install knock and send the openSSH sequence:knock -v your-server-ip 60842 31027 56118Now, try SSH access:ssh user@your-server-ipTo lock SSH again:knock -v your-server-ip 56118 31027 60842Your SSH access should now be revoked! üéâNext Steps: Automating Port KnockingWhile this setup is effective, using the same knock sequence indefinitely can pose a security risk. A more advanced approach involves automatically rotating knock sequences using a systemd timer.üìå Read Part 2: Automating Port Knocking with Dynamic Port Rotationcode and config files thereThese two articles should now be separate and more digestible for readers. Let me know if you need any tweaks before publishing! üöÄ"
  },
  
  {
    "title": "Automating Network Packet Capture for an Ethical Hacking Robot",
    "url": "/posts/ROS2-Packet-Capture/",
    "categories": "SSH, ROS2, Packet Capture",
    "tags": "cybersecurity, tcpdump, pentesting, infosec, ros2",
    "date": "2025-02-04 12:26:00 -0600",
    





    
    "snippet": "In my last article, TCPDump &amp; Python, we explored using the tcpdump command to capture local network traffic. But for my ethical hacking robot, I need to take things a step further. Instead of ...",
    "content": "In my last article, TCPDump &amp; Python, we explored using the tcpdump command to capture local network traffic. But for my ethical hacking robot, I need to take things a step further. Instead of just capturing packets from a single machine, I want to monitor the entire subnet the robot has access to.The ultimate goal? One day, the robot should be able to autonomously scan networks, analyzing traffic without human intervention. But before we get ahead of ourselves, there‚Äôs an important question: What about permission?To ensure that the robot isn‚Äôt capturing unauthorized data, I‚Äôve implemented a simple safeguard‚Äîit checks for a permission file before starting tcpdump. If the file exists, the robot assumes it has permission to scan. Otherwise, it shuts down packet capturing quietly. With that in place, let‚Äôs dive into how the robot actually captures packets.The Capture ScriptTo automate packet capture, I created a Bash script called pcap-capture.sh. This script does a few key things:1. Checking for PermissionBefore doing anything else, the script verifies whether a permission file exists. If it doesn‚Äôt, the script exits immediately.# Check permission fileif [[ ! -f \"$PERMISSION_FILE\" ]]; then    echo \"Permission file not found! Exiting.\"    exit 1fi2. Loading Configuration VariablesThe script reads a configuration file to set important variables like output directories and capture settings.# Load configCONFIG_FILE=\"/etc/pcap-capture.conf\"if [[ ! -f \"$CONFIG_FILE\" ]]; then    echo \"Config file $CONFIG_FILE not found!\"    exit 1fisource \"$CONFIG_FILE\"3. Preparing the Output DirectoryTo keep things organized, the script ensures that an output directory exists, sets the correct permissions, and cleans up any old .pcap files before starting a new capture session.# Ensure output directory existsmkdir -p \"$OUTPUT_DIR\"# Set proper permissions for output directory (owned by root but accessible)chmod 777 \"$OUTPUT_DIR\"# Remove old .pcap files before starting a new capture sessionecho \"Clearing previous capture files in $OUTPUT_DIR...\"find \"$OUTPUT_DIR\" -name \"*.pcap\" -type f -delete4. Identifying the Network SubnetThe robot needs to determine which subnet it has access to before capturing traffic. A function is included in the script to find this information dynamically.# Function to get subnet for an interfaceget_subnet() {    # Implementation to determine the network range}5. Starting the Packet CaptureOnce the subnet is identified, the script launches tcpdump for each network interface to capture traffic. It writes the captured packets to files, rotating them based on size and time limits.echo \"tcpdump -i \"$IFACE\" net \"$SUBNET\" -w \"$FILE\" -C \"$FILE_SIZE_MB\" -G \"$ROTATE_SECONDS\" -z gzip\"See full code hereWith the script ready, the next step is ensuring it runs automatically whenever the robot starts up.Running the Script at Startup with systemdTo make sure the packet capture starts on boot, I‚Äôve created a systemd service that runs pcap-capture.sh as a background process.[Service]Type=forkingExecStart=/usr/local/bin/pcap-capture.shRestart=alwaysAfter defining the service, I enable it with:systemctl daemon-reloadsystemctl enable pcap-capture.serviceNow, whenever the robot starts up, it will check for permission, and if allowed, it will begin capturing packets automatically. In the future, I plan to add network triggers so that the robot starts capturing packets only when there are network changes.Why Capture Network Traffic?So, why does the robot need to capture network traffic? There are a few good reasons:  Security Monitoring ‚Äì By analyzing network traffic, the robot can help audit networks for unusual or suspicious activity.  Hacking Exploration ‚Äì From an ethical hacking perspective, capturing packets can help discover active devices and their communication patterns.  Storage Estimation ‚Äì Unlike netstat or nmap, tcpdump generates a massive amount of data.I need to determine how much storage the robot will require in its final build, given that tcpdump can generate a massive volume of data.If you were building a hacking robot, what‚Äôs the first thing you‚Äôd make it do? Let me know‚ÄîI‚Äôm always looking for creative ideas! üöÄ"
  },
  
  {
    "title": "Mastering TCPDump & Python for Ethical Hacking - Network Packet Analysis",
    "url": "/posts/TCPDump-ROS2/",
    "categories": "SSH, ROS2, Packet Capture",
    "tags": "cyberSecurity, Python, PenTesting, InfoSec",
    "date": "2025-02-03 12:26:00 -0600",
    





    
    "snippet": "As part of the effort to build an Ethical Hacking Robot, it‚Äôs crucial to determine the right tools for network analysis. One such tool is TCPDump, a command-line packet analyzer that allows users t...",
    "content": "As part of the effort to build an Ethical Hacking Robot, it‚Äôs crucial to determine the right tools for network analysis. One such tool is TCPDump, a command-line packet analyzer that allows users to capture and inspect network traffic in real time. Understanding and analyzing these network packets can provide deep insights into network security, making TCPDump a valuable asset for the project.Table of Contents  Introduction  Understanding TCP and Why It Matters  Using TCPDump for Packet Capture  Using Python and Scapy for Packet Analysis          Key Features of Scapy        Processing PCAP Files with Python          Sample Output      Use Cases for Network Monitoring        Legal and Ethical Considerations in Ethical Hacking          Best Practices for Ethical Hacking      Understanding TCP and Why It MattersTCP (Transmission Control Protocol) is one of the core protocols responsible for reliable data transmission across networks. It is used in various essential services such as web browsing (HTTP/HTTPS), email (SMTP, IMAP, POP3), file transfers (FTP, SCP), and remote access (SSH). Since communication between devices occurs in packets, being able to capture and analyze these packets is fundamental to network security.Using TCPDump for Packet CaptureTCPDump is a powerful command-line utility that enables users to capture and store network packets in a pcap file format. These files can be analyzed later using tools like Wireshark, TShark, or even TCPDump itself. In the case of the Ethical Hacking Robot, Python will be used to process these captured packets, generate reports, and provide security insights.A simple TCPDump command to capture packets on all network interfaces and save them into a pcap file is:sudo tcpdump -i any -w dump.pcap -G 300 -W 1  -i any: Captures packets from all network interfaces.  -w dump.pcap: Saves the captured packets into a file named dump.pcap.  -G 300: Rotates the file every 300 seconds (5 minutes).  -W 1: Limits the capture to a single file.Since pcap files are in binary format, they cannot be viewed directly. This is where Python comes in to process and analyze the captured network data.Using Python and Scapy for Packet AnalysisPython provides several libraries for network packet analysis, with Scapy being one of the most powerful. Scapy allows users to sniff, craft, send, and manipulate packets, making it a versatile tool for ethical hacking and security research.Key Features of Scapy:  Packet Sniffing &amp; Analysis (like TCPDump, but in Python).  Packet Crafting &amp; Injection (generate custom packets for testing).  Protocol Support (TCP, UDP, ICMP, ARP, DNS, HTTP, etc.).  Network Security Testing (spoofing, scanning, DoS testing).  Automation &amp; Scripting (integrate with security tools for automated analysis).By combining TCPDump for packet capture and Scapy for packet processing, we can build a system that monitors network activity and identifies potential security risks.Processing PCAP Files with PythonTo analyze external network traffic, a Python script was created to filter packets and extract IP-related information code is here. The script consists of two main classes:  PCAPProcessor ‚Äì Processes pcap files and tracks internal vs. external packets.  IPCompanyInfo ‚Äì Retrieves network-related information for an IP address, including DNS resolution and company ownership details.The process is initiated by running packet_count.py, which extracts relevant network details. Sample output from the script looks like this:External IPs Found:IP: 23.193.200.10  DNS: a23-193-200-10.deploy.static.akamaitechnologies.com  Company: AS20940 Akamai International B.V.  Packet Count: 7----------------------------------------IP: 23.221.244.29  DNS: a23-221-244-29.deploy.static.akamaitechnologies.com  Company: AS16625 Akamai Technologies, Inc.  Packet Count: 8----------------------------------------IP: 224.0.0.251  DNS: mdns.mcast.net  Company: Unknown  Packet Count: 121----------------------------------------IP: 23.221.244.238  DNS: a23-221-244-238.deploy.static.akamaitechnologies.com  Company: AS16625 Akamai Technologies, Inc.  Packet Count: 8----------------------------------------From this analysis, we can determine:  The IP addresses the device is communicating with.  The DNS resolution for each IP address.  The company ownership of the IP address.This information can be useful for:  Network monitoring ‚Äì Tracking normal traffic patterns and flagging anomalies.  Threat detection ‚Äì Identifying suspicious traffic patterns or unauthorized access attempts.  Firewall rule creation ‚Äì Setting up rules to filter unwanted or suspicious traffic.By continuously analyzing captured packets, the Ethical Hacking Robot can provide real-time insights into network security and suggest protective measures.Legal and Ethical Considerations in Ethical HackingEthical hacking should always be conducted within legal and ethical boundaries. Before using tools like TCPDump and Scapy, ensure that you have explicit permission from the network owner. Unauthorized packet capture and analysis can violate laws such as the Computer Fraud and Abuse Act (CFAA) and General Data Protection Regulation (GDPR).Best Practices for Ethical Hacking:  Obtain Proper Authorization ‚Äì Always get written permission before testing a network.  Follow Responsible Disclosure Policies ‚Äì If you discover vulnerabilities, report them responsibly.  Use Secure Environments ‚Äì Perform tests in isolated or controlled lab environments.  Comply with Local Laws ‚Äì Understand and adhere to cybersecurity laws in your region.  Avoid Data Misuse ‚Äì Do not access, store, or share sensitive information without consent.By adhering to ethical hacking guidelines, we can leverage tools like TCPDump and Python for cybersecurity research while ensuring compliance with legal standards."
  },
  
  {
    "title": "Hacking Robot Needed - Raspberry Pi Need Not Apply",
    "url": "/posts/Hacking-Robot/",
    "categories": "SSH, ROS2, Packet Capture",
    "tags": "Security, Hacking, Linux, CyberSecurity",
    "date": "2025-01-31 12:26:00 -0600",
    





    
    "snippet": "Alright, let‚Äôs set the record straight‚ÄîI don‚Äôt hate the Raspberry Pi. In fact, some of my earliest projects were built on one. But this time, I want something more than a Raspberry Pi project that ...",
    "content": "Alright, let‚Äôs set the record straight‚ÄîI don‚Äôt hate the Raspberry Pi. In fact, some of my earliest projects were built on one. But this time, I want something more than a Raspberry Pi project that just happens to have some hacking capabilities. I want to build a serious hacking robot‚Äîsomething autonomous, mobile, and packed with real security tools.So, let‚Äôs dive into the plan!üéØ The Mission: Build a Real Ethical Hacking RobotThe goal is to create a fully autonomous, mobile security auditing bot that can:‚úÖ Scan its environment using AI-based vision &amp; LiDAR.‚úÖ Detect and track Wi-Fi &amp; Bluetooth signals.‚úÖ Map physical spaces and log security vulnerabilities.‚úÖ Use GPS for precise tracking &amp; logging.‚úÖ Securely send data to a base server over VPN, Wi-Fi, or 5G.‚úÖ Support mesh networking (Meshtastic) for real-time data sharing.‚úÖ Operate in a defense mode, alerting when security threats are detected.üîß The Plan            Phase      Tasks                  üîπ Step 1: Choose Hacking Hardware      Select and configure ethical hacking tools (Wi-Fi sniffers, SDRs, GPS modules, etc.).              üîπ Step 2: Design the Mobile Base      Pick a platform that can carry all this gear.              üîπ Step 3: Power &amp; Weight Calculation      Determine power consumption and weight distribution.              üîπ Step 4: Build &amp; Integrate      Assemble everything into a working, mobile hacking robot.      üèóÔ∏è Project Blueprintüí° Project Name: Autonomous Ethical Hacking Robot (AEHR)üìÖ Estimated Completion: 12 monthsüí∞ Budget: $2,000 - $5,000‚öñÔ∏è Goal Weight: Light enough to carry with one hand and bring to work (because why not?).ü§ñ What‚Äôs Next?This is just the beginning! I‚Äôll be documenting my progress here, and I‚Äôd love your input and suggestions. Have ideas on hardware, mobility, or software? Let‚Äôs talk in the comments!Stay tuned‚Äîthis hacking bot is happening! üöÄüíª"
  },
  
  {
    "title": "Crafting a Balanced Patching Strategy",
    "url": "/posts/Secure-File-Transfers/",
    "categories": "Linux, DEVOPS",
    "tags": "CommandLine, TechTips, SysAdmin, LinuxTips",
    "date": "2025-01-26 22:53:00 -0600",
    





    
    "snippet": "SCP (Secure Copy Protocol), much like SSH, is a cornerstone technology for managing remote servers. While SSH enables access to a remote server‚Äôs command line, SCP extends this functionality by all...",
    "content": "SCP (Secure Copy Protocol), much like SSH, is a cornerstone technology for managing remote servers. While SSH enables access to a remote server‚Äôs command line, SCP extends this functionality by allowing the transfer of files and directories to and from a remote server‚Äôs file system. The majority of SCP commands follow a straightforward syntax:# Put a file on a remote serverscp &lt;file name&gt; &lt;user&gt;@&lt;remote server&gt;:&lt;path to put file&gt;# Get a file from a remote serverscp &lt;user&gt;@&lt;remote server&gt;:&lt;file with full path&gt; &lt;where to put it locally&gt;This simple approach is commonly used, but SCP offers a host of additional features that can make it even more versatile.Here‚Äôs a table of contents for your polished article:Table of Contents  Introduction  Moving a Directory  Bandwidth Limiting  Compression  Verbose Mode  Ports, Proxies, and SSH Keys  Preserving File Attributes  ConclusionEach section heading in the article can be directly linked using the corresponding anchor tags. Let me know if you‚Äôd like help formatting the article with these links or making further adjustments!Moving a DirectorySometimes, transferring entire directories‚Äîcomplete with their contents‚Äîis necessary. SCP supports this with the -r option, which recursively copies entire directories. The commands for this are similarly intuitive:# Put a directory and its contents on a remote serverscp -r &lt;directory&gt; &lt;user&gt;@&lt;remote server&gt;:&lt;path to put the directory&gt;# Get a directory from a remote serverscp -r &lt;user&gt;@&lt;remote server&gt;:&lt;directory with full path&gt; &lt;where to put it locally&gt;Bandwidth LimitingA less commonly used but critical feature of SCP is its ability to limit bandwidth usage. This is particularly useful when transferring large files during business hours, as excessive network traffic can degrade performance for others. The -l option allows you to specify a bandwidth limit in kilobits per second, which can help mitigate such issues. For example, to limit SCP to 5 Mbps:BITS_TO_BYTES=8KILO_TO_MEGA=1024SPEED_IN_MEGA=5scp -l $((${BITS_TO_BYTES}*${KILO_TO_MEGA}*${SPEED_IN_MEGA})) &lt;file name&gt; &lt;user&gt;@&lt;remote server&gt;:&lt;path to put file&gt;Additionally, SCP includes the -B option to adjust the buffer size, which determines how much data is read and written in a single operation. The default is 32 KB, but larger sizes can improve performance on high-speed networks, while smaller sizes may be better for low-speed or high-latency networks.CompressionIf network performance is a concern, compression can also be employed using the -C option. This compresses the data during transfer, which can speed up file transfers over slower connections:scp -C &lt;file name&gt; &lt;user&gt;@&lt;remote server&gt;:&lt;path to put file&gt;Verbose ModeWhen troubleshooting SCP issues, verbose mode is invaluable. The -v option provides detailed output of what SCP is doing, with additional levels of verbosity available using -vv or -vvv. A common approach to debugging involves first testing connectivity with SSH, as SCP issues are often rooted in SSH configuration problems:scp -v &lt;file name&gt; &lt;user&gt;@&lt;remote server&gt;:&lt;path to put file&gt;Ports, Proxies, and SSH KeysSCP supports advanced options like specifying a custom port (-P), using proxy servers (-o), and providing SSH keys (-i). While these can be included in the SCP command, a more efficient approach is to define them in your .ssh/config file. This simplifies commands, documents your setup for future reference, and applies the same settings to both SCP and SSH. Here‚Äôs an example configuration:Host remote    HostName remote.example.com    User myuser    ProxyJump firewall_jump           ## Proxy server - defined in another SSH config    Port 2222                         ## Port    IdentityFile ~/.ssh/keys/auth_key ## SSH keysWith this configuration, the SCP command becomes much simpler:scp &lt;file name&gt; &lt;remote&gt;:&lt;path to put file&gt;Transferring files between two remote hosts can also benefit from this approach. Define both hosts in your .ssh/config, and the SCP command is just as straightforward:scp &lt;remote from server&gt;:&lt;file name&gt; &lt;remote to server&gt;:&lt;path to put file&gt;Preserving File AttributesWhen transferring files, preserving attributes such as ownership and permissions can be crucial. The -p option ensures these attributes remain intact, making it ideal for tasks like backups, file system migrations, and compliance:scp -p &lt;file name&gt; &lt;user&gt;@&lt;remote server&gt;:&lt;path to put file&gt;ConclusionThis guide covered some of the most useful features of SCP, demonstrating its flexibility beyond basic file transfers. If you need to achieve something not listed here, I recommend checking the manual pages (man scp) or reaching out‚ÄîI‚Äôd be happy to help!"
  },
  
  {
    "title": "SSH Hardening Made Easy with OpenSCAP",
    "url": "/posts/SSH-Hardening-OpenSCAP/",
    "categories": "Linux, DEVOPS",
    "tags": "CyberSecurity, Server Hardening, Secure SSH, Hardening Guide",
    "date": "2025-01-18 22:53:00 -0600",
    





    
    "snippet": "Why is SSH Important?SSH is a critical technology, with over 80% of Linux servers relying on it for secure remote access. Without SSH, managing remote servers or virtual machines would be significa...",
    "content": "Why is SSH Important?SSH is a critical technology, with over 80% of Linux servers relying on it for secure remote access. Without SSH, managing remote servers or virtual machines would be significantly more challenging. A secure connection is fundamental for administering systems safely, which makes securing your SSH setup a high priority. But how do you go about doing that effectively?Table of Contents  Why is SSH Important?  Introducing OpenSCAP  Installing and Running OpenSCAP          Installation      Running OpenSCAP        Using the Report  The Power of OpenSCAP  Beyond Automation: Understanding Why  Conclusion    Introducing OpenSCAP  In my quest to secure SSH, I‚Äôve explored numerous resources, from YouTube tutorials to online articles listing the ‚Äútop 10 steps‚Äù for SSH hardening. However, while researching security tools, I discovered OpenSCAP‚Äîa powerful solution that stands out not only as an auditing and remediation tool but also as a learning resource. OpenSCAP doesn‚Äôt just automate fixes; it helps you understand the how and why behind the recommendations. Although OpenSCAP‚Äôs capabilities extend well beyond SSH security, that‚Äôs the focus of this article. Let‚Äôs dive into how it works.Installing and Running OpenSCAPOne of the great things about OpenSCAP is how easy it is to install, especially on Red Hat Enterprise Linux (RHEL) or Oracle Linux, as it‚Äôs included in their repositories. You can install it with a simple command:sudo dnf -y install openscap openscap-scanner scap-security-guideOpenSCAP offers several security profiles to evaluate your system. I chose the pci-dss profile, an industry standard for e-commerce platforms, to assess and harden SSH on my server. Running OpenSCAP with this profile is straightforward:For RHEL:sudo oscap xccdf eval \\    --profile xccdf_org.ssgproject.content_profile_pci-dss \\    --report pci-dss-report.html \\    /usr/share/xml/scap/ssg/content/ssg-rhel9-ds.xmlFor Oracle Linux 9:sudo oscap xccdf eval \\    --profile xccdf_org.ssgproject.content_profile_pci-dss \\    --report pci-dss-report.html \\    /usr/share/xml/scap/ssg/content/ssg-ol9-ds.xmlThe tool generates various report formats, but I find the HTML report (--report pci-dss-report.html) particularly user-friendly for analysis.Using the ReportAfter generating the report, transfer it to your local machine and open it in a web browser. The first section of the report, ‚ÄúCompliance and Scoring,‚Äù provides a summary of your system‚Äôs compliance status. For example, a basic Oracle Linux 9 installation will show baseline results that highlight potential vulnerabilities.Report BreakdownScrolling further down, you‚Äôll find a detailed section dedicated to SSH security recommendations. Each item outlines specific concerns, such as disabling root login via SSH. This level of insight empowers you to identify the issues and plan your next steps for remediation.SSH SuggestionThe Power of OpenSCAPOpenSCAP goes beyond pointing out issues‚Äîit provides solutions. The tool can generate a remediation script tailored to your selected profile. Here‚Äôs how you can create one:sudo oscap xccdf \\    generate fix \\    --profile xccdf_org.ssgproject.content_profile_pci-dss \\    /usr/share/xml/scap/ssg/content/ssg-ol9-ds.xml &gt; remediation-script.shThis command produces a script named remediation-script.sh. While comprehensive, I recommend reviewing the script carefully before executing it to ensure it doesn‚Äôt conflict with your server‚Äôs existing configuration or policies.One of my favorite features of OpenSCAP is its integrated documentation. The generated reports include links to detailed guidance for each issue. For instance, clicking on ‚ÄúDisable SSH Root Login‚Äù opens a resource explaining not only what to change but why. This fosters a deeper understanding, which is invaluable when implementing security policies.Remediation and WhyBeyond Automation: Understanding WhyWhile it‚Äôs tempting to rely entirely on automated tools, securing a server requires a thoughtful approach. Overly restrictive policies can disrupt critical applications or workflows. OpenSCAP‚Äôs detailed reports allow you to not only fix vulnerabilities but also understand their implications.This understanding is particularly valuable when:  Discussing changes with business stakeholders to justify security improvements.  Expanding your technical knowledge to address similar concerns across different technologies.  Demonstrating your expertise during job interviews or professional discussions.Ultimately, saying ‚ÄúI saw this on a YouTube video‚Äù doesn‚Äôt carry the same weight as a well-informed explanation backed by industry standards and a clear understanding of security principles.ConclusionOpenSCAP has become one of my go-to tools for SSH hardening and general security auditing. Its combination of automation, insight, and educational value makes it a standout resource. Whether you‚Äôre managing enterprise systems or tinkering with Linux as a hobby, OpenSCAP provides a structured way to enhance your security practices while deepening your technical knowledge.What other tools have you used that combine functionality with learning opportunities? Share your recommendations‚ÄîI‚Äôd love to explore them!  Disclaimer: I wasn‚Äôt contacted or sponsored by OpenSCAP to write this article. It‚Äôs simply a tool I find immensely useful and believe others will too."
  },
  
  {
    "title": "Using Rsync on Oracle Linux 9 for Secure Data Synchronization",
    "url": "/posts/Rsync/",
    "categories": "Linux, DEVOPS",
    "tags": "DataSynchronization, LinuxSecurity, SecureData, FileSync",
    "date": "2025-01-12 22:53:00 -0600",
    





    
    "snippet": "I recently worked on a data analysis project where security was a top priority. My goal was to ensure that sensitive data was shared efficiently while restricting direct access to the primary data ...",
    "content": "I recently worked on a data analysis project where security was a top priority. My goal was to ensure that sensitive data was shared efficiently while restricting direct access to the primary data server. This presented an interesting challenge: how to securely share data with multiple users without granting them access to the data server itself.Table of Contents  Using Rsync on Oracle Linux 9 for Secure Data Synchronization          Table of Contents      Rsync and Samba Server: A Secure Solution      Why Use Rsync?      Syncing Data to the Samba Server      Syncing Data Back to the Server      Automating the Sync with Systemd Timers      Why Not Use Rsync as a Daemon?      Let me know if you‚Äôd like me to adjust the article sections or refine the table of contents further!Rsync and Samba Server: A Secure SolutionTo address this, I configured a Samba server combined with an rsync service to synchronize data between the main data server and the Samba server. The Samba server allows users to access the shared data while keeping the primary server secure.To enhance security, I set up rsync over SSH, ensuring that all data transfers are encrypted. SSH authentication keys were configured for automated logins, eliminating the need for password entry during synchronization. Some might wonder: ‚ÄúIf you‚Äôre already using SSH and authentication keys, why not just use scp?‚Äù The answer lies in the efficiency and flexibility of rsync.Why Use Rsync?Rsync is a powerful tool that identifies differences between directories on two servers and synchronizes them efficiently. This includes deleting files that no longer exist on the source directory, making it an excellent choice for maintaining an up-to-date replica. I was genuinely surprised at how simple it was to set up and use.Syncing Data to the Samba ServerFor this project, I needed to synchronize all files from the data server‚Äôs directory /data/stock_predictor/ to a directory on the Samba server. Here‚Äôs the basic command I used:rsync -avz --delete \"/data/stock_predictor/\" \"rsyncuser@192.168.178.77:/srv/data/\"In my case, there was an additional requirement: I didn‚Äôt want the database files to be included in the sync. Excluding the database directory was as simple as adding an --exclude flag:rsync -avz --delete --exclude \"/data/stock_predictor/database/\" \"/data/stock_predictor/\" \"rsyncuser@192.168.178.77:/srv/data/\"Syncing Data Back to the ServerThe users could also upload XLSX files to a designated folder on the Samba server. These files needed to be synchronized back to the data server for processing. The command for this reverse synchronization was straightforward:rsync -avz --delete \"/srv/data/xlsx/upload/\" \"rsyncuser@192.168.178.76:/data/stock_predictor/xlsx/upload/\"Automating the Sync with Systemd TimersTo automate these sync processes, I created a script, /usr/local/bin/rsync_data.sh, tailored for each server. I then used systemd timers and services to schedule the synchronization tasks. Since most of the data generation occurred during off-hours, I scheduled the sync to run every 15 minutes. This ensured that any updated data was available shortly after it was generated.Why Not Use Rsync as a Daemon?While rsync can operate as a daemon, it doesn‚Äôt inherently support encryption for data transfers. For my project, encryption was non-negotiable. By combining rsync with SSH and systemd timers, I achieved secure, efficient, and automated data synchronization‚Äîall with just a few lines of configuration.  code hereThis setup highlights the flexibility and power of Linux. With tools like rsync, SSH, and systemd, you can create a robust and secure system tailored to your specific needs."
  },
  
  {
    "title": "How to Build and Manage Virtual Machines Using Proxmox CLI - A Step-by-Step Guide",
    "url": "/posts/Building-VM/",
    "categories": "Linux, DEVOPS",
    "tags": "Proxmox. ITAutomation, VMDeployment, CommandLineTools",
    "date": "2025-01-05 22:53:00 -0600",
    





    
    "snippet": "Recently, I needed to set up a series of virtual machines (VMs) in Proxmox for a project. One of the VMs was intended for high-end data processing, and I wasn‚Äôt entirely certain about the exact req...",
    "content": "Recently, I needed to set up a series of virtual machines (VMs) in Proxmox for a project. One of the VMs was intended for high-end data processing, and I wasn‚Äôt entirely certain about the exact requirements at the outset. To address this, I decided to experiment with various configurations until I found the optimal setup. While the Proxmox web interface is excellent, I needed a quicker, more flexible way to make incremental changes without navigating through multiple steps in the GUI. As someone who spends most of their time in the command line, using the Proxmox qm command-line interface felt like a natural fit.Table of Contents  Building a Virtual Machine with Proxmox CLI  Leveraging the Proxmox CLI          Script Setup        Removing Existing VMs  Creating the Base VM  Configuring Storage  Configuring Networking  Why Not Use the Web Interface?  Why Not Use a Clone or Template?  Adapting the Script for Your ProjectLeveraging the Proxmox CLIProxmox provides the qm command for managing VMs directly from the command line, offering functionality comparable to the web interface and, in some cases, even more flexibility. You can refer to the official qm man page for comprehensive details. To make my workflow more efficient and reusable, I began by setting up some variables in a script. These variables included information about the VM ID, name, storage locations, disk sizes, memory, and CPU configuration. Here‚Äôs the script snippet:#!/bin/bash## Basic VM InfoVMID=990NAME=\"ThisVM\"## Linux to InstallISO_STORAGE=\"ISOs\"           # Replace with your ISO storage nameISO_FILE=\"OracleLinux-R9-U4-x86_64-dvd.iso\" # Replace with your ISO filename## Hard Drive SizeDISK_STORAGE=\"vm_storage\"    # Replace with your disk storage nameSYSTEM_DISK=\"300G\"DATA_DISK=\"500G\"## Memory SizeMEMORY=32MEMORY_SIZE=$(( MEMORY * 1024 ))## ProcessorsCORES=7SOCKETS=2## NetworkVLAN_TAG=20INTERFACE=\"vmbr0\"Removing Existing VMsIf the VM already existed, I made sure to remove it before proceeding. This allowed me to tweak configurations and rerun the script without issues. Of course, if you plan to use this code, ensure the VM is either backed up or no longer needed.## If existing, removeif qm list | awk '{print $1}' | grep -q \"^$VMID$\"; then    qm stop $VMID    qm destroy $VMIDfiCreating the Base VMWith the setup cleared, I moved on to creating the base VM using qm create and configuring its resources. Here‚Äôs the snippet for setting memory, CPUs, and enabling NUMA (Non-Uniform Memory Access):## Create VMqm create $VMID --name $NAME ## Setup Memory and CPUsqm set $VMID --memory ${MEMORY_SIZE}qm set $VMID --balloon ${MEMORY_SIZE}qm set $VMID --cpu cputype=hostqm set $VMID --cores ${CORES} --sockets ${SOCKETS} --numa 1Note: NUMA support helps optimize memory access on modern multi-CPU servers by aligning memory regions with specific processors. This can be particularly important for high-performance workloads.Configuring StorageNext, I set up the boot ISO and the VM‚Äôs storage drives. For this project, I added a secondary disk specifically for data storage:## Install ISO and Hard Drivesqm set $VMID --cdrom $ISO_STORAGE:iso/$ISO_FILE### OS Drivepvesm alloc vm_storage $VMID vm-${VMID}-disk-0 300Gqm set $VMID --scsi0 vm_storage:vm-${VMID}-disk-0,iothread=1,cache=writeback### Data Drivepvesm alloc vm_storage $VMID vm-${VMID}-disk-1 500Gqm set $VMID --scsihw virtio-scsi-single qm set $VMID --scsi1 vm_storage:vm-${VMID}-disk-1,iothread=1### Boot Orderqm set $VMID --boot order='ide2;scsi0'Configuring NetworkingFor networking, the project required a single network card on VLAN 20. Here‚Äôs the configuration I used:## Network qm set $VMID --net0 virtio,bridge=${INTERFACE},tag=${VLAN_TAG},queues=4full code here Why Not Use the Web Interface?While there‚Äôs absolutely nothing wrong with the Proxmox web interface, for this project, I needed the flexibility to tinker and rebuild quickly. Writing the script allowed me to fine-tune the setup iteratively, and it ensured that future rebuilds would be consistent and straightforward.Why Not Use a Clone or Template?Cloning or using a VM template would certainly be a valid approach for creating similar VMs in the future. However, as someone who enjoys coding and experimenting, I found scripting this process to be both rewarding and practical. It also gave me complete control over every aspect of the VM‚Äôs configuration.How might you adapt this script for your project? Whether you‚Äôre automating VM deployments or just testing out configurations, using the Proxmox CLI can save time and streamline your workflows."
  },
  
  {
    "title": "Firewall Testing 101 - How to Secure Your Network and Block Cyber Threats",
    "url": "/posts/Firewall-Testing/",
    "categories": "Linux, DEVOPS",
    "tags": "LearnCyberSecurity, CyberSecurity, Firewall, CyberDefense",
    "date": "2024-12-16 10:21:00 -0600",
    





    
    "snippet": "A firewall is often the first line of defense for securing home or business networks. Whether you purchase a commercial solution or build your own, its purpose is to filter network traffic‚Äîallowing...",
    "content": "A firewall is often the first line of defense for securing home or business networks. Whether you purchase a commercial solution or build your own, its purpose is to filter network traffic‚Äîallowing legitimate data to pass while blocking potentially harmful activity. In larger network environments, firewalls may also exist between internal network segments, adding layers of protection.Table of Contents  Firewall Testing: Ensuring Network Security          Table of Contents      My Firewall Setup      Creating a Test Plan      External Testing      Internal Testing      Is My Firewall Secure?      Why Firewall Testing Matters      What‚Äôs Next for You?      The general rule for firewalls is to ‚Äúdeny everything in and allow anything out.‚Äù While this approach provides a baseline for security, it can leave gaps that attackers may exploit, such as reverse SSH tunnels or other vulnerabilities. So, how can you ensure your firewall is as secure as it should be?My Firewall SetupAt the center of my homelab is an OPNsense firewall. It serves as both the central failure point and the primary attack target in my network. To make matters more challenging, I‚Äôm relatively new to configuring firewalls, and after five months of experimentation, my setup is in need of a serious cleanup. This raises the critical question: is my firewall still secure?Creating a Test PlanBefore diving into testing, I conducted research to create a structured plan for evaluating my firewall‚Äôs security. You can review the plan here.The next step was to install a tool for testing‚Äîin this case, nmap, a powerful network scanning utility. It became apparent during this process that without regular testing, a false sense of security can develop. Simply assuming a firewall is secure without verification is a risky approach. What isn‚Äôt tested isn‚Äôt secure.External TestingFor external testing, I ran an nmap scan on both TCP and UDP traffic. The results were encouraging, as no open ports were detected. I also ran vulnerability scans using nmap and identified an Avahi service utilizing the Multicast DNS (mDNS) protocol. However, this service was found to be non-vulnerable.On the surface, the external (WAN) side of the firewall seems secure, but what about the internal (LAN) side?Internal TestingAs expected, scanning from the LAN side revealed open ports for SSH, DNS, HTTP, and HTTPS during the TCP scan. The UDP scan identified a domain service related to DNS. While this is normal, a vulnerability scan flagged the potential for a Slowloris DoS attack on one of the ports. This will require further investigation.Despite this finding, the results so far are promising.Is My Firewall Secure?The initial results suggest that my firewall is reasonably secure, but further testing is necessary to confirm. Each detected port must be individually tested to ensure no vulnerabilities exist. Additionally, the Slowloris DoS issue must be addressed.Future posts will document these tests in detail to avoid making this one overly lengthy.Why Firewall Testing MattersIt‚Äôs easy to install a firewall or router and assume your network is secure. Unfortunately, many people stop there, neglecting updates and routine security checks because they feel protected. This mindset can lead to untested vulnerabilities remaining unnoticed for years.To stay proactive, I‚Äôve started developing a scanning app based on my testing plan. The app is still in its early stages, with limited functionality implemented so far, but you can explore it on GitHub. My goal is to add more features as I continue refining my approach and documenting further tests.What‚Äôs Next for You?After reading this, what steps will you take to ensure your firewall is secure? Regular testing and verification are key to maintaining a robust defense against threats. If you‚Äôve never tested your setup, now is the time to start."
  },
  
  {
    "title": "Understanding SSH and Reverse SSH - A Guide for Beginers",
    "url": "/posts/Reverse-SSH/",
    "categories": "Linux, DEVOPS",
    "tags": "ReverseSSH, RemoteAccess, CyberSecurity, NetworkingTips",
    "date": "2024-12-08 10:21:00 -0600",
    





    
    "snippet": "When I first started using SSH, I saw it as a straightforward way to access a remote server from my local desktop. In the simplest terms, SSH (Secure Shell) was my go-to tool for connecting to a re...",
    "content": "When I first started using SSH, I saw it as a straightforward way to access a remote server from my local desktop. In the simplest terms, SSH (Secure Shell) was my go-to tool for connecting to a remote server to execute commands. The typical usage was clear: initiate a connection from my local machine, and access a remote resource.However, as I delved deeper into SSH concepts like jump and tunnel connections, I began to see the power of creating secure, encrypted tunnels. These tunnels allow traffic to pass securely between machines. Even then, the model was still straightforward‚ÄîI‚Äôd start locally and connect to remote resources.But what happens when your local machine doesn‚Äôt have direct access to the remote resource? This is where Reverse SSH becomes a valuable tool.Here‚Äôs a brief table of contents for your article:Table of Contents  Introduction  What Is Reverse SSH?  Setting Up Reverse SSH          Restricting Access      Starting the Reverse SSH Connection        Why Use Reverse SSH?  ConclusionYou can now link this table to the corresponding sections in the article for easy navigation.What Is Reverse SSH?Reverse SSH leverages a common network rule: traffic initiated from a device is generally allowed to return to that device. This principle underpins much of how web browsing works. Normally, SSH involves a local device initiating a connection to a remote server. But in cases where firewall rules or network configurations block this access, Reverse SSH offers a solution.With Reverse SSH, the remote device initiates the SSH connection, creating a secure tunnel back to the local device. Through this tunnel, the local device can communicate with the remote server as if it had direct access.Setting Up Reverse SSHTo illustrate how Reverse SSH works, I tested it using three devices:  Remote device (192.168.178.17): Initiates the Reverse SSH connection.  Local device (192.168.178.19): Utilizes the Reverse SSH connection to access the remote server.  Testing device (192.168.178.10): Always maintains direct SSH access to the remote device.Initially, both the local and testing devices had SSH access to the remote device.Restricting AccessTo simulate a real-world scenario, I modified the firewall on the remote device to allow SSH access only from the testing device:sudo ufw allow from 192.168.178.10 to any port 22sudo ufw deny 22sudo ufw enableAfter applying these rules, the local device could no longer access the remote device directly.Starting the Reverse SSH ConnectionNext, I initiated a Reverse SSH connection from the remote device to the local device:ssh -R 7000:localhost:22 richard@192.168.178.19This command forwards SSH traffic from the remote device‚Äôs port 22 through a tunnel to port 7000 on the local device. With this tunnel in place, the local device can connect back to the remote device:ssh richard@localhost -p 7000This command uses the tunnel to access the remote device, granting terminal access as if the local device had a direct connection.To run the Reverse SSH connection in the background, you can use the following command:ssh -f -N -R 7000:localhost:22 richard@192.168.178.19  -f: Runs the SSH session in the background.  -N: Useful for port forwarding without executing remote commands.      But why would you need Reverse SSH if you already have access to the remote device?  Why Use Reverse SSH?You might wonder: if the connection starts with the remote server, doesn‚Äôt that imply access is already available? The answer lies in automation. Reverse SSH is often used for scenarios where remote devices‚Äîlike IoT devices‚Äîneed to initiate a connection or, in a worst-case scenario, allow unauthorized control of a remote server.In my case, I discovered that adding a simple line to my SSH configuration effectively blocked Reverse SSH, even when the firewall rules didn‚Äôt:AllowUsers richard@192.168.178.10  # Restrict to Richard on the testing deviceThis highlights the importance of carefully restricting which users and devices can access your SSH servers.ConclusionReverse SSH is a powerful tool for navigating network restrictions, enabling secure communication when direct connections aren‚Äôt possible. It‚Äôs especially useful for automation or managing devices in restrictive environments.So, how would you use Reverse SSH in your setup? Whether for troubleshooting, automation, or security testing, it‚Äôs a technique worth mastering."
  },
  
  {
    "title": "Mastering SSH Tunneling - A Guide to Securing Your Network Traffic",
    "url": "/posts/SSH-Tunneling/",
    "categories": "Linux, DEVOPS",
    "tags": "SecureConnections, EncryptedTraffic, CyberSecurity, RemoteAccess",
    "date": "2024-12-01 10:21:00 -0600",
    





    
    "snippet": "In a previous blog post, I discussed SSH Proxy Jump, a method that allows SSH traffic to be routed through another server. In this article, we will explore SSH Tunneling, a similar but distinct con...",
    "content": "In a previous blog post, I discussed SSH Proxy Jump, a method that allows SSH traffic to be routed through another server. In this article, we will explore SSH Tunneling, a similar but distinct concept. While SSH Proxy Jump routes SSH traffic through an intermediary server, SSH Tunneling takes things a step further by securing traffic between two machines using SSH encryption.Table of Contents  Introduction  What is SSH Tunneling?  How Does SSH Tunneling Work?  How to Use SSH Tunneling  Simplifying SSH Tunneling with the .ssh/config File  When Should You Use SSH Tunneling?  ConclusionWhat is SSH Tunneling?SSH Tunneling is a method of sending non-SSH traffic securely between two computers by encrypting it through an SSH connection. It is commonly used to protect the confidentiality and integrity of data being transmitted over the network.While the traffic could already be encrypted (for instance, Remote Desktop Protocol or RDP), SSH Tunneling can also encrypt traffic that is typically not secure, such as File Transfer Protocol (FTP). For example, when using FTPS, the FTP traffic is already encrypted, but SSH Tunneling can still add an extra layer of security, ensuring the data remains private as it traverses the network.How Does SSH Tunneling Work?In an SSH Tunnel, the application on the local computer sends traffic to a specified port. In the background, the SSH client listens for traffic on that port and then forwards it to the default SSH port (usually port 22) to establish a secure connection. Once the traffic reaches the remote machine, it is forwarded to the target application‚Äôs port.Here‚Äôs how this works step by step:  The application on your local machine sends traffic to a designated port.  SSH, running in the background, listens for traffic on that port.  The traffic is then forwarded securely to the SSH port (usually port 22).  The remote server receives the traffic and forwards it to the application‚Äôs port.This process ensures that the traffic is encrypted and secure, protecting sensitive data from eavesdropping or tampering during transmission.How to Use SSH TunnelingOne of the most common use cases for SSH Tunneling is accessing remote systems securely. For example, you can use it to access a Remote Desktop (RDP) service on a remote server.Here is an example command for creating an SSH tunnel to access a remote machine via RDP:ssh -p 2222 -L 3389:localhost:3389 richard@192.168.116.2Let‚Äôs break down the command:  -p 2222: Specifies the port to connect to on the remote server.  -L 3389:localhost:3389: Sets up the local port forwarding. This means traffic sent to port 3389 on the local machine will be forwarded to port 3389 on the remote machine through the SSH tunnel.  richard@192.168.116.2: Specifies the user and IP address of the remote machine.After running this command, you can open your RDP application and connect to the remote machine as if it were directly accessible.Simplifying SSH Tunneling with the .ssh/config FileTo make SSH tunneling easier and more convenient, you can configure it in your ~/.ssh/config file. This allows you to avoid typing long commands and simplifies the process of initiating the tunnel.Here‚Äôs how to configure the .ssh/config file:Host rdp_desktop    HostName 192.168.116.2    User richard    IdentityFile ~/.ssh/keys/zta_desktop/zta_desktop    LocalForward 3389 localhost:3389In this configuration:  Host: Defines a shortcut name for the SSH connection (in this case, rdp_desktop).  HostName: Specifies the IP address or hostname of the remote server.  User: Defines the user to authenticate as on the remote machine.  IdentityFile: Specifies the path to your private SSH key for authentication (optional).  LocalForward: Sets up the port forwarding (in this case, forwarding port 3389 from localhost to the remote machine).Once this is configured, you can start the tunnel with a simple command:ssh rdp_desktopThis makes it much easier to manage your SSH tunneling connections, especially when working with multiple remote machines.When Should You Use SSH Tunneling?SSH Tunneling is most beneficial when you need to establish a secure connection between your local system and a remote machine, particularly when you are dealing with sensitive data. Here are a few common use cases:  Encrypting unencrypted traffic: SSH Tunnels are useful when you need to secure traffic that is typically unencrypted, such as FTP.  Bypassing firewalls or restrictions: Tunneling can be used to bypass network restrictions or firewalls that block certain protocols.  Protecting network traffic: If you are concerned about eavesdropping or man-in-the-middle attacks, SSH Tunneling provides an encrypted channel to secure your data.If you‚Äôve ever been worried about the security of your network traffic, SSH Tunneling provides a robust solution to ensure your data is transmitted safely.ConclusionIn summary, SSH Tunneling is a powerful tool for securing traffic between local and remote systems, even when the traffic itself isn‚Äôt encrypted. By using SSH‚Äôs secure channel to route non-secure traffic, you can protect sensitive information from potential threats on the network. Whether you‚Äôre accessing remote desktops, databases, or other services, SSH Tunneling provides a simple and effective way to secure your communications."
  },
  
  {
    "title": "Unlock the Secrets of Your Command Line with the History Command",
    "url": "/posts/History-Command/",
    "categories": "Linux, DEVOPS",
    "tags": "Productivity, Linux, CLI, Shell",
    "date": "2024-11-24 10:21:00 -0600",
    





    
    "snippet": "I recently worked on a project that required accessing the Linux terminal through a web interface. Unfortunately, this setup didn‚Äôt support the use of cut-and-paste functionality, which meant I had...",
    "content": "I recently worked on a project that required accessing the Linux terminal through a web interface. Unfortunately, this setup didn‚Äôt support the use of cut-and-paste functionality, which meant I had to rely heavily on manual typing. While this was initially frustrating, it turned out to be an excellent opportunity to address some of the bad habits I had developed over time. To borrow a phrase, it reminded me that those who don‚Äôt learn from the history command are doomed to retype it.Table of Contents  Unlock the Secrets of Your Command Line with the History Command          Table of Contents      History Command      Rerunning a command through history      How the history command can be helpful      Do you need to use your history?      History CommandThe history command is a powerful tool in Linux, designed to keep track of commands entered into the terminal. I‚Äôve often used it to look up commands I vaguely remember but need to revisit to refresh my memory. During my recent project‚Äîwhere I couldn‚Äôt cut and paste‚ÄîI discovered a new appreciation for the history command as I began using it to quickly rerun commands.For example, here‚Äôs a snippet from my terminal history that illustrates how I utilized it:history  547  echo \"Rebuilding sendmail.cf file\"  548  echo \"==========================\"  549  vim /etc/mail/sendmail.mc  550  m4 sendmail.mc &gt;sendmail.cf  551  systemctl restart sendmail  552  echo \"Subject: Test Email\"|sendmail  -v info@sebostechnology.ca  553  echo \"Rebuilding after changing access file\"  554  echo \"==========================\"  555  vim /etc/mail/access  556  makemap hash /etc/mail/access &lt; /etc/mail/access  557  vim /etc/mail/sendmail.mc  558  m4 sendmail.mc &gt;sendmail.cf  559  systemctl restart sendmail  560  journalctl --unit=sendmail  561  echo \"Subject: Test Email\"|sendmail  -v info@sebostechnology.caWith the history command, I could easily reference or rerun previous commands, saving time and effort when performing repetitive tasks like rebuilding configuration files or troubleshooting. It‚Äôs an invaluable resource that turns the terminal into a personal assistant, logging your every step.Rerunning a command through historyThe history command makes it incredibly simple to rerun a previous command by using the ! symbol followed by the command‚Äôs associated number. For instance, to rerun m4 sendmail.mc &gt;sendmail.cf, I could type either !550 or !558, depending on which instance I want to execute. This functionality is especially helpful for repetitive tasks.During my project, I found myself regularly running a sequence of the same commands. To streamline this process, I combined the history command with Bash‚Äôs &amp;&amp; operator, allowing me to execute multiple commands in one go. For example, here‚Äôs a set of commands I frequently used:556  makemap hash /etc/mail/access &lt; /etc/mail/access558  m4 sendmail.mc &gt;sendmail.cf559  systemctl restart sendmail561  echo \"Subject: Test Email\"|sendmail  -v info@sebostechnology.caBy leveraging history, I could run all four commands with a single line:!556 &amp;&amp; !558 &amp;&amp; !559 &amp;&amp; !561This command sequence translates to:makemap hash /etc/mail/access &lt; /etc/mail/access &amp;&amp; m4 sendmail.mc &gt;sendmail.cf &amp;&amp; systemctl restart sendmail &amp;&amp; echo \"Subject: Test Email\"|sendmail  -v info@sebostechnology.caUsing this method saved me a significant amount of time and reduced the risk of errors from manual typing. It also highlights how history can be more than just a log‚Äîit can serve as a productivity booster.But how else can the history command be helpful? For starters, you can search for specific commands using history | grep, create reusable scripts from frequently used sequences, or even prevent retyping errors by referencing previous entries. The possibilities make it an essential tool for anyone working in the terminal.How the history command can be helpfulThe history command offers a wide range of features that can be incredibly helpful in managing and optimizing your workflow in the terminal. Here are some practical ways to make the most of it:  Starting Fresh: Before beginning a new project, it‚Äôs sometimes useful to clear the history to ensure you‚Äôre starting with a clean slate. This can be done using:    history -c        Saving History for Documentation: When finishing a project, you may want to save your command history as part of your project‚Äôs documentation. This allows you to reference the commands you used later:    history &gt; project_history.txt        Analyzing Command Usage: For larger projects where you frequently repeat commands, it can be helpful to analyze your command usage. The following command generates a list of the most frequently used commands:    history | awk '{CMD[$2]++} END {for (a in CMD) print CMD[a], a}' | sort -nr | head        Disabling History Temporarily: If you want to run sensitive commands without them being logged in your history file, you can temporarily disable the history feature:    unset HISTFILE            Reverse Searching: If you need to quickly find a previously used command, you can press Ctrl + r to initiate a reverse search. Start typing, and the terminal will display matching commands from your history.    Keyword Searches: Another way to locate a specific command is by using grep to filter your history. For example:    history | grep &lt;some string&gt;      These techniques can save time, improve efficiency, and even help with troubleshooting by providing a record of what‚Äôs been done. Whether you‚Äôre clearing history, saving it, or analyzing it, the history command is a versatile tool for any terminal user.Do you need to use your history?Do you need to use the history command? Not necessarily. Using it doesn‚Äôt automatically make you a better Linux user, nor is it a command you‚Äôll likely rely on daily. However, it‚Äôs one of those underrated tools that can save you significant time and effort when you do use it.Personally, I‚Äôve found it invaluable for documenting the steps I‚Äôve taken to complete a task. It acts like a breadcrumb trail, helping me retrace my actions and ensure repeatability. Despite its usefulness, I think history is a command that doesn‚Äôt get the attention it deserves. It‚Äôs not flashy, but it‚Äôs highly practical.What about you? What‚Äôs a terminal command you think deserves more recognition or should be used more often?"
  },
  
  {
    "title": "Handling Sensitive Information in Automated Processes",
    "url": "/posts/Sensitive-Information/",
    "categories": "Linux, DEVOPS",
    "tags": "PasswordManagement, DataSecurity, AutomatedProcesses, AnsibleTips",
    "date": "2024-11-17 10:21:00 -0600",
    





    
    "snippet": "When running automated processes like Ansible playbooks via systemd timers or cron jobs, securely managing sensitive information, such as passwords, becomes crucial. This information must remain en...",
    "content": "When running automated processes like Ansible playbooks via systemd timers or cron jobs, securely managing sensitive information, such as passwords, becomes crucial. This information must remain encrypted at rest and be securely decrypted during runtime. Here‚Äôs a guide to handling this challenge effectively.Here‚Äôs a Markdown table of contents for the blog post:Table of Contents  Handling Sensitive Information in Automated Processes  The Challenge of Secure Password Management  Using Password Files with Strict Permissions  Generating Passwords with SSH Keys and Hashing Tools  Using Ansible Vault with a Hashed Password          Creating an Encrypted File      Using the Encrypted File in a Playbook      Example Workflow        Is This a Serious Solution?          Pros      Cons        Encrypting Files at Rest  Final ThoughtsThe Challenge of Secure Password ManagementThe key question is: How can passwords or sensitive information be stored and accessed securely during runtime? While there‚Äôs no one-size-fits-all solution, the method you choose should balance security, simplicity, and practicality.Using Password Files with Strict PermissionsOne common approach is to store the password in a file with strict permissions so that only the application requiring it can access it. For example:echo \"securE-p@ssw0rd\" &gt; password_file.txtchmod 600 password_file.txtsudo chown app_owner:app_owner password_file.txtIn this setup:  The file‚Äôs permissions (chmod 600) ensure only the owner can read or write to it.  The ownership (chown) restricts access to the application user.Important Note: The password file should not have an obvious name like password_file.txt. Use a non-descriptive name to avoid drawing attention to its purpose.Generating Passwords with SSH Keys and Hashing ToolsAn interesting alternative is to derive a password from an SSH key using a hashing tool. This method creates a repeatable, secure password without storing it in plaintext:## Create a test SSH authentication keyssh-keygen -t ed25519 -f innocent_file## Use hashing tool to generate a passwordecho $(cat innocent_file | sha256sum | awk '{print $1}')For example, hashing the key file might generate a password like:604e8f705bdb411ff3813e4fd536a52e6e25545d3ff5ea6d038d460e84201c88Using Ansible Vault with a Hashed PasswordAnsible provides a built-in solution for encrypting sensitive information: Ansible Vault. This approach allows you to securely store and use sensitive data in playbooks.Creating an Encrypted FileYou can create an encrypted file using the hashed password:ansible-vault create encrypted_file --vault-password-file &lt;(echo $(cat innocent_file | sha256sum | awk '{print $1}'))Using the Encrypted File in a PlaybookWhen running a playbook, provide the vault password as a hashed value:ansible-playbook &lt;some_file&gt;.yml --vault-password-file &lt;(echo $(cat innocent_file | sha256sum | awk '{print $1}'))Example WorkflowIn one implementation, the innocent_file (SSH key) was generated on a separate server. It was securely transferred to the Ansible server when needed and deleted after use. This added an extra layer of security by ensuring the password was ephemeral.Is This a Serious Solution?The SSH key-based approach is not a universal solution but has its use cases:  Pros:          Quick and easy for generating secure passwords.      No need to store plaintext passwords on disk.      Repeatable and deterministic.        Cons:          Relies on securing the SSH key file (innocent_file).      Requires additional steps for transferring and managing the key file.      Might not meet compliance standards for environments requiring long-term key management.      For environments with stringent security requirements, more robust solutions like a secrets manager (e.g., HashiCorp Vault, AWS Secrets Manager) might be better suited.Encrypting Files at RestEncrypting files at rest is a fundamental security practice. Ansible Vault is a powerful tool for this purpose, offering seamless integration with Ansible workflows. For non-Ansible processes, consider using tools like GPG or OpenSSL to encrypt sensitive files before storing them.Example using GPG:gpg --output encrypted_file.gpg --symmetric --cipher-algo AES256 sensitive_file.txtTo decrypt at runtime:gpg --decrypt encrypted_file.gpg &gt; sensitive_file.txtFinal ThoughtsSecurely managing sensitive information in automated workflows requires a thoughtful approach. Whether you choose password files, hashed keys, or secrets management solutions, always assess your security needs and operational constraints. The methods outlined here provide a good starting point, but remember: there‚Äôs no substitute for a well-designed security strategy."
  },
  
  {
    "title": "Reproducible and Scalable VM Cloning on Proxmox-Ansible to the Rescue!",
    "url": "/posts/Proxmox-Ansible/",
    "categories": "Linux, DEVOPS",
    "tags": "homelab, vm, ansible, automation",
    "date": "2024-11-10 10:21:00 -0600",
    





    
    "snippet": "Since my teens, I‚Äôve been fascinated with computers, starting with classics like the Tandy TRS-80 Pocket Computer. That curiosity evolved over the years into a passion for creating and experimentin...",
    "content": "Since my teens, I‚Äôve been fascinated with computers, starting with classics like the Tandy TRS-80 Pocket Computer. That curiosity evolved over the years into a passion for creating and experimenting with virtual machines (VMs). To streamline the process and reduce repetitive tasks, I developed an Ansible setup to automate VM cloning on Proxmox. This guide walks through how to do the same, making VM cloning easier, faster, and more efficient for any level of user.Server to CloneTo streamline the cloning process, I keep a minimal install of Oracle Linux available, specifically set up as a ‚Äúclone-ready‚Äù VM. Normally, this VM stays powered off to save resources, so the first step in cloning is to start it up. Below is an example of an Ansible task to start a Proxmox VM, with sensitive credentials (like api_host, api_user, and api_password) securely stored in an Ansible vault:tasks:  - name: Start the VM    community.general.proxmox_kvm:      api_host: \"\"           # Your Proxmox server IP      api_user: \"\"           # Accessing variables securely from the vault      api_password: \"\"      api_validate_certs: false      node: \"pensask\"                      # Specify your Proxmox node name      vmid: 120                            # VM ID to start      state: started                       # Set VM state to 'started'    delegate_to: localhost  Note: To run this playbook, ensure the proxmoxer and requests Python packages are installed using pip.With the VM running, I can proceed to update it. For more on automating server updates with Ansible, check out this guide on Dev.to. Once updated, I‚Äôll stop the VM with a similar Ansible task before starting the cloning process.Cloning a VM in ProxmoxCloning a VM in Proxmox with Ansible is a straightforward extension of the same start/stop code used earlier. The main change is to replace the state attribute with cloning-specific attributes. Here‚Äôs an example:      clone: Oracle9master.sebostech       # Source VM name to clone      name: NewClone                       # Name for the new VM      pool: CyberSecurity                  # Target pool for the cloned VM      full: true                           # Use 'true' for a full clone  Note: You can find the complete set of Ansible scripts here.Once the Ansible code is executed, a new VM named NewClone will appear in your Proxmox VE.Why Use This Approach?At first glance, using Ansible for VM cloning might seem like overkill, especially when a few clicks in the Proxmox web interface can achieve the same result. However, with just a few more Ansible scripts, you can take this automation further.The beauty of this approach is its repeatability and scalability. The same script used to create a development server can be repurposed to deploy a production server with consistent, reliable results. Plus, it‚Äôs easily extendable, allowing you to customize additional types of servers as needed-stay tuned for more on that!What steps are you taking to automate your VM environment?"
  },
  
  {
    "title": "Dynamic Risk-Based Updates Using Python and Excel",
    "url": "/posts/Dynamic-Updates/",
    "categories": "Linux, DEVOPS",
    "tags": "python, Ansible, Linux, Cybersecurity",
    "date": "2024-11-03 10:21:00 -0600",
    





    
    "snippet": "Learn how to elevate your Ansible update strategy by creating a dynamic, risk-based inventory using Excel üìä and Python üêç. This article walks you through replacing static hosts files with a flexible...",
    "content": "Learn how to elevate your Ansible update strategy by creating a dynamic, risk-based inventory using Excel üìä and Python üêç. This article walks you through replacing static hosts files with a flexible, easy-to-maintain setup that prioritizes updates based on risk levels ‚ö†Ô∏è-keeping your patching process efficient and adaptable üöÄ.In this blog, we‚Äôll take a simple Ansible server update script and turn it into a Risk-Based Update System. Here, servers with the lowest risk get patched first, giving us a chance to test thoroughly before moving on to higher-priority systems.  Ansible Automation:          Dynamic Risk-Based Updates Using Python and Excel‚Äù      Host File      Dynamic Host List      Why Not Use a Hosts File?      The secret sauce? Setting up well-defined groups to make this flow seamlessly. But the real question is: can we pull this off without major changes to our Ansible script from last time? Let‚Äôs find out!Host FileThe host file is at the heart of this change. In the last post, we used a static file grouped by server types. Now, we‚Äôre adding a second layer of grouping by risk level-which does add some complexity to the host file.But here‚Äôs the twist: what if our host file could be dynamically generated from a more generic source? That would keep things flexible and save us from endless file editing!Dynamic Host ListAnsible can work with dynamically created host files, which gives us a more flexible way to keep track of servers. In this example, we‚Äôll use an Excel file to organize our hosts.Example hosts_data.xlsx Structure:            Host Name      Server Environment      Ansible User      Server Type      DNS      Notes                  mint      dev      richard      desktop      desktop.sebostech.LOCAL      Mint desk top              ansible_node      dev      ansible_admin      Ansible      ansible_node.sebostech.local      Development server; Only updates monthly              clone_master      dev      ansible_admin      clone      clone.dev.sebostech.local      Development server; Only updates monthly              mele      staging      richard      nas      nas.stage.sebostech.local      Testing server; Used for application testing              pbs      production      root      backup server      pbs.prod.sebostech.local      Testing server; Used for application testing              pve      production      root      hypervisor      api.stage.sebostech.local      Testing server; Used for application testing              samba      production      richard      nas      nas.prod.sebostech.local      Critical server; Requires daily backup              firewall      production      richard      firewall      firewall.sebostech.local      Critical server; Requires daily backup      Most IT departments already have a list of servers stashed in an Excel file, so why not put it to good use? This approach makes it easy to keep our Ansible hosts organized and up-to-date without constant manual updates.But how does Ansible use the Excel file? Let‚Äôs dive into how we can transform this data into a usable dynamic inventory!## This will run agains all hostansible-playbook -i dynamic_inventory.py playbook.ymlYou can also use  environment variables option to target specific groups, based on Server Environment, Server Type, or even a combination of both:## Just productionSERVER_ENVIRONMENT=\"production\" ansible-playbook -i dynamic_inventory.py playbook.yml --limit \"high:web\"## Just nasSERVER_TYPE=\"nas\" ansible-playbook -i dynamic_inventory.py playbook.yml --limit \"high:web\"## production nasSERVER_ENVIRONMENT=\"production\" SERVER_TYPE=\"nas\" ansible-playbook -i dynamic_inventory.py playbook.yml --limit \"high:web\"Need new groups? Just update the Excel file and adjust the Python script accordingly-easy as that!For a look at the Python code, see here.Why Not Use a Hosts File?When I first started using Ansible, the hosts file was my go-to. But as I added more servers, especially ones with dual roles, that file got more and more complex.Could you use a traditional hosts file to achieve this? Sure-but there are a few drawbacks.With a hosts file, you‚Äôd likely end up with duplicate entries or additional variables to capture all the structure you need. An Excel file, on the other hand, provides a clean, easy-to-maintain structure that keeps things organized.In a corporate environment, there‚Äôs a good chance there‚Äôs already at least one Excel file with a server list, so why not take advantage of it?If you‚Äôd like me to dive deeper into the Python code, just let me know!"
  },
  
  {
    "title": "Crafting a Balanced Patching Strategy",
    "url": "/posts/Balanced-Patching-Strategy/",
    "categories": "Linux, DEVOPS",
    "tags": "Linux, devops, cybersecurity",
    "date": "2024-10-27 10:21:00 -0600",
    





    
    "snippet": "Crafting a Balanced Patching StrategySecurity, Risk, and Automation in HarmonyRegular updates are essential to any cybersecurity model, ensuring that vulnerabilities are quickly patched to minimize...",
    "content": "Crafting a Balanced Patching StrategySecurity, Risk, and Automation in HarmonyRegular updates are essential to any cybersecurity model, ensuring that vulnerabilities are quickly patched to minimize exposure to threats. A common approach is to enable automatic security updates, so servers remain consistently up to date. However, this strategy involves a trade-off: while it reduces vulnerability windows, it also introduces the risk that an update might cause unexpected issues that go unnoticed.  Crafting a Balanced Patching Strategy          Should You Automate?      Risk-Based Patching      Reactive Patching      Security-First Patching      Balancing these risks can be challenging. Delaying updates can extend the time a vulnerability remains active, yet full automation may compromise stability. So, what‚Äôs the best approach? Let‚Äôs explore a middle ground that maximizes both security and reliability.Should You Automate?As discussed above, automation is an effective way to keep your servers updated. In a home lab environment, automated updates ensure systems stay current with minimal oversight. In larger-scale data centers, automation becomes essential-it‚Äôs a cornerstone of a broader strategy that includes Risk-Based Patching to ensure servers remain secure and up to date.Risk-Based PatchingRisk-Based Patching is a strategic approach that aims to minimize the potential impact of problematic patches. This method evaluates the overall business risk if certain servers experience downtime. Servers with the lowest risk are patched first, allowing for thorough testing before moving on to higher-priority systems. This cycle continues until all servers are updated.Automated patching works well in this framework, but what about critical patches that require immediate application?Reactive PatchingSome days, you walk into work with a clear plan and a productive day ahead-until a critical patch needs to be applied immediately, putting everything on hold. Welcome to the world of Reactive Patching.Automation tools like Ansible are invaluable on these days. A robust automation setup should be flexible enough to handle last-minute patches efficiently. So, if automation is in place, why not implement Security-First Patching to ensure essential updates are prioritized?Security-First PatchingMost Linux distributions offer options to apply only security updates, focusing primarily on critical and high-severity vulnerabilities or security flaws. Security-First Patching targets these areas, leaving non-security patches and feature updates aside. While this approach enhances security, it can sometimes lead to unexpected application issues due to missing non-security updates.Each of the above approaches has its own pros and cons, but when combined, they create an effective patching strategy. Regularly applying security patches on a short cycle, along with scheduled full patches, keeps servers consistently up to date.When possible, Risk-Based Patching should be followed: start with development servers, move to staging, and finally, apply patches to production. For Reactive Patching, automated tools should follow this same process, ensuring consistency and minimizing disruption.What‚Äôs one aspect of your current patching strategy that you would improve?DesciptionsExplore effective patching strategies that balance security, risk management, and automation. This guide covers approaches like Security-First, Risk-Based, and Reactive Patching to help keep systems secure and reliable without compromising stability."
  },
  
  {
    "title": "Level Up Your Linux Scheduling",
    "url": "/posts/Linux-Scheduling/",
    "categories": "Linux, DEVOPS",
    "tags": "Linux, systemd, scheduling, devops",
    "date": "2024-10-14 11:59:00 -0600",
    





    
    "snippet": "An Intro to systemd TimersWhen I first began using Linux, crontab was the go-to tool for scheduling tasks on Unix and Linux systems. Its flexibility in a single line of text was impressive, allowin...",
    "content": "An Intro to systemd TimersWhen I first began using Linux, crontab was the go-to tool for scheduling tasks on Unix and Linux systems. Its flexibility in a single line of text was impressive, allowing you to create highly specific schedules. For instance, if you wanted a task to run on the 15th of each month at 5:45 am but only on Mondays, crontab could handle it effortlessly with the simple format: 45 5 15 * 1, followed by your task command. In 2010, however, systemd emerged, it offering an alternative method for scheduling tasks, adding new capabilities to Linux systems.  Level Up Your Linux Scheduling: An Intro to systemd Timers          What is systemd      Difference between systemd and crontab      Times File      Service File      Scheduling  Timer      What is systemdSystemd is a service management tool that takes over once the boot process is complete, initiating and managing essential processes during system startup and throughout runtime. One of its key features is the ability to schedule tasks, similar to what crontab offers, but with additional capabilities and greater flexibility for system administrators.Difference between systemd and crontabOne fundamental difference between cron jobs and systemd timers is how the scheduled tasks are managed. With cron, tasks are user-specific, and identifying which user scheduled a particular job isn‚Äôt always straightforward. In contrast, systemd clearly defines the user who will run a service within the [Service] section of the service file.Times FileTo schedule a process with systemd, you‚Äôll need two configuration files: a timer file and a service file. The timer file defines when a process will run and follows the same initialization (INI) file format as most systemd files, organized into sections.  The first section, [Unit]          describes what the process will do and may contain sequencing conditions to ensure that required processes are running before this one starts.        Next, the [Timer] section          specifies when the systemd process should run. This section includes functions similar to crontab,      but with additional options such as OnBootSec=15min, which starts the process 15 minutes after boot.      If the timer and service files have different names, the service can be referenced directly in this section.        Finally, the [Install] section          provides options for additional configurations, such as ensuring the process starts on boot or specifying the target runlevel.      Service FileTo start a process or daemon service with systemd, you create a service file. Like the timer file, the service file is organized into three main sections: [Unit], [Install], and [Service].  The [Unit] and [Install] sections function similarly to those in the timer file, handling descriptions and installation options.  The [Service] section, however, is where the main task is defined.Below is an example [Service] configuration for running an Ansible script that performs system updates:[Service]# Service call to perform updatesUser=ansible_adminWorkingDirectory=/opt/ansible/projects/ExecStart=/usr/bin/ansible-playbook ansible_update.ymlThis setup specifies the following:  The User directive determines which user the service runs as.  WorkingDirectory defines the startup directory for the process.  ExecStart calls the Ansible script.All that is needed is to start the schedule.Scheduling  TimerTo start and manage timers, systemd uses the systemctl command to enable, start, or disable timers. For instance, to enable and start a timer immediately, you would use:systemctl enable --now &lt;service_timer.timer&gt;You can view all scheduled timers with systemctl --list-timers, which displays timers currently set to run.For the code and configuration of the systemd timer and service created to run an Ansible script for Linux updates, see this code example and read the article here.This article provided a high-level overview of using systemd timers, though systemd offers a wide range of powerful options beyond what we‚Äôve covered here.When I first heard about systemd timers, I questioned the need to change such a reliable task scheduler as crontab. However, after experiencing systemd timers, I‚Äôve become a true convert.Have you started using systemd timers to schedule tasks? If not, what scheduling tool are you currently using?"
  },
  
  {
    "title": "Getting Started with Ansible - Automation Meets Cybersecurity",
    "url": "/posts/Getting-Started-with-Ansible/",
    "categories": "ProxoxVE, Cybersecurity, Ansible",
    "tags": "Linux, Ansible, Cybersecurity, Automation",
    "date": "2024-10-14 11:59:00 -0600",
    





    
    "snippet": "Ansible SetupIn this article, I will walk you through setting up the Ansible Control server.In future articles,we will create Ansible playbooks to deploy Zero Trust Access (ZTA) principles across m...",
    "content": "Ansible SetupIn this article, I will walk you through setting up the Ansible Control server.In future articles,we will create Ansible playbooks to deploy Zero Trust Access (ZTA) principles across my home lab environment.So, what exactly is Ansible?Table of content:  What is Ansible  The Chicken-and-Egg Problem   Building the Control Server  Getting Hostnames Using AnsibleWhat is AnsibleAnsible is an automation tool used to execute tasks on remote servers. It relies on YAML-formatted files, known as Playbooks, to define these tasks, which are then pushed and executed on the remote servers using SSH.At its core, a Playbook is a simple list of tasks that are run against a set of servers by a specified user.Where to Start: The Chicken-and-Egg ProblemSetting up the Ansible Control server with Zero Trust Access (ZTA) in mind requires a few additional steps:  Creating a VLAN for the Ansible server  Configuring the network settings on the Ansible server  Setting firewall rules to allow access  Creating and managing users for the Ansible server  And a few other tasks along the wayThe big question is: Do I set everything up manually, including ZTA changes, or do I first set up a basic Ansible server and use it to automate the migration to a ZTA model?Time to buildBuilding the Control ServerI have decided to go with the latter approach since it aligns with the philosophy of building cybersecurity into the process from the start. Here is how I set up the Ansible Control server on a fresh installation of Oracle Linux 9.Step 1: Install Ansible on Oracle Linux 9First, ensure your system is up to date:# Check for updates first  sudo dnf update  Next, install Ansible:# Install Ansible  sudo dnf install ansible  To confirm the installation, run the following command:# Test if Ansible is installed  ansible  Step 2: Add an Ansible Admin UserWe need to create an admin user for Ansible:# Create a user with sudo privileges for Ansible on remote serverssudo useradd ansible_admin  Step 3: Grant Sudo Access to the Ansible UserTo allow the ansible_admin user to run commands with sudo privileges without entering a password, we will modify the sudoers file. Depending on the system, hereis where you can find the file:  OPNsense: /usr/local/etc/sudoers  RHEL, Debian: /etc/sudoersAdd the following line near the bottom of the file:## Grant sudo access without a password to the Ansible user  ansible_admin ALL=(ALL) NOPASSWD: ALL    Note:In the future, I‚Äôll be setting up multiple Ansible users with more granular permissions to enhance server security.4: Create the Ansible Hosts FileThe Ansible hosts file defines the servers you want to manage. There are several places where you can store this file, but I‚Äôll use the main configuration:# Edit the main hosts file  sudo nano /etc/ansible/hosts  Below is the content I used for my setup:[firewall]  # OPNsense Firewall  Firewall  [proxmox]  # Proxmox VE and Backup Server  PVE  PBS  [nas]  # Orange Pi NAS  NAS  [ansible_node]  # Local Control Node  control_node  Now That the Build is Complete, Let‚Äôs Test It Out Getting Hostnames Using AnsibleWith Ansible installed and the servers configured, let‚Äôs test it by running a simple playbook that gathers the hostname and OS type from all the connected servers.Step 1: Create the Ansible Playbook  Start by creating a new file called system_info.yml:    nano system_info.yml          Add the following lines to the file and save:```yaml‚Äî          name: Gather hostname and OS type from all hostshosts: allgather_facts: yestasks:                  name: Get hostname and OS typeansible.builtin.debug:  msg: ‚ÄúHost: , OS: ‚Äú```                    Step 2: Run the PlaybookTo execute the playbook, use the following command:ansible-playbook system_info.yml  Step 3: Review the OutputAnsible can be quite verbose, but the key part of the output will look like this:TASK [Get hostname and OS type] **********************************************************************************************************************************************ok: [firewall] =&gt; {      \"msg\": \"Host: firewall, OS: FreeBSD\"  }  ok: [pve] =&gt; {      \"msg\": \"Host: pensask, OS: Debian\"  }  ok: [pbs] =&gt; {      \"msg\": \"Host: bkp1, OS: Debian\"  }  ok: [ansible] =&gt; {      \"msg\": \"Host: stock, OS: RedHat\"  }  ok: [samba] =&gt; {      \"msg\": \"Host: orangepi5plus, OS: Debian\"  }  Step 4: Final ThoughtsSome of the hostnames could use a bit of work, but no worries-Ansible can help streamline those details later. This playbook confirms that the servers are reachable and that Ansible is properly configured to gather system information from all the hosts.With Ansible up and running, the next step is to start adding additional layers of security to the remote servers using Ansible. Stay tuned-there‚Äôs more to come!Ansible is a powerful automation tool that simplifies complex tasks, making server management more efficient.  If you had an automation tool like Ansible, what tasks would you automate?"
  },
  
  {
    "title": "Automating Server Updates",
    "url": "/posts/Automating-Server-Updates/",
    "categories": "ProxoxVE, Cybersecurity",
    "tags": "ansible, devops, Linux, cybersecurity",
    "date": "2024-10-14 11:59:00 -0600",
    





    
    "snippet": "A Simple Yet Scalable ApproachRegular OS patching is a crucial part of maintaining cybersecurity, as it helps reduce the risk of zero-day vulnerabilities and other security issues by ensuring that ...",
    "content": "A Simple Yet Scalable ApproachRegular OS patching is a crucial part of maintaining cybersecurity, as it helps reduce the risk of zero-day vulnerabilities and other security issues by ensuring that patches are applied soon after they‚Äôre released. However, when updates aren‚Äôt automated, it leaves a window where vulnerabilities remain active, making the decision to automate or not a difficult one. In my home lab, I work with a variety of operating systems, each using different update methods, which makes finding a scalable patching solution increasingly important as my lab expands. This is why I‚Äôve decided to use Ansible for managing updates‚Äîlet me explain why it‚Äôs the right tool for the job.Table of content:  Automating Server Updates: A Simple Yet Scalable Approach          Understanding Ansible      The Update Tasks      What User Will Run the Tasks      Where to Run the Playbook      Understanding AnsibleAnsible is a powerful automation tool that simplifies repetitive tasks and system management. For a more in-depth overview, click here. At its core, Ansible uses Playbooks to execute one or more tasks in sequence. Each task typically consists of a name, a module, arguments for that module, and‚Äîwhen it comes to updates‚Äîcertain conditions to determine when the task should run. While Ansible offers many other advanced features, we‚Äôll be focusing on this basic structure for now.The Update TasksIn my home lab, I‚Äôm working with three different operating systems: two Linux distributions and one FreeBSD system. To handle updates for all of them, the Ansible Playbook needs to include tasks tailored to each OS. For Red Hat-based distributions like RHEL, CentOS, Rocky, and Oracle, I use the yum module. Debian-based systems, such as Ubuntu and Mint, require the apt module, and for FreeBSD, the pkgng module does the job. Thankfully, Ansible has built-in modules for all of these package managers, making it easy to manage updates across different platforms.Here‚Äôs how the Playbook is structured:  tasks:    # Task to update RedHat family servers (RHEL, CentOS, Rocky, Oracle)    - name: Update RHEL family servers, including RHEL, CentOS, Rocky, Oracle...      yum:        name: '*'                   # Update all packages        security: yes               # Apply security updates        state: latest               # Ensure packages are at their latest version      when: ansible_facts['os_family'] == 'RedHat'  # Run only on RedHat family systems    # Task to update Debian family servers by refreshing package cache    - name: Update Debian family servers      apt:        update_cache: yes           # Refresh the APT package cache      when: ansible_facts['os_family'] == 'Debian'  # Run only on Debian family systems    # Block to handle updates for OPNsense (FreeBSD-based system)    - block:        # Task to check for available updates on OPNsense        - name: Check for updates on OPNsense          ansible.builtin.shell: \"opnsense-update -c\"          register: update_check     # Register output for conditional check          changed_when: false        # Mark as not changed (no need to record as a change)          ignore_errors: true        # Continue playbook if this command fails        # Task to apply updates if updates are available        - name: Update OPNsense if updates are available          ansible.builtin.shell: \"opnsense-update -u &amp;&amp; opnsense-update -bk\"          when:            - ansible_facts['os_family'] == 'FreeBSD'  # Run only on FreeBSD systems (OPNsense)            - update_check.stdout != \"\"                # Ensure updates are available before proceeding          register: update_result                      # Register result for reboot condition        # Task to reboot OPNsense after updates        - name: Reboot OPNsense to apply updates          ansible.builtin.shell: \"reboot\"          async: 1                                     # Run asynchronously to allow reboot          poll: 0                                      # Detach from task immediately          when:            - ansible_facts['os_family'] == 'FreeBSD'  # Run only on FreeBSD systems (OPNsense)            - update_result is defined                 # Ensure update task has a result            - update_check.stdout != \"\"                # Check if updates were available          ignore_errors: true                          # Continue playbook if reboot task fails      when: ansible_facts['os_family'] == 'FreeBSD'    # Apply this block only on FreeBSD systemsNow that we know what needs to be done, the next step is figuring out under which user or permissions we‚Äôll be executing these tasks.What User Will Run the TasksThe next step is determining which user the tasks in the Playbook will run as on the remote servers. In my setup, I‚Äôve already created an Ansible user on each of the remote servers, so we‚Äôll be using that user to execute the tasks.  become: root  become: yes  remote_user: ansible_admin  Note: The ansible_admin user was set up in a previous article, which you can check out here.Now that we have the user sorted, the final step is deciding where to run the Playbook.Where to Run the PlaybookThe final piece of the puzzle is deciding which servers the tasks will run on. This is defined in the Ansible hosts file, which lists all the servers to be managed. In my home lab, the Playbook runs across all servers.- name: Update servers in my home lab  hosts: allIn a corporate environment, you would typically follow a more structured patching strategy, rolling out updates in stages. For more information on corporate patching strategies, you can learn more here.With the Ansible Playbook in place, the next step is to set up a systemd timer to schedule the job, ensuring my servers are consistently kept up to date. While there may be cases where a reboot is necessary after applying updates, that will be addressed in a future project.You can find the complete code for this article here, including the systemd timer setup to automate the Playbook execution.How do you manage patching to keep your servers up to date?"
  },
  
  {
    "title": "Streamlining SSH Key Management",
    "url": "/posts/Streamlining-SSH-Key-Management/",
    "categories": "SSH, Auth Keys",
    "tags": "SSH, servers, cybersecurity",
    "date": "2024-10-13 13:12:00 -0600",
    





    
    "snippet": "Automating and Securing SSH Configurations with a Custom ScripWhen managing multiple servers on a daily basis, SSH authentication keys are invaluable. In the past, I would typically create a single...",
    "content": "Automating and Securing SSH Configurations with a Custom ScripWhen managing multiple servers on a daily basis, SSH authentication keys are invaluable. In the past, I would typically create a single key pair and reuse it across all my servers. While convenient, this approach poses a significant security risk‚Äîif that key were to be compromised, it could expose all of my servers to potential threats. To mitigate this risk, generating unique key pairs for each server is the best practice, but managing all those keys can quickly clutter the .ssh directory. To address this, I developed a Bash script that not only generates key pairs but also keeps the directory organized and simplifies the process overall.the .ssh/configSSH relies on the .ssh/config file to store information about various SSH connections. In this file, you define several key parameters:  Host ‚Äì The alias used to reference the server.  HostName ‚Äì The server‚Äôs IP address or DNS name.  User ‚Äì The username SSH will use to log in.  IdentityFile ‚Äì The path to the SSH key pair for authentication.By using include files within the SSH config, it becomes much easier to organize multiple server logins. The script I developed automates this process by creating individual config files for each server and linking them back to the main .ssh/config file. Here‚Äôs how the script works:How the Script WorksThe script takes a hostname, IP address, and username as inputs. It then generates the necessary SSH keys, creates a config file for the server, and automatically links that file to the main .ssh/config file.Server DirectoryCreates the directory to store the SSH key pair and configuration file# Define directoriesconfig_directory=/home/${local_user}/.ssh/include.d/${host_name}echo \"${config_directory}\"# Check if the SSH config directory exists, create it if notif [ ! -d \"${config_directory}\" ]; then  echo \"Creating SSH config directory at ${config_directory}...\"  mkdir -p ${config_directory}  if [ $? -ne 0 ]; then    echo \"Error: Failed to create config directory at ${config_directory}\"    exit 1  fifiFile PermissionsTo secure the keys generated in the next steps, the default file permissions must be set to 600. This ensures that only the file owner has read and write access. The following code sets the correct permissions on the parent directory to maintain this level of security.# Set default ACL permissions on the login directorysetfacl -d -m u::rw,g::-,o::- ${config_directory}if [ $? -ne 0 ]; then  echo \"Error: Failed to set ACL on ${config_directory}\"  exit 1fiGenerates the KeysWith a designated place to store the keys, the script proceeds to generate the SSH key pair and copy them to the server  Note: The script does not prompt for a password when generating the key pair, but it will ask for the remote server‚Äôs login password during the connection process.# Generate a new SSH key pair using ed25519 algorithm, with no passphrase (-N \"\")ssh-keygen -t ed25519 -f ${config_directory}/${host_name} -N \"\"  # Empty passphraseif [ $? -ne 0 ]; then  echo \"Error: SSH key generation failed for ${host_name}\"  exit 1fi# Copy the SSH key to the remote hostecho \"Copying SSH public key to ${user}@${ip_address}...\"ssh-copy-id -i ${config_directory}/${host_name}.pub ${user}@${ip_address}if [ $? -ne 0 ]; then  echo \"Error: Failed to copy SSH public key to ${user}@${ip_address}\"  exit 1fi.ssh config File for ServerThe config file for the new login is created and stored in the same directory as the SSH keys.# Create the config file inside the config_directory and write the necessary SSH config linesecho \"Creating SSH config file at ${config_directory}/config...\"cat &lt;&lt;EOL &gt; ${config_directory}/configHost ${host_name}     HostName ${ip_address}     User ${user}     IdentityFile ${config_directory}/${host_name}EOLMaking Server CallableOne of the key benefits of using a config file is that it simplifies connecting to servers. By linking the server‚Äôs address and username to a single alias, you can easily initiate a connection with a simple ssh &lt;alias&gt; command, eliminating the need to remember or type the full server details each time.The next step adds the server‚Äôs config file to the main .ssh/config, ensuring it‚Äôs included for future SSH connections.# Append the Include line to ~/.ssh/config if it's not already presentssh_config_file=/home/${local_user}/.ssh/config# Ensure the ~/.ssh/config file existsif [ ! -f \"${ssh_config_file}\" ]; then  touch \"${ssh_config_file}\"  chmod 600 \"${ssh_config_file}\"fi# Check if the Include line is already in the fileif ! grep -Fxq \"Include ${config_directory}/config\" \"${ssh_config_file}\"; then  echo \"Adding 'Include ${config_directory}/config' to ${ssh_config_file}...\"  #echo \"Include ${config_directory}/config\" &gt;&gt; \"${ssh_config_file}\"  echo \"Include ${config_directory}/config\" | cat - \"${ssh_config_file}\" &gt; temp_file &amp;&amp; mv temp_file ${ssh_config_file}  if [ $? -ne 0 ]; then    echo \"Error: Failed to append the Include line to ${ssh_config_file}\"    exit 1  fifiTest the ConnectionFinally, it test the login:# Try to SSH into the server using the newly created keyssh ${host_name}if [ $? -ne 0 ]; then  echo \"Error: Failed to connect to ${host_name}\"  exit 1fiecho \"SSH key successfully created, user logged into the server, and config files updated.\"Don‚Äôt forget to logout the remote server at the end of the script.Creating a new authentication key and login is simple:Usage: create_ssh_login.bash &lt;host_name&gt; &lt;ip_address&gt; &lt;username&gt;Once the script completes, you can easily connect to the server with:ssh &lt;hostname&gt;Running it for:create_ssh_login.bash proxmox 192.168.177.7 richard  will create the blow directory structure.~/.ssh‚îú‚îÄ‚îÄ config‚îú‚îÄ‚îÄ include.d/‚îÇ   ‚îî‚îÄ‚îÄ proxmox/‚îÇ      ‚îú‚îÄ‚îÄ proxmox.pub‚îÇ      ‚îú‚îÄ‚îÄ proxmox‚îÇ      ‚îú‚îÄ‚îÄ configWhat I appreciate about this script is that it enhances SSH key security while making it easier to generate keys and keep them well-organized.For the full version of the script, visit https://github.com/richard-sebos/Streamlining-SSH-Key.What steps are you taking to secure your SSH connections?"
  },
  
  {
    "title": "Ansible and Cybersecurity",
    "url": "/posts/Ansible-Cybersecurity/",
    "categories": "ProxoxVE, Cybersecurity",
    "tags": "Ansible, Linux, Cybersecurity, ZTA",
    "date": "2024-10-10 19:11:00 -0600",
    





    
    "snippet": "CybersecurityIn today‚Äôs world, cybersecurity is more critical than ever. Whether it‚Äôs adding new functionality, upgrading existing features, or auditing current systems, the question of how to enha...",
    "content": "CybersecurityIn today‚Äôs world, cybersecurity is more critical than ever. Whether it‚Äôs adding new functionality, upgrading existing features, or auditing current systems, the question of how to enhance security always comes up.How many of us have been disappointed when a solution doesn‚Äôt address security concerns due to time constraints or budget limitations? Is there a way to integrate security into the core of our solutions from the start?And how do you achieve this when working with virtual machines (VMs) in a distributed architecture?Cybersecurity and Automation ToolsCybersecurity focuses on the processes and procedures that protect corporate infrastructure from attacks. When breaches do occur, these processes help limit the attacker‚Äôs access and minimize damage. Automating these processes can significantly reduce the cost of maintaining a secure environment.Automation tools like Ansible, Puppet, Chef, SaltStack, and Terraform are widely used in the industry, with Ansible and Terraform being particularly prominent leaders. In this article, I will focus on strategies that can be implemented using Ansible to enhance cybersecurity while lowering costs.What is AnsibleAnsible is an automation tool used to execute tasks on remote servers. It relies on YAML-formatted files, known as Playbooks, to define these tasks, which are then pushed and executed on the remote servers using SSH.At its core, a Playbook is a simple list of tasks that are run against a set of servers by a specified user.But how does that work?Ansible MindsetLike any tool, Ansible can be used as simply or as complexly as you need. Personally, I approach Ansible with a ‚Äúto-do list‚Äù mindset. Breaking down tasks into small, manageable steps and then linking them into larger workflows has always been the most effective approach for me.For instance, if I need to make a change to a server, such as installing and configuring a Samba service on a VM, I break it down into a series of steps:  Take a snapshot of the VM  Update the repositories on the Linux server  Apply any necessary updates  Reboot, if required  Set up new users or groups  Install Samba  Configure the server files  Create new directories for shared folders  Assign permissions to shared folders  Add firewall rules  Start the Samba service  Test the Samba server  Remove the snapshot once everything is workingEach of these steps can be divided into one or more tasks, which can then be combined into a role within a Playbook. A series of roles or tasks can be grouped together in a Playbook to handle the entire process of installing Samba.However, simply following these steps doesn‚Äôt inherently improve security.Adding Security to the ProcessWhen security is integrated into this process and included in Ansible scripts, it ensures that all new installations consistently follow established security protocols. For example:  Samba configuration files adhere to a predefined enterprise standard.  Copies of configuration files are stored in a central repository for auditing.  User and group setups follow enterprise procedures.  Checkpoints are added to verify that:          Only authorized users have access to new shared folders.      The firewall is enabled, and necessary rules are applied.      The Ansible script created for installing Samba can easily be adapted to install Apache, with the base security measures already in place. This means only the Apache-specific changes are needed, saving time while maintaining a high level of security.Once these security measures are embedded into the scripts, every future execution will automatically follow these security standards. While it‚Äôs still a manual process to verify that these security protocols are followed, that‚Äôs where code reviews come in.So Why Do This?Security models like Zero Trust Access (ZTA) introduce additional steps to ensure that solutions are secure. If these steps aren‚Äôt followed, new attack surfaces can be created, which attackers can exploit.By integrating the security models into your automation tool, you incur a one-time development effort and minimal ongoing cost. Once the models is in place, security becomes an integral part of the process.This approach also ensures a consistent security models across the enterprise. As security needs evolve, the tools required for deployment are already in place, making it much easier to roll out updates and maintain security standards.What are you doing to integrate security into your enterprise solutions?"
  },
  
  {
    "title": "Proxmox Network Storage Firewall Rules",
    "url": "/posts/Proxmox-Network-Storage-Firewall/",
    "categories": "Proxmox, Firewall",
    "tags": "proxmox, NAS, storage, Firewall",
    "date": "2024-09-26 19:41:00 -0600",
    





    
    "snippet": "Proxmox Network StorageSo far, we‚Äôve utilized a Direct Attached Storage (DAS) and a Single Board Computer (SBC) to set up a NAS (Network Attached Storage) using Samba. The next step was securing th...",
    "content": "Proxmox Network StorageSo far, we‚Äôve utilized a Direct Attached Storage (DAS) and a Single Board Computer (SBC) to set up a NAS (Network Attached Storage) using Samba. The next step was securing the NAS by placing it on a dedicated VLAN subnet, isolating it from other devices in my home lab. This setup effectively reduces the attack surface and limits exposure to only the systems that need access. Now, we‚Äôll focus on configuring firewall rules to ensure secure communication between the authorized devices and the NAS.Firewall AliasesWhen setting up firewall rules, you‚Äôre assigning permissions for devices to access other devices. This can be done by specifying attributes such as MAC addresses, IP addresses, ports, and Fully Qualified Domain Names (FQDNs). However, managing these values, especially MAC addresses or IPs, can become cumbersome and difficult to remember after creating the rules.For example, the dock for my MacBook Air has the MAC address 3A:2C:4F:5E:8B:1A. Instead of trying to remember that, I can create an alias called macbookair_dock on the firewall. Now, whenever I use macbookair_dock in my firewall configuration, the system will recognize it as the corresponding MAC address 3A:2C:4F:5E:8B:1A, making it easier to manage.For this project, I‚Äôve created the following aliases:            Alias      Type      Value                  macbookair_dock      MAC address      3A:2C:4F:5E:8B:1A              iPad      MAC address      F2:77:3C:6D:1E:82              iPhone      MAC address      09:AF:BA:63:92:4E              ProxmoxVE      Host(s)      192.168.177.7,192.168.177.8              ProxmoxBS      Host(s)      192.168.177.129              SambaPorts      Port(s)      137,138,139,445              SambaPve      Host(s)      192.168.197.7              SambaMedia      Host(s)      192.168.198.8      With these aliases in place, the firewall rules we create in the next section will be much clearer and easier to manage.  Note: MAC vs HostsThe servers on my network have static IP addresses, so using IP address would work well for them. However, many other devices do not have static IPs, making their IP addresses less reliable for identification. In these cases, using the MAC address (Media Access Control address) provides a more consistent and stable way to reference those devices.Proxmox Servers to NASThe Proxmox servers will be utilizing the NAS for two main purposes:  ISO storage  BackupsTo enable this, the following firewall rules were necessary:            #      Interface      Source      Destination      Ports                  1      VLANforProxmox      ProxmoxVE      SambaPve      SambaPorts              2      VLANforProxmox      ProxmoxBS      SambaPve      SambaPorts      Rule 1:This rule permits the Proxmox servers to store ISO images on the NAS and ensures that virtual machines (VMs) can access the necessary storage resources for proper operation.Rule 2:This rule enables the Proxmox Backup Server to securely store backups on the NAS, ensuring that data is protected and recoverable as needed.Virtual Servers to NASThe virtual servers will use the NAS for shared storage. To facilitate this, the following firewall rules are required:            #      Interface      Source      Destination      Ports                  1      VLANforProxmoxVM      ProxmoxVE      SambaPve      SambaPorts      WiFi to MediaThis setup allows my iPhone and iPad to access documents and media stored on the NAS. The following firewall rules were implemented to ensure secure access:Since access was not granted to the entire WiFi subnet, any additional users connecting to the WiFi network will not have access to the files on the shared NAS. This ensures a higher level of security by restricting access to only authorized devices.At this point, the Samba service has been installed, the VLANs are configured, and the firewall rules are in place. In the next article, I will cover mounting external hard drives and configuring Samba to further restrict access to the shared resources."
  },
  
  {
    "title": "Implementing VLANs for Proxom and NAS",
    "url": "/posts/Proxmox-NAS-VLANs/",
    "categories": "ProxoxVE, Cybersecurity, NAS",
    "tags": "proxmox, nas, VLAN, security",
    "date": "2024-09-23 17:56:00 -0600",
    





    
    "snippet": "Proxmox: Network for NASIn the previous post, we completed the Samba server setup, though the network connection strategy was still in development. Now, it‚Äôs time to explore how the Samba server wi...",
    "content": "Proxmox: Network for NASIn the previous post, we completed the Samba server setup, though the network connection strategy was still in development. Now, it‚Äôs time to explore how the Samba server will be integrated into the network to fulfill various roles:  Proxmox VE/Backup Server (BS): Providing dedicated storage for backups and ISO images  Proxmox VMs: Enabling shared storage for virtual machines in the Proxmox environment  Personal Use: Serving as a central hub for managing documents and mediaIn this post, I‚Äôll guide you through setting up the network for this homemade NAS, ensuring it‚Äôs ready to meet the needs of both your Proxmox environment and personal use.Bridging vs. VLANWhen setting up the Samba server, I had two network cards at my disposal. Initially, I considered bridging the connections, with one card linked to the Proxmox network and the other connected to WiFi. However, I wanted to isolate the networks for better security, but I ran into a limitation‚Äîthere weren‚Äôt enough physical ports.That‚Äôs where VLANs came in.Between the two options, VLANs offered a more secure and scalable solution. Ultimately, I chose VLANs to segment and secure the network effectively.VLANThe simplest way to understand VLANs is to think of them as virtual networks that share the same physical network interface but remain logically separated. Through a process called ‚Äútagging,‚Äù VLANs add an identifier to each network packet, ensuring that data is routed only to the devices within the same VLAN. This isolation boosts security and improves traffic management, especially in environments with multiple devices and different access requirements.In my case, I created two VLANs on two of the four LAN ports on my firewall to segment the traffic:  VLAN Tagging: Each network packet is tagged with a VLAN ID, ensuring it stays within its designated network, preventing interference with other VLANs.  Improved Security and Control: VLANs allow me to separate traffic, ensuring that my Proxmox network, personal devices, and other systems are isolated from each other. This limits access and reduces the risk of unauthorized traffic.  Efficient Use of Hardware: Instead of needing multiple physical network interfaces for each network, VLANs enable me to use a single interface to handle multiple networks, maximizing the efficiency of my existing hardware.Here‚Äôs how I set up VLANs in my configuration:Network IsolationTo enhance the security and organization of my Proxmox servers and NAS, I created four VLANs on my OPNsense firewall to ensure each component of the infrastructure is properly isolated:  VLAN 10 - Proxmox Management          Used for accessing the Proxmox web interface      Handles backup traffic between Proxmox VE and Proxmox Backup Server        VLAN 20 - Proxmox VMs          A dedicated subnet for virtual machines        VLAN 30 - NAS for Personal Use          Designed for managing personal documents and media        VLAN 40 - NAS for Proxmox          Reserved for backups and ISO storage      Used by VMs to access common files      With these VLANs in place, I ensured that each type of traffic is isolated, improving both security and performance. The next step was configuring the servers to access their respective VLANs.Configuring the Physical ServersNext, I configured the Proxmox VE, Proxmox Backup Server (BS), and Orange Pi 5 Pro to use the VLANs. Interestingly, while all of these systems are based on Debian, the configuration process differed for each.Proxmox Backup Server (BS)Proxmox Backup Server was the simplest to configure since it only required a single connection to the NAS for storing backups. The VLAN setup was straightforward, as it primarily handled backup traffic without any complex routing needs.auto loiface lo inet loopback# Management network (VLAN 10, 192.168.177.0/24)auto eno1.10iface eno1.10 inet static    address 192.168.177.129    netmask 255.255.255.0    gateway 192.168.177.1    vlan-raw-device eno1  This configure the backup server to use VLAN 10Orange Pi NASSince the Orange Pi 5 Pro has two network interfaces, I had a few options for configuring the VLANs:  Option 1: Set up VLANs on one network card and dedicate the other for administrative access.  Option 2: Create a redundant network setup using both interfaces for failover and load balancing.  Option 3: Separate the VLANs across both network cards, which is the approach I chose.This setup allows for better network segmentation and isolation between traffic types. Below are the configuration files I used to implement this setup:  10-enP3p49s0.network```[Match]Name=enP3p49s0[Network]VLAN=vlan30&gt; 10-enP4p65s0.network[Match]Name=enP4p65s0[Network]VLAN=vlan40&gt; 20-vlan30.network[Match]Name=vlan30[Network]Address=192.168.198.8/24Gateway=192.168.198.1DNS=8.8.8.8 8.8.4.4&gt; 20-vlan30.netdev[NetDev]Name=vlan30Kind=vlan[VLAN]Id=30&gt; 20-vlan40.network[Match]Name=vlan40[Network]Address=192.168.197.7/24Gateway=192.168.197.1DNS=8.8.8.8 8.8.4.4&gt; 20-vlan40.netdev[NetDev]Name=vlan40Kind=vlan[VLAN]Id=40### Proxmox VESimilar to the Orange Pi 5 setup, the Proxmox VE server also has two network cards. I decided to separate the VLANs, assigning each VLAN to its own network interface for better isolation and performance. Below is the configuration file for this setup:auto loiface lo inet loopbackauto enp11s0iface enp11s0 inet manualauto vmbr0iface vmbr0 inet static        bridge-ports enp11s0        bridge-stp off        bridge-fd 0        bridge-vlan-aware yes        bridge-vids 2-4094auto vmbr0.10iface vmbr0.10 inet static\taddress 192.168.177.7/24\tgateway 192.168.177.1auto ens5iface ens5 inet manualauto vmbr1iface vmbr1 inet static        bridge-ports ens5        bridge-stp off        bridge-fd 0        bridge-vlan-aware yes        bridge-vids 2-4094auto vmbr1.20iface vmbr1.20 inet static        address 192.168.178.8/24        gateway 192.168.178.1```Now that the Samba server and network setup is complete, in the next post, I‚Äôll be covering the firewall rules that control access to the NAS. These rules will ensure that each system can access the NAS as needed while maintaining proper security and isolation."
  },
  
  {
    "title": "Proxmox Network Storages",
    "url": "/posts/Proxmox-Network-Storage/",
    "categories": "Proxmox, Samba",
    "tags": "proxmox, NAS, storage",
    "date": "2024-09-15 19:11:00 -0600",
    





    
    "snippet": "When I imagine the future of my home lab, I envision it filled with professional-grade equipment‚Äîa sleek server rack, servers humming away, and network switches blinking behind a perforated door. H...",
    "content": "When I imagine the future of my home lab, I envision it filled with professional-grade equipment‚Äîa sleek server rack, servers humming away, and network switches blinking behind a perforated door. However, like many enthusiasts, my current setup is a bit more modest, made up of a mix of end-of-life workstations and desktops.But that‚Äôs part of the fun, right? Taking bits and pieces of hardware and transforming them into something practical and useful. That‚Äôs the beauty of a home lab.In this article series, I‚Äôll walk you through how I built a Network Attached Storage (NAS) solution using a DAC (Direct Access Storage) and a Single Board Computer (SBC). While this may not be the most conventional approach, it was certainly a fun and rewarding experience!The SBC NASAt its core, a NAS (Network Attached Storage) is simply a device that shares storage across a network. For my setup, I had an Orange Pi 5 Pro that I‚Äôd been experimenting with, and since it features two network interfaces, it was a perfect candidate for the NAS device to connect to the DAC.One network interface would serve Proxmox, acting as the NAS for my virtual environment, while the other would be dedicated to my personal media storage. I set up the Orange Pi with a headless Debian server and connected it to the DAC, using Samba to share the storage across the network.And just like that, I had something that was almost a fully functional NAS!To Bridge or Not to BridgeThe Orange Pi 5 Pro comes with two network interfaces, and when I first envisioned this project, I considered bridging them. However, bridging the two would essentially connect two subnets that are normally separated by a firewall, bypassing that isolation.This led to an important question: did I really want to bridge these two networks on a Debian SBC? After weighing the pros and cons, I had to carefully consider whether merging these networks would compromise the structure and security of my home lab.Security Concerns with the BridgeAlthough my home lab isn‚Äôt a production-level environment, I try to approach it with the same security considerations. By bridging the networks, I realized a couple of important access concerns:  From a Proxmox and personal device perspective, bridging effectively gave both access to the NAS.  From a NAS perspective, it now had access not only to the Proxmox server and VMs running on it, but also to any personal devices I use to access media.This raised a significant question: was I compromising security by bridging these networks?In my next post, I‚Äôll walk through the steps I took to secure access to the NAS and maintain proper isolation.  /etc/samba/smb.conf  ```[global]    workgroup = sebos    log file = /var/log/samba/smb.log    max log size = 10000    log level = 1    server string = sebos nas %v    security = user    min protocol = SMB2# Include additional configuration files [media]include = /etc/samba/smb.d/media.conf [pve]include = /etc/samba/smb.d/pve.conf&gt; /etc/samba/smb.d/media.conf&gt;comment = home media serverpath = /srv/nas_storage/mediabrowseable = yeswritable = yesguest ok = nocreate mask = 0664directory mask = 0775force user = samba_mediaforce group = samba_admin_mediavalid users = samba_media ```  /etc/samba/smb.d/pve.conf      comment = pve storage    path = /srv/nas_storage/pve    browseable = yes    writable = yes    guest ok = no    create mask = 0664    directory mask = 0775    force user = samba_pve    force group = samba_admin_pve    valid users = samba_pve  "
  },
  
  {
    "title": "Proxmox NAS Storage - Securing a Samba DAS",
    "url": "/posts/Securing-Samba-DAS/",
    "categories": "Proxomx, NAS, Samba",
    "tags": "samba, cybersecurity, NAS, linux",
    "date": "2024-09-08 15:22:00 -0600",
    





    
    "snippet": "With the network and firewall rules configured, there are just a few final tasks to complete on the Samba server:  Enhance Samba security  Mount the DAS volumes on the serverWhile this may seem red...",
    "content": "With the network and firewall rules configured, there are just a few final tasks to complete on the Samba server:  Enhance Samba security  Mount the DAS volumes on the serverWhile this may seem redundant given the firewall settings, the next step adds an extra layer of protection. We‚Äôll limit access to the Samba server by specifying allowed hosts directly in the Samba configuration file. This ensures that only designated servers can connect, further tightening security.Finalizing Samba SecurityWhen we first set up the Samba server, we configured allowed users and set the minimum SMB protocol. Now, it‚Äôs time to finish securing the server by restricting which computers can access it using the hosts allow directive in the Samba configuration.Here‚Äôs how I‚Äôve set up host access control:  Proxmox Share the Samba config file:# 192.168.177.129 - Proxmox Backup Server# 192.168.177.7 - Proxmox Server Management Port# 192.168.178.0/24 - Proxmox VM Netork# This also keeps my laptops off this share 192.168.166.0/24hosts allow = 192.168.177.129 192.168.177.7  192.168.178.0/24This configuration ensures that only the necessary servers can access the Samba pbe share. It also prevents my personal laptop, which are on the 192.168.166.0/24 subnet, from accessing these shares.  Media Share the Samba config file:## 192.168.168.0/24 - Allow device on WI-FI to access media## 192.168.166.66 - Allow personal desktop but stop work laptop access## Proxmox server do not have access hosts allow = 192.168.168.0/24 192.168.166.66This configuration ensures that only the Wi-Fi devices can access the Samba media share, while also allowing my personal laptop to share access to the media.Now, let‚Äôs move on to mounting the DAS volumes and securing them.Mounting DAS DrivesThe DAS has five bays, and two of the drives are being mounted for Samba use. Since these drives were previously used on a Linux server and already contain data, there‚Äôs no need for partitioning or formatting.Information Needed to Mount the DrivesTo mount a drive on a Linux system using the /etc/fstab file, you‚Äôll need the following details:  What is being mounted ‚Äì The device name or UUID of the drive  Where it is being mounted ‚Äì The target directory (mount point)  File system type ‚Äì The type of file system (e.g., ext4, xfs)  Mount options ‚Äì Specific options for mounting (e.g., defaults, noatime)  Dump ‚Äì Whether the filesystem should be backed up by the dump command  Pass ‚Äì The order in which filesystems should be checked by fsck (file system check)To mount the drives correctly, I needed to gather some essential information, such as the UUID and file system type (FSTYPE) of the drives. Here‚Äôs how I did it:Identifying the Drives and File System TypesI used the lsblk (list block devices) command to display the hard drives and their details, including the UUID and file system type. Here‚Äôs an example of the output:lsblk -f  # returns the followingNAME         FSTYPE FSVER LABEL    UUID                                 FSAVAIL FSUSE% MOUNTPOINTSsda                                                                                    sdb                                                                                    sdc                                                                                    sdd                                                                                    ‚îî‚îÄsdd1       ext4   1.0            3208c81f-0714-42bb-a8fc-adfbfc4ca336                sde                                                                                    ‚îî‚îÄsde1       ext4   1.0            65a350ce-4548-4ef1-86bb-e48965924224   Note: This project uses UUIDs instead of device names for added stability and security.  Drives can be mounted using either the device name (e.g., /dev/sdd1) or the UUID (e.g., 3208c81f-0714-42bb-a8fc-adfbfc4ca336). However, device names are assigned at boot and can change if new disks or partitions are added or removed.  The UUID, on the other hand, is a unique identifier assigned to a partition and only changes when the partition is recreated. This ensures that even if partitions or disks are added or removed, the UUID remains the same.  Additionally, using the UUID ensures that if a drive is removed from the DAS and replaced with a different one, the system will not automatically load the new drive. While this adds a layer of security, it also means that the system won‚Äôt support hotswapping drives without further configuration.Setting Up Mount Point OptionsNow, let‚Äôs configure the options for the mount points. Since this system will be used as storage for a NAS and is non-production, I‚Äôve opted for security over performance. After reviewing the available options, the following settings seem to fit the requirements:  atime: Ensures file access timestamps are updated. This can improve security by tracking file access but may impact performance in some cases.  noexec: Prevents the execution of binaries on this mount, adding an extra layer of security.  nodev: Disallows the creation or usage of device files, which enhances security on the storage volume.  errors=remount-ro: If any errors are detected on the filesystem, this option automatically remounts the filesystem as read-only, preventing further issues.Finally, the seutp for dump and passConfiguring Dump and PassSince the dump command is not installed on the server, I‚Äôve set the dump value to 0, meaning the filesystem will not be included in backup routines via dump.For the pass value, I‚Äôve chosen to set it to 2, which ensures that the mounted filesystems will be checked during boot. While this will slightly slow down the boot process, it‚Äôs acceptable for a back-end system that isn‚Äôt rebooted frequently.The possible values for pass are:  0: Do not check the filesystem at boot.  1: Check the filesystem first, typically used for the root (/) filesystem.  2: Check the filesystem after the root filesystem has been checked.Updating the fstab FileDuring boot, Linux systems reference the /etc/fstab file to mount additional hard drives. To finalize this project, let‚Äôs bring everything together and add the appropriate entries to the fstab file.Here‚Äôs how we‚Äôll define the lines based on the information we‚Äôve gathered:## From DAS bay 1UUID=3208c81f-0714-42bb-a8fc-adfbfc4ca336 /srv/nas_storage/media ext4 atime,atime,nodev,noexec,errors=remount-ro 0 0## From DAS bay 2UUID=65a350ce-4548-4ef1-86bb-e48965924224 /srv/nas_storage/pve ext4  atime,nodev,noexec,commit=600,errors=remount-ro 0 0With all the changes in place, I rebooted the server and tested the mounts‚Äîthey all worked perfectly.Looking back, if I were to do this again, I might choose a different device than the Orange Pi 5 Pro. The version of Debian available for it doesn‚Äôt support Logical Volume Manager (LVM), which would have allowed me to combine the hard drives into a single large logical volume for better storage flexibility.While the initial NAS setup took a few steps, adding additional shared folders in the future will be much simpler.If you had a NAS, how would you use it?"
  },
  
  {
    "title": "Mastering SSH Traffic Segregation - Enhance Security and Performance",
    "url": "/posts/Segregation-SSH-Traffic/",
    "categories": "ssh, cybersecurity",
    "tags": "ssh traffic, segregation, cybersecurity",
    "date": "2024-09-08 14:48:00 -0600",
    





    
    "snippet": "In today‚Äôs fast-paced, automation-driven environments, SSH traffic is the backbone of countless administrative and deployment tasks. From powerful tools like Ansible, rsync, and Terraform to contai...",
    "content": "In today‚Äôs fast-paced, automation-driven environments, SSH traffic is the backbone of countless administrative and deployment tasks. From powerful tools like Ansible, rsync, and Terraform to container platforms like Docker, these services generate a significant amount of SSH traffic.If this traffic isn‚Äôt properly managed, it can lead to security blind spots, noisy logs, and operational inefficiencies. That‚Äôs why segregating SSH traffic by service and user type is a simple yet highly effective way to improve both security and performance.In this guide, you‚Äôll learn how to:  Separate SSH traffic using different ports.  Apply advanced SSH configuration with the Match directive.  Optimize firewall rules and logs for easier management.  Improve Ansible automation performance with connection tuning.üìö Why Segregate SSH Traffic?Segregating SSH traffic allows you to:  Implement tighter access controls.  Streamline firewall configurations.  Enhance monitoring and log analysis.  Improve automation tool performance.For example, you may want regular user traffic on port 22 while directing automation tools like Ansible through port 2222. This way, you can apply different security settings and logging levels to each traffic type without conflict.üîß Using the SSH Match StatementThe Match directive in sshd_config allows you to apply specific configurations based on conditions such as:  Match User ‚Äì Match specific SSH users.  Match Group ‚Äì Apply settings based on user groups.  Match Address ‚Äì Filter by client IP address.  Match Host ‚Äì Filter by hostname.  Match LocalPort ‚Äì Apply settings based on the SSH port.  Match RemoteAddress ‚Äì Filter based on the client‚Äôs IP.  Match RemotePort ‚Äì Filter based on the client‚Äôs connection port.In our scenario, we‚Äôll use Match LocalPort to separate traffic across ports 22 and 2222.üóÇÔ∏è Organizing SSH Configuration FilesTo make management easier, we‚Äôll split the SSH configuration into separate files:  Main sshd_config ‚Äì Common global settings.  User Traffic Config (55-ssh-user.conf) ‚Äì Rules for human users.  Ansible Traffic Config (51-ansible_admin.conf) ‚Äì Optimized for Ansible and automation tools.This modular approach helps you maintain clean and targeted configurations without cluttering a single config file.üöÄ Optimizing Ansible Traffic on Port 2222Ansible traffic typically generates many SSH sessions during playbook execution. You can improve both security and performance by:  Restricting access to specific users and IP addresses.  Setting LogLevel to ERROR to reduce log noise.  Lowering ClientAliveInterval for faster detection of dropped sessions.  Increasing MaxSessions to allow more parallel SSH connections.‚û§ Example: 51-ansible_admin.confMatch LocalPort 2222    AllowUsers ansible_admin@192.168.167.17    DenyGroups ssh-users    ClientAliveInterval 60    ClientAliveCountMax 3    LogLevel ERROR    MaxSessions 10üë§ Managing User Traffic on Port 22For regular user SSH sessions, security and detailed logging take priority:  Restrict SSH access to the ssh-users group.  Increase ClientAliveInterval to reduce keep-alive traffic.  Enable INFO-level logging for better activity tracking.  Limit concurrent sessions to prevent resource abuse.‚û§ Example: 55-ssh-user.confMatch LocalPort 22    AllowGroups ssh-users    ClientAliveInterval 300    ClientAliveCountMax 0    LogLevel INFO    MaxSessions 10üìÑ Main SSH Configuration: sshd_config# Strong SSH Key AlgorithmsKexAlgorithms curve25519-sha256@libssh.org,diffie-hellman-group-exchange-sha256Ciphers aes256-ctr,aes192-ctr,aes128-ctrMACs hmac-sha2-512,hmac-sha2-256PubkeyAcceptedKeyTypes ssh-rsa-cert-v01@openssh.com,ssh-ed25519Protocol 2# Listening on Multiple PortsPort 22Port 2222PermitRootLogin noAuthorizedKeysFile /home/%u/.ssh/authorized_keysPasswordAuthentication noPermitEmptyPasswords noGSSAPIAuthentication noChallengeResponseAuthentication noMaxAuthTries 3LoginGraceTime 30sAllowAgentForwarding noPermitTunnel noX11Forwarding no# Include Custom ConfigurationsInclude /etc/ssh/sshd_config.d/51-ansible_admin.confInclude /etc/ssh/sshd_config.d/55-ssh-user.confüìà Benefits of SSH Traffic Segregation  ‚úÖ Simplifies firewall rule management.  ‚úÖ Helps reduce log clutter by customizing log levels.  ‚úÖ Improves security with tailored access controls.  ‚úÖ Enhances Ansible performance through optimized SSH parameters.  ‚úÖ Makes troubleshooting easier by filtering logs based on port usage.üìñ FAQ: SSH Traffic SegregationQ1: Is changing the SSH port enough to improve security?While changing the port isn‚Äôt a complete security measure, it reduces automated bot attacks and, when combined with proper controls, strengthens your SSH defense.Q2: Can I apply similar segregation for other tools?Yes! This method works for any tool using SSH, including rsync, Git over SSH, and custom scripts.Q3: Does this affect my existing Ansible playbooks?No. You simply need to update the Ansible inventory to specify the new port using ansible_port=2222.üì¢ Final ThoughtsSegregating SSH traffic is a low-cost, high-reward strategy that makes managing a secure and high-performance environment easier. Whether you‚Äôre optimizing for automation tools like Ansible or ensuring detailed logging for user activities, this approach gives you granular control over how SSH connections are handled.üõ°Ô∏è New to SSH Security? Start Here!Kick off your SSH hardening journey with Your First Steps to a Hardened SSH Server.Drop a comment or reach out‚Äîwe‚Äôre here to help. For more content like this, tools, and walkthroughs, visit my site at Sebos Technology."
  },
  
  {
    "title": "SSH Through Firewall",
    "url": "/posts/SSH-Through-Firewall/",
    "categories": "ProxoxVE, Cybersecurity",
    "tags": "SSH, Security, Proxy Jump, Network, Hardening, Best Practices",
    "date": "2024-09-01 09:40:00 -0600",
    





    
    "snippet": "Proxmox Security Series:SSH Through FirewallIn my effort to find fresh ways to improve SSH security beyond the usual tips and tricks, I looked into using the OPNsense firewall‚Äîsomething I already h...",
    "content": "Proxmox Security Series:SSH Through FirewallIn my effort to find fresh ways to improve SSH security beyond the usual tips and tricks, I looked into using the OPNsense firewall‚Äîsomething I already had‚Äîas a gateway for my Proxmox server. The idea was to make this firewall the main entry point for server access. This setup simplifies access but also means that if the firewall has issues, everything does. Although it was an interesting idea, I was initially unsure about how much it would really enhance security. Stay tuned as I dive deeper into this later in the article.SSH Authentication KeysPrior to implementing the firewall adjustments, I generated two SSH keys: one for the connection to the firewall and another for the Proxmox server.## Auth Keys for Proxmox Serverssh-keygen -t ed25519 -f ~/.ssh/pve  ## Auth Keys for OPNsense Firewallssh-keygen -t ed25519 -f ~/.ssh/firewallI admit that using two different keys might seem a bit much, and really, one key is usually enough. But I was curious‚Äîcould using two separate keys actually make things more secure? I saw this as a chance to test out that idea.Firewall Modifications for SSHConfiguring the firewall for SSH was straightforward and could be managed entirely through the web interface. Keep in mind that different firewalls offer varying features, so the following is a high-level description of the adjustments I made:  SSH was enabled on the LAN interface exclusively.  Root user login was disabled to enhance security.  Password-based login was disabled, requiring more secure authentication methods.  A new user account was created, which required administrative privileges for SSH access.  I selected the preferred shell for the user.  The SSH authentication key was uploaded to ensure secure access.These changes successfully enabled SSH access to the firewall, setting the stage for more secure operations.SSH Jump CommandSSH includes a useful -J switch, which allows routing through a jump server.## SSH Jump Commandssh -J &lt;firewall_user_id&gt;@&lt;firewall_ip_address&gt; &lt;pve_user_id&gt;@&lt;pve_ip_address&gt;As you begin to integrate options like authentication keys, the command can become cluttered. This is where the .ssh/config file becomes invaluable. It allows users to assign aliases to SSH servers and specify options for each connection. For environments with multiple servers, the file can become complex. Using include files within the .ssh/config can help manage this complexity, keeping configurations organized and maintainable.Configuring SSH with .ssh/configTo streamline SSH commands using configuration files, you can create specific config files for each server, like so:Config File Name: firewall# Configuration file for firewallHost firewall    HostName 192.168.166.1    User sebos    IdentityFile ~/.ssh/firewallConfig File Name: pve# Configuration file for Proxmox VEInclude ~/.ssh/include.d/firewallHost pve    HostName 192.168.167.127    User richard    ProxyJump firewall    IdentityFile ~/.ssh/pveChanges to: ~/.ssh/configInclude ~/.ssh/include.d/firewallInclude ~/.ssh/include.d/pveWith these configurations in place, connecting to your Proxmox server via the firewall jump host simplifies to a single command:ssh pve  Note:  Storing additional configuration files in the ~/.ssh/include.d/ directory isn‚Äôt mandatory but helps maintain organization and clarity in your SSH setup.Did It Enhance Security?My assessment is affirmative. The setup resulted in some interesting outcomes:  The Proxmox server can no longer be directly accessed via SSH without routing through the jump server‚Äîa predicted but significant tightening of security.  Surprisingly, even from the firewall‚Äôs shell, I was unable to initiate an SSH connection to the Proxmox server.To successfully access the Proxmox server through SSH, the following conditions must be met:  Physical presence within my LAN network.  Possession of both sets of my SSH keys.  Knowledge of the user IDs and server IP addresses, though the .ssh/config file simplifies this aspect.Would I extend this setup to my WiFi or WAN network? Potentially, yes‚Äîif I stored my configuration files and authentication keys within an encrypted directory secured by a password, I would consider it feasible and safe to do so.Here‚Äôs a simpler version for a general audience:Would I create two authentication key files again in the future? Yes, but only if I planned to store them in different places, like one on my local machine and the other on a network share within my own network.Can This Stop All Hackers?No security system is perfect. Highly skilled hackers can find vulnerabilities, including Zero Day exploits, in any system. The best strategy is to minimize potential entry points and add layers of security to discourage most attackers.I‚Äôd love to hear about your methods. How do you secure SSH in your network?"
  },
  
  {
    "title": "Securing Root Access",
    "url": "/posts/Securing-Root-Access/",
    "categories": "ProxoxVE, Cybersecurity",
    "tags": "proxmox, vm, servers, security",
    "date": "2024-08-24 18:35:00 -0600",
    





    
    "snippet": "Continuing our series on securing your Proxmox server, this article focuses on an essential aspect: securing root access. Like many other systems, Proxmox requires an administrator account to perfo...",
    "content": "Continuing our series on securing your Proxmox server, this article focuses on an essential aspect: securing root access. Like many other systems, Proxmox requires an administrator account to perform critical operations, and it uses the Linux root account by default. However, leaving this root access open presents significant security vulnerabilities, such as:  Unrestricted access to the web-based root console  Direct root access via SSHThe good news is that these vulnerabilities are easy to address. Even after implementing these security measures, you‚Äôll still retain the necessary root access to manage your Proxmox environment securely.Install sudoBefore disabling root access, it‚Äôs crucial to create a new administrative user with restricted privileges. To manage administrative tasks without using the root account, we‚Äôll need to install sudo on the Proxmox server. You can do this by accessing the server either through the web console as root or by connecting via SSH. First, update the server‚Äôs packages, then install sudo using the following commands:# Update the server firstapt update &amp;&amp; apt upgrade -y# Install sudoapt install sudoWith sudo installed, we can create an administrative account.Create a New System UserCreating a new user on Linux is straightforward using the adduser command. This user will differ from the standard user account (like ‚Äúbob‚Äù we created in a previous article) as it will have administrative privileges rather than just access to the Proxmox web interface. In this example, we‚Äôll create a new user named ‚ÄúRichard‚Äù to replace the root user for administrative tasks.Steps to Create a New System User      Create the user account:Use the adduser command to create a new user with a specified home directory.    # Create a new user with a home directoryadduser --home /home/richard richard            Create a new group for system administrators:If you don‚Äôt already have a group for administrative users, you can create one. This group can be used to manage permissions more efficiently.    # Create a system admin groupaddgroup system-admin            Grant sudo and group access:Add the new user to the newly created system-admin group.    # Add the new user to the system admin groupgpasswd --add richard system-admin      Next, we will create a custom sudoers file to further secure the server and fine-tune the permissions for our new administrative user.Configure the sudoers File for RichardThe sudoers file allows you to specify which commands a sudo user can execute, providing granular control over administrative permissions. You can create user aliases or assign permissions directly to existing system accounts. Since I often reuse my sudoers files across multiple servers, I prefer to use aliases for better management and scalability.In this section, we‚Äôll create a list of command aliases that define what actions Richard can perform and assign these command aliases to his user group.Steps to Configure the sudoers File      Define user aliases: First, create a user alias for the system-admin group. This makes it easy to manage permissions for all users in this group.    # Sudoers file for adminUser_Alias SYSTEM_ADMIN = %system-admin            Create command aliases: Define command aliases for specific tasks Richard and other system administrators need to perform. This restricts the commands they can run with sudo and enhances security.    # Allow system updatesCmnd_Alias UPDATE_CMDS = /usr/bin/apt update, /usr/bin/apt upgrade# Check error logsCmnd_Alias LOG_CMDS = /usr/bin/tail -f /var/log/*, /usr/bin/journalctl, /bin/grep sshd /var/log/auth.log# Restricted Vim (rvim) to edit files without dropping to a shellCmnd_Alias RVIM = /usr/bin/rvim            Assign command aliases to the user group: Finally, assign these command aliases to the SYSTEM_ADMIN user alias. This allows members of the system_admin group to run the specified commands without a password prompt.    SYSTEM_ADMIN ALL=(ALL) NOPASSWD: UPDATE_CMDS, LOG_CMDS, RVIM      Root access can be restricted now that we have an additional SSH account.Restricting Root Access from SSHCreating an additional user account on the server allows us to restrict root access through SSH while still maintaining SSH access for administrators. To enforce this, we‚Äôll use the ssh-users group to control who can access the server via SSH. While this might seem like overkill for a Proxmox server with just one user-level account, it‚Äôs a best practice for securing all servers that use SSH.Steps to Restrict Root SSH Access      Create an SSH access group: First, create a new group named ssh-users and add the new administrative user, Richard, to this group.    # Create an SSH access groupaddgroup ssh-users# Add the new user to the SSH access groupgpasswd --add richard ssh-users            Edit the SSH configuration file: Next, edit the SSH daemon configuration file (sshd_config) to disable root login and allow only users in the ssh-users group to connect via SSH.    # Disable root loginPermitRootLogin no# Allow only specific groups to access SSHAllowGroups ssh-users        Note: If neither AllowGroups nor AllowUsers is specified, any new user could potentially access the server through SSH. Defining these parameters helps to restrict access to only those users explicitly allowed.      Restart the SSH service: To apply these changes, restart the SSH service.    # Restart SSH servicesystemctl restart sshd        Note:  In a future article, we will cover additional steps to further &gt;secure the SSH daemon beyond just restricting root access.With these SSH security measures in place, we‚Äôve significantly reduced the attack surface by restricting root access and controlling who can access the server. Next, we will focus on restricting access to the Proxmox web shell.Add System Admin to ProxmoxThe richard system account we created earlier does not automatically appear in the Proxmox web interface. To grant the richard account administrative access, we need to manually add it as an Administrator within Proxmox.  Add the richard user to Proxmox  Add administrator the richardBy doing this, Richard will have the necessary permissions to manage all aspects of the Proxmox environment, allowing us to disable the root account safely. Once root is disabled, the server console will require a password for login instead of automatically providing root access.  password is needed on node web consoleBenefits of Replacing Root with RichardBy configuring Richard as the sole user with SSH access and limiting the commands executable with sudo, we‚Äôve effectively secured SSH access. As Richard has Administrator privileges in Proxmox, any tasks that cannot be performed through SSH can still be handled through the Proxmox web interface.Additionally, if the web interface is ever unavailable, you can still access the server directly with physical access. This setup ensures robust security while maintaining the flexibility needed to manage the Proxmox environment effectively."
  },
  
  {
    "title": "Strengthening Your Virtual Environment",
    "url": "/posts/Strengthening-Your-Virtual/",
    "categories": "ProxoxVE, Cybersecurity",
    "tags": "proxmox, vm, servers, security",
    "date": "2024-08-18 06:27:00 -0600",
    





    
    "snippet": "Proxmox:Baby steps to Increase SecurityProxmox is an open-source virtual environment tool for creating and managing virtual machines and containers. Currently a niche solution in a growing market, ...",
    "content": "Proxmox:Baby steps to Increase SecurityProxmox is an open-source virtual environment tool for creating and managing virtual machines and containers. Currently a niche solution in a growing market, Proxmox is  used  by companies and educational institutions as an alternative to VMware ESXi, Proxmox offers robust features at a lower to no cost.  However, since it is built on a Linux system, additional security measures are necessary to protect your environment.One crucial aspect of securing Proxmox is controlling who has access and what actions they can perform, such as starting, stopping, deleting, or modifying virtual machines. In this series of articles, I will guide you through a series of steps to add layers of security to your Proxmox server.We‚Äôll start with the simpler tasks, gradually moving to more advanced security configurations. In this first article, we‚Äôll focus on creating and managing user permissions within Proxmox to ensure that only authorized personnel can access and control your virtual environment.Proxmox UsersTo get started, we‚Äôll create a few users based on their specific needs. These users will be set up with Proxmox VE authentication server logins:      Bob:Bob is the Proxmox Administrator for multiple teams. He is responsible for creating, maintaining, and backing up virtual servers, as well as managing the Proxmox server itself.        Betty:Betty works on external-facing web pages. She needs to monitor the status of the servers and occasionally use the web console for her tasks.        Jim:Jim focuses on internal-facing web pages. Like Betty, he needs to check the status of the servers and occasionally access the web console.        Nancy:Nancy is the Team Manager for the Web Programmers. Betty and Jim report to her when there are issues with the virtual servers. While she has the ability to reboot the VMs, she contacts Bob if more significant action is required.    Note: The permissions discussed in this article are for  accessing Proxmox, not the operating systems of the virtual machines themselves.Proxmox PoolsOne of the powerful features in Proxmox is the ability to organize VMs into Pools, essentially creating groups of VMs based on their function or purpose. In our example, we‚Äôll create two sets of VMs:      ExtWebSer:This pool will contain external web servers.        IntWebSer:This pool will contain internal web servers.  In this setup,      Nancy and Betty::Will be assigned to the ExtWebSer pool        Nancy and JIM::Will be assigned to the IntWebSer pool  Proxmox Privileges and RolesProxmox uses roles to assign a set of predefined privileges to users or groups. These roles are  fixed but can be customized by creating additional roles based on the available privileges. We‚Äôll apply these roles to our users based on their specific needs.      Bob:Bob is assigned the Administrator role, which is a predefined role with comprehensive privileges.        Betty and Jim:Betty and Jim are assigned a new role called WebDev, which includes the privileges Mapping.Audit VM.Audit, VM.Console, and **VM.Monitor. This role is tailored to their needs for monitoring and occasional access.        Nancy:Nancy is assigned a new role called WebAppManager, which includes the privileges Mapping.Audit, VM.Audit, VM.Monitor, and VM.PowerMgmt. This role allows her to oversee the web application servers and perform basic management tasks like rebooting VMs.    Note:  Note: A complete listing of Proxmox privileges can be found here.To ensure that users with the WVWebDev and WebAppManager roles can view these pools in Proxmox, the Pool.Audit permission must be added to their roles.Proxmox PermissionsProxmox pools created the assocations between user, roles and resources, in our case, pools. Premission defind what access a users has to what resource.  Bob:  Has access to / (all)  as Adminstrator  Betty:  Has access to /pool/ExtWebSer as WVWebDev  Jim:  Has access to /pool/IntWebSer as WVWebDev  Nancy:  Has access to /pool/ExtWebSer as WebAppManager  Has access to /pool/IntWebSer as WebAppManagerWhy Do This?In a larger organization, there could be tens or even hundreds of VMs. The teams managing these VMs, along with the critical applications running on them, need appropriate access to perform their duties. By restricting access based on roles and pools, you can secure critical servers, such as those used for HR, strategic planning, and accounting, as well as others ensuring that only authorized personnel have access.What the users see      Betty         Jim         Nancy  In the next article, we will explore how to restrict root access on the Proxmox server itself, adding another layer of security to your environment.I hope you found this article valuable, and I appreciate the time you took to read it. If you have any questions or suggestions, please feel free to reach out. When it comes to securing a virtual host, what steps would you consider taking?"
  }
  
]

