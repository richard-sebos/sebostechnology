[
  
  {
    "title": "Securing ROS 2 with AppArmor and Auditd",
    "url": "/posts/Armor-Audit/",
    "categories": "Robotics, ROS2 Install Series, Security",
    "tags": "ros2, robotics, cybersecurity, linux",
    "date": "2025-04-10 04:00:00 -0600",
    





    
    "snippet": "AppArmor and Auditd  Three Linux security softwares that don‚Äôt get talk about enough are SELinux, AppArmor and Auditd.  Like file permission and firewalls, they can  are there to keep unwanted user...",
    "content": "AppArmor and Auditd  Three Linux security softwares that don‚Äôt get talk about enough are SELinux, AppArmor and Auditd.  Like file permission and firewalls, they can  are there to keep unwanted users and processes from access resources.  They can also report when this access is attenpted  These allow you to create security policies that can be customized to you robots needs.  Remember, you robot will be interacting with the real world and you will want to protect is software from itSELinux and AppArmor  SELinux and AppArmor are Mandatory Access Control (MAC) security application.  The access control is built into kernel giving great access and less chance to be bypass  RHEL base distro normally have SELinux and  AppArmor is normally assocated with Debian and SuSe based system  Normally you would have either SELinux or AppArmor running but not both.  But what to do with it hand how to create policiesAudit  Auditd collects security-relevant events include from SELinux and Auditd  It also allow you to search/report on events that are happening on the system  From a secure stand point, you would use these events to create SELinux or AppArmor to restrict or allow access to resources.How do they work together  Normally you would setup SELinux or AppArmor in a very restrictive policies  You would have them either in Enforce (stopping and report access) or Permissive (reporting only) state.  As users are interaction with the server‚Äôs reources, you would monutor the auditd records and add, change or remove security policies as needed.Starting AppArmor and Auditd  Since ROS2 nornally installing on an Ubuntu system, the script for this part will work with AppArmor and Audit  Below parts of the script lays the groundwork for:          MAC enforcement via AppArmor (though it needs real profiles to be effective).      Auditing access to ROS 2 files, code, config, and binaries.              Ideal for use in secure robotics or ICS environments where accountability and control are key.            üîê 1. Enable and Start AppArmorsystemctl enable apparmorsystemctl start apparmor  AppArmor is a Linux kernel security module for mandatory access control.  This ensures AppArmor is enabled on boot and starts immediately, enforcing any profiles that exist.üìÅ 2. Create a Directory and Placeholder Profile for ROS 2mkdir -p /etc/apparmor.d/ros2/echo \"# Placeholder ROS2 AppArmor profile\" &gt; /etc/apparmor.d/ros2/ros2-default  Prepares the system for a custom AppArmor profile for ROS 2.  Right now, it‚Äôs just a placeholder (ros2-default)‚Äîdoesn‚Äôt enforce anything yet, but sets up the structure for future enforcement.üïµÔ∏è 3. Configure Auditd Rules for ROS 2 Componentscat &lt;&lt;EOF &gt; /etc/audit/rules.d/ros2.rules-w /opt/ros -p x -k ros_exec-w /home/$ROS_USER/ros2_ws -p x -k ros_ws_exec-w /home/$ROS_USER/ros2_ws/src -p wa -k ros_src-w /etc/ros2 -p wa -k ros_conf-w /usr/bin/colcon -p x -k colcon_exec-w /usr/bin/rosdep -p x -k rosdep_exec-w /home/$ROS_USER/.bashrc -p wa -k ros_envEOFThese rules tell auditd to:  Log execution attempts (-p x) for:          ROS binaries (/opt/ros)      Workspace build system (colcon)      Dependency tool (rosdep)        Watch for changes (-p wa) in:          Source code (ros2_ws/src)      Configuration files (/etc/ros2)      User environment setup (.bashrc)      Each rule is tagged with a key (-k) for easy filtering in audit logs.  $ROS_USER should be an exported environment variable or replaced with an actual username before use.üîÑ 4. Apply Audit Rules and Start Auditdaugenrules --loadsystemctl enable auditdsystemctl start auditd  augenrules --load compiles and applies the new rules.  auditd is enabled to start at boot and immediately starts collecting logs.  At this point, we have a          freshly installed Ubuntu      Update it and added a ROS2 user      Installed ROS2 and setup the ROS2 enviroment      Setup AppArmor and Audiitd to receive new policies and rules as the robot application is built.        next we will be create firewall rules to protect the robot."
  },
  
  {
    "title": "Installing ROS 2 on Ubuntu with `ros2_install.sh`",
    "url": "/posts/Installing-ROS2/",
    "categories": "Robotics, ROS2 Install Series, Security",
    "tags": "ros2, robotics, cybersecurity, linux",
    "date": "2025-04-09 11:58:00 -0600",
    





    
    "snippet": "Here‚Äôs a polished, professional-yet-friendly article version of your script documentation. It‚Äôs structured with a Table of Contents, clear SEO-friendly wording, and paragraph formatting to make it ...",
    "content": "Here‚Äôs a polished, professional-yet-friendly article version of your script documentation. It‚Äôs structured with a Table of Contents, clear SEO-friendly wording, and paragraph formatting to make it reader- and documentation-friendly.Setting up a reliable and efficient ROS 2 development environment is a key step in any robotics or automation project. In our previous post, we covered how to:  Update a fresh Ubuntu installation.  Create and configure a dedicated ROS 2 user.  Prepare the system for installing ROS 2.Now, we‚Äôll continue that journey by walking through the ros2_install.sh script. This script automates the installation of ROS 2 base components and supporting tools to streamline your development setup.  ‚úÖ Note: These steps are tailored for Ubuntu 20.04 (Focal) and Ubuntu 22.04 (Jammy) systems.üìö Table of Contents  Overview of the ros2_install.sh Script  Step-by-Step Breakdown          Installing Prerequisite Packages      Adding the ROS 2 Repository      Installing ROS 2 Base and Tools      Initializing rosdep      Setting Up the ROS 2 Environment        Next Steps  SEO TagsOverview of the ros2_install.sh ScriptThe ros2_install.sh script is designed to automate the key steps needed to install ROS 2 on a clean Ubuntu system. It handles:  Installing all necessary system dependencies.  Adding the official ROS 2 package repository and its GPG key.  Installing core ROS 2 packages along with useful developer tools.  Initializing rosdep, which handles ROS dependency management.This script greatly simplifies what would otherwise be a tedious manual installation process.Step-by-Step BreakdownInstalling Prerequisite Packagesapt install -y curl gnupg2 lsb-release software-properties-commonThis command installs essential tools needed to add third-party APT repositories:  curl: Downloads content from URLs (used to retrieve the GPG key).  gnupg2: Manages GPG keys to verify repository authenticity.  lsb-release: Outputs distribution codename (e.g., focal, jammy) for dynamic configuration.  software-properties-common: Allows managing additional APT sources.Adding the ROS 2 RepositoryImport the GPG Key:curl -sSL \"$ROS_KEY_URL\" -o /etc/apt/trusted.gpg.d/ros.ascThis command downloads the ROS 2 repository‚Äôs public GPG key and stores it in the system‚Äôs trusted keyring, ensuring secure installation of ROS packages.Add the Repository:echo \"deb [arch=amd64 signed-by=/etc/apt/trusted.gpg.d/ros.asc] $ROS_REPO_URL $(lsb_release -cs) main\" &gt; /etc/apt/sources.list.d/ros2.listThis command adds the ROS 2 repository to the system‚Äôs APT sources. It automatically uses the appropriate Ubuntu codename (e.g., jammy) to ensure compatibility.Update APT Cache:apt updateThis refreshes the package index so the newly added ROS 2 repository is recognized.Installing ROS 2 Base and Toolsapt install -y ros-$ROS_DISTRO-ros-base python3-rosdep python3-colcon-common-extensions python3-argcomplete colconThis installs the ROS 2 base system and essential development tools:  ros-$ROS_DISTRO-ros-base: The minimal installation of ROS 2.  python3-rosdep: Tool to resolve and install ROS package dependencies.  colcon, python3-colcon-common-extensions: ROS 2‚Äôs recommended build system.  python3-argcomplete: Adds command-line autocompletion for ROS CLI tools.Initializing rosdepOne-Time Initialization (if needed):[ -f /etc/ros/rosdep/sources.list.d/20-default.list ] || rosdep initThis checks whether rosdep has already been initialized. If not, it runs the initialization process.Update rosdep Definitions:rosdep updateDownloads the latest package dependency definitions so that rosdep can install required system packages for ROS nodes.Setting Up the ROS 2 EnvironmentSource the Environment Manually:source /opt/ros/&lt;distro&gt;/setup.bashThis sets up necessary environment variables (like ROS_PACKAGE_PATH) for ROS 2 in the current terminal session.Make it Persistent for a User:grep -q \"source /opt/ros/$ROS_DISTRO/setup.bash\" /home/$ROS_USER/.bashrc \\|| echo \"source /opt/ros/$ROS_DISTRO/setup.bash\" &gt;&gt; /home/$ROS_USER/.bashrcThis command ensures the ROS 2 environment is automatically sourced every time the user opens a new terminal. It checks for an existing entry to avoid duplicates.Next StepsAt this point, your system should be fully prepared for ROS 2 development. Here‚Äôs what we‚Äôve achieved so far:  A freshly installed and updated Ubuntu system.  A dedicated user account configured for ROS 2.  ROS 2 base system and build tools installed and ready to use.In the next post, we‚Äôll focus on securing the ROS 2 environment using AppArmor and Auditd, adding an extra layer of protection to your robotics platform."
  },
  
  {
    "title": "üõ°Ô∏è Setting Up a Secure ROS 2 System Part 1 ‚Äì Updating Ubuntu and Creating a ROS User",
    "url": "/posts/ROS2_Build-Part1/",
    "categories": "Robotics, ROS2 Install Series, Security",
    "tags": "ros2, robotics, cybersecurity, linux",
    "date": "2025-04-09 07:06:00 -0600",
    





    
    "snippet": "IntroductionWelcome to the first post in my series on setting up a secure ROS 2 system on Ubuntu. In this part, we‚Äôre going to lay the groundwork by updating the operating system and setting up a d...",
    "content": "IntroductionWelcome to the first post in my series on setting up a secure ROS 2 system on Ubuntu. In this part, we‚Äôre going to lay the groundwork by updating the operating system and setting up a dedicated user for ROS 2. These initial steps might seem routine, but they‚Äôre crucial for building a system that‚Äôs stable, maintainable, and secure.Table of Contents  Introduction  Modular Bash Script for Setup  Making the Script Re-Runnable  Setting the Timezone, Updating Ubuntu, and Creating the ROS User  Next Steps  SEO KeywordsTo make the process repeatable and reliable, I‚Äôm using a modular Bash script. Throughout this series, I‚Äôll be adding functionality to this script so that by the end, you‚Äôll be able to start from a fresh Ubuntu install and fully configure your ROS 2 system with automation and security best practices baked in.Let‚Äôs dive in and start crafting the setup script.Modular Bash Script for SetupTo keep things organized, I‚Äôve broken the script into logical modules. This makes it easier to maintain, debug, and extend. At the core, there‚Äôs a common.sh script where shared variables and functions live. Here‚Äôs an overview of what‚Äôs included:#!/bin/bash# Shared configuration and utility functionsTIMEZONE=\"UTC\"                           # System timezoneROS_DISTRO=\"jazzy\"                       # ROS 2 distributionROS_USER=\"rosbot\"                        # Local ROS userROS_WS=\"/home/$ROS_USER/ros2_ws\"        # Development workspace# ROS repository and keyROS_KEY_URL=\"https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc\"ROS_REPO_URL=\"http://packages.ros.org/ros2/ubuntu\"# IPs of other ROS nodes in the systemROS_NODE_IPS=(\"192.168.1.10\" \"192.168.1.11\")This file will be sourced by each module, ensuring all scripts have access to shared settings and configuration values.Making the Script Re-RunnableOne important design goal is that the script should be safe to re-run. Whether you‚Äôre tweaking things during development or reinstalling on a new version of Ubuntu, being able to resume where you left off is essential.Each task in the script is wrapped in a run_step function, which checks whether that step has already completed. If it has, it skips it. If not, it runs and logs the result.STATE_FILE=\"/var/log/ros2_setup_state\"log_step() { echo \"$1\" &gt;&gt; \"$STATE_FILE\"; }has_run() { grep -q \"$1\" \"$STATE_FILE\" 2&gt;/dev/null; }abort_on_failure() { echo \"[-] ERROR: $1\"; exit 1; }run_step() {    local STEP_NAME=\"$1\"    shift    if ! has_run \"$STEP_NAME\"; then        echo \"[*] Running: $STEP_NAME...\"        \"$@\" || abort_on_failure \"$STEP_NAME failed\"        log_step \"$STEP_NAME\"    else        echo \"[‚úì] Skipping: $STEP_NAME (already completed)\"    fi}This approach ensures robustness and resilience as you build out more complex setups.Setting the Timezone, Updating Ubuntu, and Creating the ROS UserThe first actual setup script we‚Äôll use is system_setup.sh. This assumes you‚Äôre starting from a clean Ubuntu install and performs the following:  Sets the system timezone  Updates and upgrades all system packages  Installs commonly needed software tools  Creates a dedicated user for running ROS 2Here‚Äôs the relevant snippet:#!/bin/bashsource ./common.shtimezone_and_update() {    timedatectl set-timezone \"$TIMEZONE\"    apt update    apt full-upgrade -y    apt install -y curl gnupg2 lsb-release software-properties-common ufw apparmor apparmor-utils auditd vim nano}And for setting up the user:create_ros_user() {    id -u \"$ROS_USER\" &amp;&gt;/dev/null || (        adduser --disabled-password --gecos '' \"$ROS_USER\"        usermod -aG sudo \"$ROS_USER\"    )}With these two functions, we begin the process of hardening the base system and preparing a clean, controlled environment for running ROS 2.While these steps might seem a bit mundane, they are foundational. A secure system is built in layers, and that starts with good hygiene: consistent configuration, up-to-date software, and properly scoped user permissions.Next StepsIn the next post, we‚Äôll continue building the script to install ROS 2 itself, configure the environment, and begin applying security tools such as firewalls and intrusion detection systems.Security doesn‚Äôt come from one magic tool ‚Äî it‚Äôs a layered approach that begins at the OS level. With the base system updated and a dedicated user in place, we‚Äôre ready to move on to more exciting territory.Stay tuned!"
  },
  
  {
    "title": "Hardening Your Robot Project from the Start",
    "url": "/posts/ROS2_Build/",
    "categories": "Robotics, ROS2 Install Series, Security",
    "tags": "ros2, robotics, cybersecurity, linux",
    "date": "2025-04-08 04:00:00 -0600",
    





    
    "snippet": "As robotics becomes more accessible and developers take the leap into building their own intelligent machines, the importance of security cannot be overstated. Whether you‚Äôre tinkering with your fi...",
    "content": "As robotics becomes more accessible and developers take the leap into building their own intelligent machines, the importance of security cannot be overstated. Whether you‚Äôre tinkering with your first robot at home or developing a prototype for industrial use, it‚Äôs critical to think beyond just getting ROS2 installed and your nodes communicating.Welcome to the first post in a new series focused on securing your ROS2 environment ‚Äî starting from the ground up, at the Linux OS level. This guide is crafted to walk you through practical and effective steps to immediately improve the security posture of your robot projects. Our goal is to empower builders and developers to safeguard their work from common attack vectors without getting lost in complexity.üõ°Ô∏è Why Create This SeriesThere‚Äôs no shortage of great tutorials out there that walk you through setting up ROS2, spinning up topics, or launching nodes. But once you have ROS2 up and running, what comes next? How do you ensure your system isn‚Äôt just functional, but secure?Robots are complex systems with internal hardware communication and often external network access. These connections‚Äîif left unsecured‚Äîcan become points of vulnerability. Especially when you‚Äôre dealing with expensive hardware or devices that interact with the real world, protecting your infrastructure becomes essential.This series aims to fill that gap‚Äîbridging the world of ROS2 functionality with practical cybersecurity strategies.üîß What This Series CoversIn this series, we‚Äôll walk through key steps to build a more secure ROS2 setup from the OS level and beyond:  ‚úÖ Post-install system updates  üë§ Creating a dedicated ROS user  ü§ñ Installing ROS2 securely  üîê Configuring AppArmor and Auditd  üåê Setting up basic firewall rules  üß∞ Enabling SROS2 for secure communication  üîé Installing and configuring Suricata for network intrusion detection  üìÑ Creating a baseline security report  üóìÔ∏è Automating periodic security checks with scheduled reportsEach topic will have its own deep-dive post, so you can follow along step by step or jump to the parts that are most relevant to your setup.üöÄ Next StepsStay tuned for the first hands-on guide: ‚ÄúPost-Install Hardening for Linux on a ROS2 Host‚Äù, where we‚Äôll cover system updates, user creation, and setting up your environment securely from the beginning.You can follow this series here and subscribe via RSS for updates when new parts are released.üß© Closing ThoughtsSecuring robotics projects might seem daunting at first, but just a few well-placed security practices can make a world of difference. Whether you‚Äôre a hobbyist, educator, or professional, adopting a ‚Äúsecurity-first‚Äù mindset from the beginning of your ROS2 journey will help protect both your hardware and your data.If you have questions, suggestions, or topics you‚Äôd love to see covered, drop a comment below or reach out via GitHub Discussions. Let‚Äôs make robotics not just exciting and innovative‚Äîbut secure as well."
  },
  
  {
    "title": "How to Use QEMU to Run Linux VMs in Minutes",
    "url": "/posts/Putty/",
    "categories": "linux, virtualization, sysadmin, devops",
    "tags": "linux, virtualization, sysadmin, devops",
    "date": "2025-03-31 08:39:00 -0600",
    





    
    "snippet": "Embracing Secure Remote Access with PuTTY SSH KeysIn today‚Äôs digital landscape, secure remote access is crucial for managing servers and networks efficiently. Enter PuTTY and SSH keys‚Äîa powerful du...",
    "content": "Embracing Secure Remote Access with PuTTY SSH KeysIn today‚Äôs digital landscape, secure remote access is crucial for managing servers and networks efficiently. Enter PuTTY and SSH keys‚Äîa powerful duo enhancing security and ease of access.What is PuTTY?PuTTY is a free, open-source tool widely used on Windows to establish SSH or Telnet connections. Developed by Simon Tatham, it‚Äôs known for its robust security features and user-friendly interface, making remote server access seamless and secure.Who Uses It?Primarily, system administrators, developers, and IT professionals leverage PuTTY for accessing Linux or Unix servers. Essential for anyone needing to manage remote systems securely and efficiently.How to Generate Keys with PuTTYgen  Download and Install PuTTYgen: Obtain it from the official website.  Launch PuTTYgen: Open the tool to generate your keys.  Generate Keys: Click ‚ÄúGenerate‚Äù and move your mouse across the screen to create a secure key pair.  Save Your Keys: Store both private (puttyPrivateKey.pem) and public (id_rsa.pub) keys securely.  Optional: Save passphrase: Enhance security by protecting your private key with a passphrase.Using PuTTY SSH Keys  Configure PuTTY: Open the tool, navigate to ‚ÄúSSH &gt; Auth.‚Äù  Browse Private Key: Select your saved private key file.  Connect Securely: Use the session settings to connect and enjoy secure access without entering a password repeatedly.Why Use SSH Keys?SSH keys offer superior security over passwords by providing cryptographic authentication. They eliminate the need for shared passwords, reducing risks of unauthorized access and enhancing protection against brute-force attacks.Closing ThoughtsIncorporating PuTTY SSH keys into your remote access toolkit is a vital step towards robust security. By following these steps, you can ensure secure and efficient management of your servers. Remember to adhere to best practices, such as safeguarding private keys, to maintain the highest level of security in your remote operations."
  },
  
  {
    "title": "How to Use QEMU to Run Linux VMs in Minutes",
    "url": "/posts/qemu-vm/",
    "categories": "linux, virtualization, sysadmin, devops",
    "tags": "linux, virtualization, sysadmin, devops",
    "date": "2025-03-31 08:39:00 -0600",
    





    
    "snippet": "How to Use QEMU to Run Linux VMs in MinutesIf you read my last post, you‚Äôll remember we explored QEMU from a high-level perspective. Today, let‚Äôs roll up our sleeves and dive into how you can quick...",
    "content": "How to Use QEMU to Run Linux VMs in MinutesIf you read my last post, you‚Äôll remember we explored QEMU from a high-level perspective. Today, let‚Äôs roll up our sleeves and dive into how you can quickly get a Linux VM up and running using QEMU. We‚Äôll cover both graphical and console-based virtual machines, using Kali Linux and Fedora Server as examples. Once QEMU is installed, you can have a fully functional Linux VM within minutes.  ‚úÖ Install QEMU if you haven‚Äôt already.Table of Contents  Running Kali Linux with GUI in QEMU  QEMU Command Breakdown (Kali)  Running Fedora Server in Console Mode  QEMU Command Breakdown (Fedora)  Why Use QCOW2 Images?  Final ThoughtsRunning Kali Linux with GUI in QEMUKali Linux is one of the most well-known distributions in the cybersecurity world‚Äîloved by blue teamers, pen-testers, and hackers alike. While you might be familiar with the live ISO, there‚Äôs also a maintained QEMU-ready version of Kali that makes spinning up a VM super easy.Once you‚Äôve downloaded the QEMU .qcow2 image of Kali, you can launch it with a few simple options. Here‚Äôs a typical configuration I use for GUI-based Kali setups:  4 GB RAM  2 virtual CPUs  Virtual hard drive (QCOW2)  NAT networking  Virtio networking for better performance  Graphical window with visible cursor  Custom name: ‚ÄúKali VM‚ÄùThis setup is ideal for local penetration testing labs or blue team sandboxing.QEMU Command Breakdown (Kali)Run Kali Linux GUI:qemu-system-x86_64 \\  -m 4096 \\  -smp 2 \\  -hda kali-linux-2025.1a-qemu-amd64.qcow2 \\  -cpu max \\  -display default,show-cursor=on \\  -device virtio-net,netdev=net0 -netdev user,id=net0 \\  -name \"Kali VM\"Explanation of Options:  qemu-system-x86_64: Launch QEMU for 64-bit x86 systems.  -m 4096: Assigns 4 GB of RAM.  -smp 2: Enables 2 virtual CPU cores.  -hda: Mounts the Kali .qcow2 disk image as the primary drive.  -cpu max: Uses all available host CPU features for better guest performance.  -display default,show-cursor=on: Opens a graphical window with visible mouse cursor.  -device virtio-net,netdev=net0: Adds a fast virtual NIC using virtio.  -netdev user,id=net0: Enables NAT (user-mode networking).  -name: Gives the VM a name for identification.üì• Download Kali Linux QEMU imageRunning Fedora Server in Console ModeWhile GUIs are great, sometimes you just want to get straight into the terminal‚Äîespecially on servers. Running QEMU in console mode is resource-efficient and better suited for headless setups or SSH-based management.Fedora offers pre-built .qcow2 images that work perfectly with this style of virtualization. Here‚Äôs a minimal setup I use:  4 GB RAM  2 CPU cores  Max CPU feature exposure  QCOW2 virtual drive  No graphical interface  Console and QEMU monitor output to terminalQEMU Command Breakdown (Fedora)Run Fedora Server Console:qemu-system-x86_64 \\  -m 4096 \\  -smp 2 \\  -cpu max \\  -hda Fedora-Server-KVM-41-1.4.x86_64.qcow2 \\  -nographic \\  -serial mon:stdioExplanation of Options:      qemu-system-x86_64: Launches QEMU with 64-bit x86 emulation.        -m 4096: Allocates 4 GB of RAM to the VM.        -smp 2: Uses 2 virtual CPUs.        -cpu max: Enables all CPU features available to the host for optimal performance in the guest.        -hda Fedora-Server-KVM-41-1.4.x86_64.qcow2: Uses this QCOW2 disk image (Fedora Server) as the main virtual hard drive.        -nographic: Disables the graphical display (no window will pop up), and instead routes VM output to the terminal. Ideal for server environments or SSH-only VMs.        -serial mon:stdio: Redirects the serial console and QEMU monitor to your terminal (STDIO), so you interact with the VM as if it were a headless physical server via serial console.  üì• Download Fedora Server QEMU imageWhy Use QCOW2 Images?The .qcow2 format is incredibly convenient‚Äîit‚Äôs a pre-installed, ready-to-boot Linux environment in a single file. Unlike Live ISOs, changes to the system are persistent, which is ideal for testing and development.  ‚ö†Ô∏è Note: These images might include default user credentials. Make sure to change the password or, better yet, create your own user immediately after boot.QCOW2 images are perfect for:  Rapid Linux prototyping  Security testing environments  Script and automation testing  Isolated lab setupsFinal ThoughtsWhether you‚Äôre prototyping, testing scripts, or building a home lab, QEMU with ready-made QCOW2 images is a powerful and fast way to get Linux up and running. In just a few minutes, you can launch a GUI-driven Kali instance or a lean Fedora Server terminal‚Äîall without touching VirtualBox or VMware.  If you could get any Linux distro running in just a couple of minutes, what would you use it for?Have questions about Linux or virtualization? Drop a comment or reach out‚Äîalways happy to chat!"
  },
  
  {
    "title": "Automating GitHub Code Check-Ins",
    "url": "/posts/AutoGithub/",
    "categories": "Github, Bash, Automation",
    "tags": "Automation, BashScripting, DevOps, Git",
    "date": "2025-03-09 18:32:00 -0600",
    





    
    "snippet": "I have a bad habit of not checking in my code. Because of this, I‚Äôve ended up with code scattered across multiple machines over the years. A few years back, I started using GitHub, but if I‚Äôm being...",
    "content": "I have a bad habit of not checking in my code. Because of this, I‚Äôve ended up with code scattered across multiple machines over the years. A few years back, I started using GitHub, but if I‚Äôm being honest, only about half of my code actually makes it there. This year, I want that to change.Here‚Äôs a Table of Contents for your article:Table of Contents  Automating the Process  Linking a Project to a GitHub Repository  Setting Up GitHub Authentication  Checking In Code  What‚Äôs Next?This makes navigation easier, especially if you‚Äôre posting it in a markdown-friendly environment like GitHub or a wiki. Let me know if you‚Äôd like any modifications! üöÄAutomating the ProcessThis morning, while creating a new directory structure for my Ethical Hacking Robot project, I realized that the code wasn‚Äôt checked in anywhere. As I designed a bash script to set up the directory structure, it seemed like the perfect time to fix this bad habit. I decided to create two additional scripts‚Äîone to link a project to a GitHub repository and another to handle syncing and pushing code automatically. After testing, both scripts worked well, and they‚Äôve already improved my workflow.Linking a Project to a GitHub RepositoryTo streamline the process, I created a script that links a new project to a GitHub repository as the project directories are being set up. This script relies on a configuration file, setup_config.sh, which includes:  USERNAME ‚Äì The local Linux system username  GITHUB_REPO ‚Äì The GitHub repository URL  GIT_USER_NAME ‚Äì The GitHub account name  GIT_USER_EMAIL ‚Äì The email associated with the GitHub account  GITHUB_API_TOKEN ‚Äì A GitHub API token for authenticationSince storing sensitive credentials directly in the config file is a bad practice, the API token isn‚Äôt hardcoded. Instead, it‚Äôs retrieved at runtime from a secure, restricted file (/home/richard/.github_token). The script structure is straightforward:#!/bin/bash# Load configuration variablessource setup_config.sh  # User and GitHub setupLOGFILE=\"/home/$USERNAME/setup.log\"exec &gt; &gt;(tee -a \"$LOGFILE\") 2&gt;&amp;1# Creates project users and files...# Call GitHub setup scriptbash setup_github.sh \"$USERNAME\" \"$GITHUB_REPO\" \"$GIT_USER_NAME\" \"$GIT_USER_EMAIL\" \"$GITHUB_API_TOKEN\"Setting Up GitHub AuthenticationOnce the script is executed, it sets up SSH authentication for GitHub access. It generates an SSH key for the project user and updates the .ssh/config file to streamline authentication:sudo -u $USERNAME ssh-keygen -t rsa -b 4096 -C \"$USERNAME@$(hostname)\" -f $USER_HOME/.ssh/id_rsa -N \"\"echo \"Host github.com    User git    IdentityFile ~/.ssh/id_rsa    StrictHostKeyChecking no\" | sudo -u $USERNAME tee $USER_HOME/.ssh/config &gt; /dev/nullNext, it uses curl and the GitHub API token to register the SSH key with GitHub:curl -H \"Authorization: token $GITHUB_API_TOKEN\" \\     -H \"Accept: application/vnd.github.v3+json\" \\     --data \"{\\\"title\\\":\\\"$USERNAME@$(hostname)\\\", \\\"key\\\":\\\"$SSH_KEY_CONTENT\\\"}\" \\     https://api.github.com/user/keysWith authentication in place, the script configures Git global settings and either clones the repository or pulls the latest updates if it already exists:sudo -u $USERNAME git config --global user.name \"$GIT_USER_NAME\"sudo -u $USERNAME git config --global user.email \"$GIT_USER_EMAIL\"if [ -d \"$USER_HOME/github/.git\" ]; then    echo \"‚úÖ Repository already exists. Pulling latest updates...\"    sudo -u $USERNAME git -C $USER_HOME/github pull origin mainelse    echo \"üîπ Cloning GitHub repository...\"    sudo -u $USERNAME git clone \"$GITHUB_REPO\" \"$USER_HOME/github\"fiSince the project script creates a new Linux user to run the project under, sudo is necessary throughout this process.Checking In CodeNow that the project is linked to a repository, the next step is automating code check-ins. My repository stores development, staging, and production code. The check-in script specifically syncs development code with GitHub.First, it defines the source and destination directories:SRC_ROS2=\"/home/ros2_dev/ros2_ws/src/\"SRC_NON_ROS=\"/home/ros2_dev/non_ros_code/\"DEST_GITHUB=\"/home/ros2_dev/github/dev/\"Then, rsync is used to sync the development directory with the local Git repository while excluding unnecessary build files:rsync -av --exclude='build/' --exclude='install/' --exclude='log/' \"$SRC_ROS2\" \"$DEST_GITHUB/src/\"# Sync non-ROS2 codersync -av \"$SRC_NON_ROS\" \"$DEST_GITHUB/non_ros_code/\"Finally, the script stages, commits, and pushes the code to GitHub:git add dev/git commit -m \"Backup dev workspace to GitHub on $(date)\"git push origin mainWhat‚Äôs Next?While this setup is a significant improvement, it‚Äôs far from perfect. Right now, there are hardcoded paths everywhere, and the process isn‚Äôt fully automated. These are the next areas I‚Äôll be addressing.If you‚Äôre interested in the full code, you can find it here."
  },
  
  {
    "title": "Crafting a Balanced Patching Strategy",
    "url": "/posts/Balanced-Patching-Strategy/",
    "categories": "Linux, DEVOPS",
    "tags": "Linux, devops, cybersecurity",
    "date": "2024-10-27 10:21:00 -0600",
    





    
    "snippet": "Crafting a Balanced Patching StrategySecurity, Risk, and Automation in HarmonyRegular updates are essential to any cybersecurity model, ensuring that vulnerabilities are quickly patched to minimize...",
    "content": "Crafting a Balanced Patching StrategySecurity, Risk, and Automation in HarmonyRegular updates are essential to any cybersecurity model, ensuring that vulnerabilities are quickly patched to minimize exposure to threats. A common approach is to enable automatic security updates, so servers remain consistently up to date. However, this strategy involves a trade-off: while it reduces vulnerability windows, it also introduces the risk that an update might cause unexpected issues that go unnoticed.  Crafting a Balanced Patching Strategy          Should You Automate?      Risk-Based Patching      Reactive Patching      Security-First Patching      Balancing these risks can be challenging. Delaying updates can extend the time a vulnerability remains active, yet full automation may compromise stability. So, what‚Äôs the best approach? Let‚Äôs explore a middle ground that maximizes both security and reliability.Should You Automate?As discussed above, automation is an effective way to keep your servers updated. In a home lab environment, automated updates ensure systems stay current with minimal oversight. In larger-scale data centers, automation becomes essential-it‚Äôs a cornerstone of a broader strategy that includes Risk-Based Patching to ensure servers remain secure and up to date.Risk-Based PatchingRisk-Based Patching is a strategic approach that aims to minimize the potential impact of problematic patches. This method evaluates the overall business risk if certain servers experience downtime. Servers with the lowest risk are patched first, allowing for thorough testing before moving on to higher-priority systems. This cycle continues until all servers are updated.Automated patching works well in this framework, but what about critical patches that require immediate application?Reactive PatchingSome days, you walk into work with a clear plan and a productive day ahead-until a critical patch needs to be applied immediately, putting everything on hold. Welcome to the world of Reactive Patching.Automation tools like Ansible are invaluable on these days. A robust automation setup should be flexible enough to handle last-minute patches efficiently. So, if automation is in place, why not implement Security-First Patching to ensure essential updates are prioritized?Security-First PatchingMost Linux distributions offer options to apply only security updates, focusing primarily on critical and high-severity vulnerabilities or security flaws. Security-First Patching targets these areas, leaving non-security patches and feature updates aside. While this approach enhances security, it can sometimes lead to unexpected application issues due to missing non-security updates.Each of the above approaches has its own pros and cons, but when combined, they create an effective patching strategy. Regularly applying security patches on a short cycle, along with scheduled full patches, keeps servers consistently up to date.When possible, Risk-Based Patching should be followed: start with development servers, move to staging, and finally, apply patches to production. For Reactive Patching, automated tools should follow this same process, ensuring consistency and minimizing disruption.What‚Äôs one aspect of your current patching strategy that you would improve?DesciptionsExplore effective patching strategies that balance security, risk management, and automation. This guide covers approaches like Security-First, Risk-Based, and Reactive Patching to help keep systems secure and reliable without compromising stability."
  },
  
  {
    "title": "Level Up Your Linux Scheduling",
    "url": "/posts/Linux-Scheduling/",
    "categories": "Linux, DEVOPS",
    "tags": "Linux, systemd, scheduling, devops",
    "date": "2024-10-14 11:59:00 -0600",
    





    
    "snippet": "An Intro to systemd TimersWhen I first began using Linux, crontab was the go-to tool for scheduling tasks on Unix and Linux systems. Its flexibility in a single line of text was impressive, allowin...",
    "content": "An Intro to systemd TimersWhen I first began using Linux, crontab was the go-to tool for scheduling tasks on Unix and Linux systems. Its flexibility in a single line of text was impressive, allowing you to create highly specific schedules. For instance, if you wanted a task to run on the 15th of each month at 5:45 am but only on Mondays, crontab could handle it effortlessly with the simple format: 45 5 15 * 1, followed by your task command. In 2010, however, systemd emerged, it offering an alternative method for scheduling tasks, adding new capabilities to Linux systems.  Level Up Your Linux Scheduling: An Intro to systemd Timers          What is systemd      Difference between systemd and crontab      Times File      Service File      Scheduling  Timer      What is systemdSystemd is a service management tool that takes over once the boot process is complete, initiating and managing essential processes during system startup and throughout runtime. One of its key features is the ability to schedule tasks, similar to what crontab offers, but with additional capabilities and greater flexibility for system administrators.Difference between systemd and crontabOne fundamental difference between cron jobs and systemd timers is how the scheduled tasks are managed. With cron, tasks are user-specific, and identifying which user scheduled a particular job isn‚Äôt always straightforward. In contrast, systemd clearly defines the user who will run a service within the [Service] section of the service file.Times FileTo schedule a process with systemd, you‚Äôll need two configuration files: a timer file and a service file. The timer file defines when a process will run and follows the same initialization (INI) file format as most systemd files, organized into sections.  The first section, [Unit]          describes what the process will do and may contain sequencing conditions to ensure that required processes are running before this one starts.        Next, the [Timer] section          specifies when the systemd process should run. This section includes functions similar to crontab,      but with additional options such as OnBootSec=15min, which starts the process 15 minutes after boot.      If the timer and service files have different names, the service can be referenced directly in this section.        Finally, the [Install] section          provides options for additional configurations, such as ensuring the process starts on boot or specifying the target runlevel.      Service FileTo start a process or daemon service with systemd, you create a service file. Like the timer file, the service file is organized into three main sections: [Unit], [Install], and [Service].  The [Unit] and [Install] sections function similarly to those in the timer file, handling descriptions and installation options.  The [Service] section, however, is where the main task is defined.Below is an example [Service] configuration for running an Ansible script that performs system updates:[Service]# Service call to perform updatesUser=ansible_adminWorkingDirectory=/opt/ansible/projects/ExecStart=/usr/bin/ansible-playbook ansible_update.ymlThis setup specifies the following:  The User directive determines which user the service runs as.  WorkingDirectory defines the startup directory for the process.  ExecStart calls the Ansible script.All that is needed is to start the schedule.Scheduling  TimerTo start and manage timers, systemd uses the systemctl command to enable, start, or disable timers. For instance, to enable and start a timer immediately, you would use:systemctl enable --now &lt;service_timer.timer&gt;You can view all scheduled timers with systemctl --list-timers, which displays timers currently set to run.For the code and configuration of the systemd timer and service created to run an Ansible script for Linux updates, see this code example and read the article here.This article provided a high-level overview of using systemd timers, though systemd offers a wide range of powerful options beyond what we‚Äôve covered here.When I first heard about systemd timers, I questioned the need to change such a reliable task scheduler as crontab. However, after experiencing systemd timers, I‚Äôve become a true convert.Have you started using systemd timers to schedule tasks? If not, what scheduling tool are you currently using?"
  },
  
  {
    "title": "Getting Started with Ansible - Automation Meets Cybersecurity",
    "url": "/posts/Getting-Started-with-Ansible/",
    "categories": "ProxoxVE, Cybersecurity, Ansible",
    "tags": "Linux, Ansible, Cybersecurity, Automation",
    "date": "2024-10-14 11:59:00 -0600",
    





    
    "snippet": "Ansible SetupIn this article, I will walk you through setting up the Ansible Control server.In future articles,we will create Ansible playbooks to deploy Zero Trust Access (ZTA) principles across m...",
    "content": "Ansible SetupIn this article, I will walk you through setting up the Ansible Control server.In future articles,we will create Ansible playbooks to deploy Zero Trust Access (ZTA) principles across my home lab environment.So, what exactly is Ansible?Table of content:  What is Ansible  The Chicken-and-Egg Problem   Building the Control Server  Getting Hostnames Using AnsibleWhat is AnsibleAnsible is an automation tool used to execute tasks on remote servers. It relies on YAML-formatted files, known as Playbooks, to define these tasks, which are then pushed and executed on the remote servers using SSH.At its core, a Playbook is a simple list of tasks that are run against a set of servers by a specified user.Where to Start: The Chicken-and-Egg ProblemSetting up the Ansible Control server with Zero Trust Access (ZTA) in mind requires a few additional steps:  Creating a VLAN for the Ansible server  Configuring the network settings on the Ansible server  Setting firewall rules to allow access  Creating and managing users for the Ansible server  And a few other tasks along the wayThe big question is: Do I set everything up manually, including ZTA changes, or do I first set up a basic Ansible server and use it to automate the migration to a ZTA model?Time to buildBuilding the Control ServerI have decided to go with the latter approach since it aligns with the philosophy of building cybersecurity into the process from the start. Here is how I set up the Ansible Control server on a fresh installation of Oracle Linux 9.Step 1: Install Ansible on Oracle Linux 9First, ensure your system is up to date:# Check for updates first  sudo dnf update  Next, install Ansible:# Install Ansible  sudo dnf install ansible  To confirm the installation, run the following command:# Test if Ansible is installed  ansible  Step 2: Add an Ansible Admin UserWe need to create an admin user for Ansible:# Create a user with sudo privileges for Ansible on remote serverssudo useradd ansible_admin  Step 3: Grant Sudo Access to the Ansible UserTo allow the ansible_admin user to run commands with sudo privileges without entering a password, we will modify the sudoers file. Depending on the system, hereis where you can find the file:  OPNsense: /usr/local/etc/sudoers  RHEL, Debian: /etc/sudoersAdd the following line near the bottom of the file:## Grant sudo access without a password to the Ansible user  ansible_admin ALL=(ALL) NOPASSWD: ALL    Note:In the future, I‚Äôll be setting up multiple Ansible users with more granular permissions to enhance server security.4: Create the Ansible Hosts FileThe Ansible hosts file defines the servers you want to manage. There are several places where you can store this file, but I‚Äôll use the main configuration:# Edit the main hosts file  sudo nano /etc/ansible/hosts  Below is the content I used for my setup:[firewall]  # OPNsense Firewall  Firewall  [proxmox]  # Proxmox VE and Backup Server  PVE  PBS  [nas]  # Orange Pi NAS  NAS  [ansible_node]  # Local Control Node  control_node  Now That the Build is Complete, Let‚Äôs Test It Out Getting Hostnames Using AnsibleWith Ansible installed and the servers configured, let‚Äôs test it by running a simple playbook that gathers the hostname and OS type from all the connected servers.Step 1: Create the Ansible Playbook  Start by creating a new file called system_info.yml:    nano system_info.yml          Add the following lines to the file and save:```yaml‚Äî          name: Gather hostname and OS type from all hostshosts: allgather_facts: yestasks:                  name: Get hostname and OS typeansible.builtin.debug:  msg: ‚ÄúHost: , OS: ‚Äú```                    Step 2: Run the PlaybookTo execute the playbook, use the following command:ansible-playbook system_info.yml  Step 3: Review the OutputAnsible can be quite verbose, but the key part of the output will look like this:TASK [Get hostname and OS type] **********************************************************************************************************************************************ok: [firewall] =&gt; {      \"msg\": \"Host: firewall, OS: FreeBSD\"  }  ok: [pve] =&gt; {      \"msg\": \"Host: pensask, OS: Debian\"  }  ok: [pbs] =&gt; {      \"msg\": \"Host: bkp1, OS: Debian\"  }  ok: [ansible] =&gt; {      \"msg\": \"Host: stock, OS: RedHat\"  }  ok: [samba] =&gt; {      \"msg\": \"Host: orangepi5plus, OS: Debian\"  }  Step 4: Final ThoughtsSome of the hostnames could use a bit of work, but no worries-Ansible can help streamline those details later. This playbook confirms that the servers are reachable and that Ansible is properly configured to gather system information from all the hosts.With Ansible up and running, the next step is to start adding additional layers of security to the remote servers using Ansible. Stay tuned-there‚Äôs more to come!Ansible is a powerful automation tool that simplifies complex tasks, making server management more efficient.  If you had an automation tool like Ansible, what tasks would you automate?"
  },
  
  {
    "title": "Automating Server Updates",
    "url": "/posts/Automating-Server-Updates/",
    "categories": "ProxoxVE, Cybersecurity",
    "tags": "ansible, devops, Linux, cybersecurity",
    "date": "2024-10-14 11:59:00 -0600",
    





    
    "snippet": "A Simple Yet Scalable ApproachRegular OS patching is a crucial part of maintaining cybersecurity, as it helps reduce the risk of zero-day vulnerabilities and other security issues by ensuring that ...",
    "content": "A Simple Yet Scalable ApproachRegular OS patching is a crucial part of maintaining cybersecurity, as it helps reduce the risk of zero-day vulnerabilities and other security issues by ensuring that patches are applied soon after they‚Äôre released. However, when updates aren‚Äôt automated, it leaves a window where vulnerabilities remain active, making the decision to automate or not a difficult one. In my home lab, I work with a variety of operating systems, each using different update methods, which makes finding a scalable patching solution increasingly important as my lab expands. This is why I‚Äôve decided to use Ansible for managing updates‚Äîlet me explain why it‚Äôs the right tool for the job.Table of content:  Automating Server Updates: A Simple Yet Scalable Approach          Understanding Ansible      The Update Tasks      What User Will Run the Tasks      Where to Run the Playbook      Understanding AnsibleAnsible is a powerful automation tool that simplifies repetitive tasks and system management. For a more in-depth overview, click here. At its core, Ansible uses Playbooks to execute one or more tasks in sequence. Each task typically consists of a name, a module, arguments for that module, and‚Äîwhen it comes to updates‚Äîcertain conditions to determine when the task should run. While Ansible offers many other advanced features, we‚Äôll be focusing on this basic structure for now.The Update TasksIn my home lab, I‚Äôm working with three different operating systems: two Linux distributions and one FreeBSD system. To handle updates for all of them, the Ansible Playbook needs to include tasks tailored to each OS. For Red Hat-based distributions like RHEL, CentOS, Rocky, and Oracle, I use the yum module. Debian-based systems, such as Ubuntu and Mint, require the apt module, and for FreeBSD, the pkgng module does the job. Thankfully, Ansible has built-in modules for all of these package managers, making it easy to manage updates across different platforms.Here‚Äôs how the Playbook is structured:  tasks:    # Task to update RedHat family servers (RHEL, CentOS, Rocky, Oracle)    - name: Update RHEL family servers, including RHEL, CentOS, Rocky, Oracle...      yum:        name: '*'                   # Update all packages        security: yes               # Apply security updates        state: latest               # Ensure packages are at their latest version      when: ansible_facts['os_family'] == 'RedHat'  # Run only on RedHat family systems    # Task to update Debian family servers by refreshing package cache    - name: Update Debian family servers      apt:        update_cache: yes           # Refresh the APT package cache      when: ansible_facts['os_family'] == 'Debian'  # Run only on Debian family systems    # Block to handle updates for OPNsense (FreeBSD-based system)    - block:        # Task to check for available updates on OPNsense        - name: Check for updates on OPNsense          ansible.builtin.shell: \"opnsense-update -c\"          register: update_check     # Register output for conditional check          changed_when: false        # Mark as not changed (no need to record as a change)          ignore_errors: true        # Continue playbook if this command fails        # Task to apply updates if updates are available        - name: Update OPNsense if updates are available          ansible.builtin.shell: \"opnsense-update -u &amp;&amp; opnsense-update -bk\"          when:            - ansible_facts['os_family'] == 'FreeBSD'  # Run only on FreeBSD systems (OPNsense)            - update_check.stdout != \"\"                # Ensure updates are available before proceeding          register: update_result                      # Register result for reboot condition        # Task to reboot OPNsense after updates        - name: Reboot OPNsense to apply updates          ansible.builtin.shell: \"reboot\"          async: 1                                     # Run asynchronously to allow reboot          poll: 0                                      # Detach from task immediately          when:            - ansible_facts['os_family'] == 'FreeBSD'  # Run only on FreeBSD systems (OPNsense)            - update_result is defined                 # Ensure update task has a result            - update_check.stdout != \"\"                # Check if updates were available          ignore_errors: true                          # Continue playbook if reboot task fails      when: ansible_facts['os_family'] == 'FreeBSD'    # Apply this block only on FreeBSD systemsNow that we know what needs to be done, the next step is figuring out under which user or permissions we‚Äôll be executing these tasks.What User Will Run the TasksThe next step is determining which user the tasks in the Playbook will run as on the remote servers. In my setup, I‚Äôve already created an Ansible user on each of the remote servers, so we‚Äôll be using that user to execute the tasks.  become: root  become: yes  remote_user: ansible_admin  Note: The ansible_admin user was set up in a previous article, which you can check out here.Now that we have the user sorted, the final step is deciding where to run the Playbook.Where to Run the PlaybookThe final piece of the puzzle is deciding which servers the tasks will run on. This is defined in the Ansible hosts file, which lists all the servers to be managed. In my home lab, the Playbook runs across all servers.- name: Update servers in my home lab  hosts: allIn a corporate environment, you would typically follow a more structured patching strategy, rolling out updates in stages. For more information on corporate patching strategies, you can learn more here.With the Ansible Playbook in place, the next step is to set up a systemd timer to schedule the job, ensuring my servers are consistently kept up to date. While there may be cases where a reboot is necessary after applying updates, that will be addressed in a future project.You can find the complete code for this article here, including the systemd timer setup to automate the Playbook execution.How do you manage patching to keep your servers up to date?"
  },
  
  {
    "title": "Ansible and Cybersecurity",
    "url": "/posts/Ansible-Cybersecurity/",
    "categories": "ProxoxVE, Cybersecurity",
    "tags": "Ansible, Linux, Cybersecurity, ZTA",
    "date": "2024-10-10 19:11:00 -0600",
    





    
    "snippet": "CybersecurityIn today‚Äôs world, cybersecurity is more critical than ever. Whether it‚Äôs adding new functionality, upgrading existing features, or auditing current systems, the question of how to enha...",
    "content": "CybersecurityIn today‚Äôs world, cybersecurity is more critical than ever. Whether it‚Äôs adding new functionality, upgrading existing features, or auditing current systems, the question of how to enhance security always comes up.How many of us have been disappointed when a solution doesn‚Äôt address security concerns due to time constraints or budget limitations? Is there a way to integrate security into the core of our solutions from the start?And how do you achieve this when working with virtual machines (VMs) in a distributed architecture?Cybersecurity and Automation ToolsCybersecurity focuses on the processes and procedures that protect corporate infrastructure from attacks. When breaches do occur, these processes help limit the attacker‚Äôs access and minimize damage. Automating these processes can significantly reduce the cost of maintaining a secure environment.Automation tools like Ansible, Puppet, Chef, SaltStack, and Terraform are widely used in the industry, with Ansible and Terraform being particularly prominent leaders. In this article, I will focus on strategies that can be implemented using Ansible to enhance cybersecurity while lowering costs.What is AnsibleAnsible is an automation tool used to execute tasks on remote servers. It relies on YAML-formatted files, known as Playbooks, to define these tasks, which are then pushed and executed on the remote servers using SSH.At its core, a Playbook is a simple list of tasks that are run against a set of servers by a specified user.But how does that work?Ansible MindsetLike any tool, Ansible can be used as simply or as complexly as you need. Personally, I approach Ansible with a ‚Äúto-do list‚Äù mindset. Breaking down tasks into small, manageable steps and then linking them into larger workflows has always been the most effective approach for me.For instance, if I need to make a change to a server, such as installing and configuring a Samba service on a VM, I break it down into a series of steps:  Take a snapshot of the VM  Update the repositories on the Linux server  Apply any necessary updates  Reboot, if required  Set up new users or groups  Install Samba  Configure the server files  Create new directories for shared folders  Assign permissions to shared folders  Add firewall rules  Start the Samba service  Test the Samba server  Remove the snapshot once everything is workingEach of these steps can be divided into one or more tasks, which can then be combined into a role within a Playbook. A series of roles or tasks can be grouped together in a Playbook to handle the entire process of installing Samba.However, simply following these steps doesn‚Äôt inherently improve security.Adding Security to the ProcessWhen security is integrated into this process and included in Ansible scripts, it ensures that all new installations consistently follow established security protocols. For example:  Samba configuration files adhere to a predefined enterprise standard.  Copies of configuration files are stored in a central repository for auditing.  User and group setups follow enterprise procedures.  Checkpoints are added to verify that:          Only authorized users have access to new shared folders.      The firewall is enabled, and necessary rules are applied.      The Ansible script created for installing Samba can easily be adapted to install Apache, with the base security measures already in place. This means only the Apache-specific changes are needed, saving time while maintaining a high level of security.Once these security measures are embedded into the scripts, every future execution will automatically follow these security standards. While it‚Äôs still a manual process to verify that these security protocols are followed, that‚Äôs where code reviews come in.So Why Do This?Security models like Zero Trust Access (ZTA) introduce additional steps to ensure that solutions are secure. If these steps aren‚Äôt followed, new attack surfaces can be created, which attackers can exploit.By integrating the security models into your automation tool, you incur a one-time development effort and minimal ongoing cost. Once the models is in place, security becomes an integral part of the process.This approach also ensures a consistent security models across the enterprise. As security needs evolve, the tools required for deployment are already in place, making it much easier to roll out updates and maintain security standards.What are you doing to integrate security into your enterprise solutions?"
  },
  
  {
    "title": "Proxmox NAS Storage - Securing a Samba DAS",
    "url": "/posts/Securing-Samba-DAS/",
    "categories": "Proxomx, NAS, Samba",
    "tags": "samba, cybersecurity, NAS, linux",
    "date": "2024-09-29 15:22:00 -0600",
    





    
    "snippet": "With the network and firewall rules configured, there are just a few final tasks to complete on the Samba server:  Enhance Samba security  Mount the DAS volumes on the serverWhile this may seem red...",
    "content": "With the network and firewall rules configured, there are just a few final tasks to complete on the Samba server:  Enhance Samba security  Mount the DAS volumes on the serverWhile this may seem redundant given the firewall settings, the next step adds an extra layer of protection. We‚Äôll limit access to the Samba server by specifying allowed hosts directly in the Samba configuration file. This ensures that only designated servers can connect, further tightening security.Finalizing Samba SecurityWhen we first set up the Samba server, we configured allowed users and set the minimum SMB protocol. Now, it‚Äôs time to finish securing the server by restricting which computers can access it using the hosts allow directive in the Samba configuration.Here‚Äôs how I‚Äôve set up host access control:  Proxmox Share the Samba config file:# 192.168.177.129 - Proxmox Backup Server# 192.168.177.7 - Proxmox Server Management Port# 192.168.178.0/24 - Proxmox VM Netork# This also keeps my laptops off this share 192.168.166.0/24hosts allow = 192.168.177.129 192.168.177.7  192.168.178.0/24This configuration ensures that only the necessary servers can access the Samba pbe share. It also prevents my personal laptop, which are on the 192.168.166.0/24 subnet, from accessing these shares.  Media Share the Samba config file:## 192.168.168.0/24 - Allow device on WI-FI to access media## 192.168.166.66 - Allow personal desktop but stop work laptop access## Proxmox server do not have access hosts allow = 192.168.168.0/24 192.168.166.66This configuration ensures that only the Wi-Fi devices can access the Samba media share, while also allowing my personal laptop to share access to the media.Now, let‚Äôs move on to mounting the DAS volumes and securing them.Mounting DAS DrivesThe DAS has five bays, and two of the drives are being mounted for Samba use. Since these drives were previously used on a Linux server and already contain data, there‚Äôs no need for partitioning or formatting.Information Needed to Mount the DrivesTo mount a drive on a Linux system using the /etc/fstab file, you‚Äôll need the following details:  What is being mounted ‚Äì The device name or UUID of the drive  Where it is being mounted ‚Äì The target directory (mount point)  File system type ‚Äì The type of file system (e.g., ext4, xfs)  Mount options ‚Äì Specific options for mounting (e.g., defaults, noatime)  Dump ‚Äì Whether the filesystem should be backed up by the dump command  Pass ‚Äì The order in which filesystems should be checked by fsck (file system check)To mount the drives correctly, I needed to gather some essential information, such as the UUID and file system type (FSTYPE) of the drives. Here‚Äôs how I did it:Identifying the Drives and File System TypesI used the lsblk (list block devices) command to display the hard drives and their details, including the UUID and file system type. Here‚Äôs an example of the output:lsblk -f  # returns the followingNAME         FSTYPE FSVER LABEL    UUID                                 FSAVAIL FSUSE% MOUNTPOINTSsda                                                                                    sdb                                                                                    sdc                                                                                    sdd                                                                                    ‚îî‚îÄsdd1       ext4   1.0            3208c81f-0714-42bb-a8fc-adfbfc4ca336                sde                                                                                    ‚îî‚îÄsde1       ext4   1.0            65a350ce-4548-4ef1-86bb-e48965924224   Note: This project uses UUIDs instead of device names for added stability and security.  Drives can be mounted using either the device name (e.g., /dev/sdd1) or the UUID (e.g., 3208c81f-0714-42bb-a8fc-adfbfc4ca336). However, device names are assigned at boot and can change if new disks or partitions are added or removed.  The UUID, on the other hand, is a unique identifier assigned to a partition and only changes when the partition is recreated. This ensures that even if partitions or disks are added or removed, the UUID remains the same.  Additionally, using the UUID ensures that if a drive is removed from the DAS and replaced with a different one, the system will not automatically load the new drive. While this adds a layer of security, it also means that the system won‚Äôt support hotswapping drives without further configuration.Setting Up Mount Point OptionsNow, let‚Äôs configure the options for the mount points. Since this system will be used as storage for a NAS and is non-production, I‚Äôve opted for security over performance. After reviewing the available options, the following settings seem to fit the requirements:  atime: Ensures file access timestamps are updated. This can improve security by tracking file access but may impact performance in some cases.  noexec: Prevents the execution of binaries on this mount, adding an extra layer of security.  nodev: Disallows the creation or usage of device files, which enhances security on the storage volume.  errors=remount-ro: If any errors are detected on the filesystem, this option automatically remounts the filesystem as read-only, preventing further issues.Finally, the seutp for dump and passConfiguring Dump and PassSince the dump command is not installed on the server, I‚Äôve set the dump value to 0, meaning the filesystem will not be included in backup routines via dump.For the pass value, I‚Äôve chosen to set it to 2, which ensures that the mounted filesystems will be checked during boot. While this will slightly slow down the boot process, it‚Äôs acceptable for a back-end system that isn‚Äôt rebooted frequently.The possible values for pass are:  0: Do not check the filesystem at boot.  1: Check the filesystem first, typically used for the root (/) filesystem.  2: Check the filesystem after the root filesystem has been checked.Updating the fstab FileDuring boot, Linux systems reference the /etc/fstab file to mount additional hard drives. To finalize this project, let‚Äôs bring everything together and add the appropriate entries to the fstab file.Here‚Äôs how we‚Äôll define the lines based on the information we‚Äôve gathered:## From DAS bay 1UUID=3208c81f-0714-42bb-a8fc-adfbfc4ca336 /srv/nas_storage/media ext4 atime,atime,nodev,noexec,errors=remount-ro 0 0## From DAS bay 2UUID=65a350ce-4548-4ef1-86bb-e48965924224 /srv/nas_storage/pve ext4  atime,nodev,noexec,commit=600,errors=remount-ro 0 0With all the changes in place, I rebooted the server and tested the mounts‚Äîthey all worked perfectly.Looking back, if I were to do this again, I might choose a different device than the Orange Pi 5 Pro. The version of Debian available for it doesn‚Äôt support Logical Volume Manager (LVM), which would have allowed me to combine the hard drives into a single large logical volume for better storage flexibility.While the initial NAS setup took a few steps, adding additional shared folders in the future will be much simpler.If you had a NAS, how would you use it?"
  },
  
  {
    "title": "Proxmox Network Storage Firewall Rules",
    "url": "/posts/Proxmox-Network-Storage-Firewall/",
    "categories": "Proxmox, Firewall",
    "tags": "proxmox, NAS, storage, Firewall",
    "date": "2024-09-26 19:41:00 -0600",
    





    
    "snippet": "Proxmox Network StorageSo far, we‚Äôve utilized a Direct Attached Storage (DAS) and a Single Board Computer (SBC) to set up a NAS (Network Attached Storage) using Samba. The next step was securing th...",
    "content": "Proxmox Network StorageSo far, we‚Äôve utilized a Direct Attached Storage (DAS) and a Single Board Computer (SBC) to set up a NAS (Network Attached Storage) using Samba. The next step was securing the NAS by placing it on a dedicated VLAN subnet, isolating it from other devices in my home lab. This setup effectively reduces the attack surface and limits exposure to only the systems that need access. Now, we‚Äôll focus on configuring firewall rules to ensure secure communication between the authorized devices and the NAS.Firewall AliasesWhen setting up firewall rules, you‚Äôre assigning permissions for devices to access other devices. This can be done by specifying attributes such as MAC addresses, IP addresses, ports, and Fully Qualified Domain Names (FQDNs). However, managing these values, especially MAC addresses or IPs, can become cumbersome and difficult to remember after creating the rules.For example, the dock for my MacBook Air has the MAC address 3A:2C:4F:5E:8B:1A. Instead of trying to remember that, I can create an alias called macbookair_dock on the firewall. Now, whenever I use macbookair_dock in my firewall configuration, the system will recognize it as the corresponding MAC address 3A:2C:4F:5E:8B:1A, making it easier to manage.For this project, I‚Äôve created the following aliases:            Alias      Type      Value                  macbookair_dock      MAC address      3A:2C:4F:5E:8B:1A              iPad      MAC address      F2:77:3C:6D:1E:82              iPhone      MAC address      09:AF:BA:63:92:4E              ProxmoxVE      Host(s)      192.168.177.7,192.168.177.8              ProxmoxBS      Host(s)      192.168.177.129              SambaPorts      Port(s)      137,138,139,445              SambaPve      Host(s)      192.168.197.7              SambaMedia      Host(s)      192.168.198.8      With these aliases in place, the firewall rules we create in the next section will be much clearer and easier to manage.  Note: MAC vs HostsThe servers on my network have static IP addresses, so using IP address would work well for them. However, many other devices do not have static IPs, making their IP addresses less reliable for identification. In these cases, using the MAC address (Media Access Control address) provides a more consistent and stable way to reference those devices.Proxmox Servers to NASThe Proxmox servers will be utilizing the NAS for two main purposes:  ISO storage  BackupsTo enable this, the following firewall rules were necessary:            #      Interface      Source      Destination      Ports                  1      VLANforProxmox      ProxmoxVE      SambaPve      SambaPorts              2      VLANforProxmox      ProxmoxBS      SambaPve      SambaPorts      Rule 1:This rule permits the Proxmox servers to store ISO images on the NAS and ensures that virtual machines (VMs) can access the necessary storage resources for proper operation.Rule 2:This rule enables the Proxmox Backup Server to securely store backups on the NAS, ensuring that data is protected and recoverable as needed.Virtual Servers to NASThe virtual servers will use the NAS for shared storage. To facilitate this, the following firewall rules are required:            #      Interface      Source      Destination      Ports                  1      VLANforProxmoxVM      ProxmoxVE      SambaPve      SambaPorts      WiFi to MediaThis setup allows my iPhone and iPad to access documents and media stored on the NAS. The following firewall rules were implemented to ensure secure access:Since access was not granted to the entire WiFi subnet, any additional users connecting to the WiFi network will not have access to the files on the shared NAS. This ensures a higher level of security by restricting access to only authorized devices.At this point, the Samba service has been installed, the VLANs are configured, and the firewall rules are in place. In the next article, I will cover mounting external hard drives and configuring Samba to further restrict access to the shared resources."
  },
  
  {
    "title": "Streamlining SSH Key Management",
    "url": "/posts/Streamlining-SSH-Key-Management/",
    "categories": "SSH, Auth Keys",
    "tags": "SSH, servers, cybersecurity",
    "date": "2024-09-23 13:12:00 -0600",
    





    
    "snippet": "Automating and Securing SSH Configurations with a Custom ScripWhen managing multiple servers on a daily basis, SSH authentication keys are invaluable. In the past, I would typically create a single...",
    "content": "Automating and Securing SSH Configurations with a Custom ScripWhen managing multiple servers on a daily basis, SSH authentication keys are invaluable. In the past, I would typically create a single key pair and reuse it across all my servers. While convenient, this approach poses a significant security risk‚Äîif that key were to be compromised, it could expose all of my servers to potential threats. To mitigate this risk, generating unique key pairs for each server is the best practice, but managing all those keys can quickly clutter the .ssh directory. To address this, I developed a Bash script that not only generates key pairs but also keeps the directory organized and simplifies the process overall.the .ssh/configSSH relies on the .ssh/config file to store information about various SSH connections. In this file, you define several key parameters:  Host ‚Äì The alias used to reference the server.  HostName ‚Äì The server‚Äôs IP address or DNS name.  User ‚Äì The username SSH will use to log in.  IdentityFile ‚Äì The path to the SSH key pair for authentication.By using include files within the SSH config, it becomes much easier to organize multiple server logins. The script I developed automates this process by creating individual config files for each server and linking them back to the main .ssh/config file. Here‚Äôs how the script works:How the Script WorksThe script takes a hostname, IP address, and username as inputs. It then generates the necessary SSH keys, creates a config file for the server, and automatically links that file to the main .ssh/config file.Server DirectoryCreates the directory to store the SSH key pair and configuration file# Define directoriesconfig_directory=/home/${local_user}/.ssh/include.d/${host_name}echo \"${config_directory}\"# Check if the SSH config directory exists, create it if notif [ ! -d \"${config_directory}\" ]; then  echo \"Creating SSH config directory at ${config_directory}...\"  mkdir -p ${config_directory}  if [ $? -ne 0 ]; then    echo \"Error: Failed to create config directory at ${config_directory}\"    exit 1  fifiFile PermissionsTo secure the keys generated in the next steps, the default file permissions must be set to 600. This ensures that only the file owner has read and write access. The following code sets the correct permissions on the parent directory to maintain this level of security.# Set default ACL permissions on the login directorysetfacl -d -m u::rw,g::-,o::- ${config_directory}if [ $? -ne 0 ]; then  echo \"Error: Failed to set ACL on ${config_directory}\"  exit 1fiGenerates the KeysWith a designated place to store the keys, the script proceeds to generate the SSH key pair and copy them to the server  Note: The script does not prompt for a password when generating the key pair, but it will ask for the remote server‚Äôs login password during the connection process.# Generate a new SSH key pair using ed25519 algorithm, with no passphrase (-N \"\")ssh-keygen -t ed25519 -f ${config_directory}/${host_name} -N \"\"  # Empty passphraseif [ $? -ne 0 ]; then  echo \"Error: SSH key generation failed for ${host_name}\"  exit 1fi# Copy the SSH key to the remote hostecho \"Copying SSH public key to ${user}@${ip_address}...\"ssh-copy-id -i ${config_directory}/${host_name}.pub ${user}@${ip_address}if [ $? -ne 0 ]; then  echo \"Error: Failed to copy SSH public key to ${user}@${ip_address}\"  exit 1fi.ssh config File for ServerThe config file for the new login is created and stored in the same directory as the SSH keys.# Create the config file inside the config_directory and write the necessary SSH config linesecho \"Creating SSH config file at ${config_directory}/config...\"cat &lt;&lt;EOL &gt; ${config_directory}/configHost ${host_name}     HostName ${ip_address}     User ${user}     IdentityFile ${config_directory}/${host_name}EOLMaking Server CallableOne of the key benefits of using a config file is that it simplifies connecting to servers. By linking the server‚Äôs address and username to a single alias, you can easily initiate a connection with a simple ssh &lt;alias&gt; command, eliminating the need to remember or type the full server details each time.The next step adds the server‚Äôs config file to the main .ssh/config, ensuring it‚Äôs included for future SSH connections.# Append the Include line to ~/.ssh/config if it's not already presentssh_config_file=/home/${local_user}/.ssh/config# Ensure the ~/.ssh/config file existsif [ ! -f \"${ssh_config_file}\" ]; then  touch \"${ssh_config_file}\"  chmod 600 \"${ssh_config_file}\"fi# Check if the Include line is already in the fileif ! grep -Fxq \"Include ${config_directory}/config\" \"${ssh_config_file}\"; then  echo \"Adding 'Include ${config_directory}/config' to ${ssh_config_file}...\"  #echo \"Include ${config_directory}/config\" &gt;&gt; \"${ssh_config_file}\"  echo \"Include ${config_directory}/config\" | cat - \"${ssh_config_file}\" &gt; temp_file &amp;&amp; mv temp_file ${ssh_config_file}  if [ $? -ne 0 ]; then    echo \"Error: Failed to append the Include line to ${ssh_config_file}\"    exit 1  fifiTest the ConnectionFinally, it test the login:# Try to SSH into the server using the newly created keyssh ${host_name}if [ $? -ne 0 ]; then  echo \"Error: Failed to connect to ${host_name}\"  exit 1fiecho \"SSH key successfully created, user logged into the server, and config files updated.\"Don‚Äôt forget to logout the remote server at the end of the script.Creating a new authentication key and login is simple:Usage: create_ssh_login.bash &lt;host_name&gt; &lt;ip_address&gt; &lt;username&gt;Once the script completes, you can easily connect to the server with:ssh &lt;hostname&gt;Running it for:create_ssh_login.bash proxmox 192.168.177.7 richard  will create the blow directory structure.~/.ssh‚îú‚îÄ‚îÄ config‚îú‚îÄ‚îÄ include.d/‚îÇ   ‚îî‚îÄ‚îÄ proxmox/‚îÇ      ‚îú‚îÄ‚îÄ proxmox.pub‚îÇ      ‚îú‚îÄ‚îÄ proxmox‚îÇ      ‚îú‚îÄ‚îÄ configWhat I appreciate about this script is that it enhances SSH key security while making it easier to generate keys and keep them well-organized.For the full version of the script, visit https://github.com/richard-sebos/Streamlining-SSH-Key.What steps are you taking to secure your SSH connections?"
  },
  
  {
    "title": "Implementing VLANs for Proxom and NAS",
    "url": "/posts/Proxmox-NAS-VLANs/",
    "categories": "ProxoxVE, Cybersecurity, NAS",
    "tags": "proxmox, nas, VLAN, security",
    "date": "2024-09-22 17:56:00 -0600",
    





    
    "snippet": "Proxmox: Network for NASIn the previous post, we completed the Samba server setup, though the network connection strategy was still in development. Now, it‚Äôs time to explore how the Samba server wi...",
    "content": "Proxmox: Network for NASIn the previous post, we completed the Samba server setup, though the network connection strategy was still in development. Now, it‚Äôs time to explore how the Samba server will be integrated into the network to fulfill various roles:  Proxmox VE/Backup Server (BS): Providing dedicated storage for backups and ISO images  Proxmox VMs: Enabling shared storage for virtual machines in the Proxmox environment  Personal Use: Serving as a central hub for managing documents and mediaIn this post, I‚Äôll guide you through setting up the network for this homemade NAS, ensuring it‚Äôs ready to meet the needs of both your Proxmox environment and personal use.Bridging vs. VLANWhen setting up the Samba server, I had two network cards at my disposal. Initially, I considered bridging the connections, with one card linked to the Proxmox network and the other connected to WiFi. However, I wanted to isolate the networks for better security, but I ran into a limitation‚Äîthere weren‚Äôt enough physical ports.That‚Äôs where VLANs came in.Between the two options, VLANs offered a more secure and scalable solution. Ultimately, I chose VLANs to segment and secure the network effectively.VLANThe simplest way to understand VLANs is to think of them as virtual networks that share the same physical network interface but remain logically separated. Through a process called ‚Äútagging,‚Äù VLANs add an identifier to each network packet, ensuring that data is routed only to the devices within the same VLAN. This isolation boosts security and improves traffic management, especially in environments with multiple devices and different access requirements.In my case, I created two VLANs on two of the four LAN ports on my firewall to segment the traffic:  VLAN Tagging: Each network packet is tagged with a VLAN ID, ensuring it stays within its designated network, preventing interference with other VLANs.  Improved Security and Control: VLANs allow me to separate traffic, ensuring that my Proxmox network, personal devices, and other systems are isolated from each other. This limits access and reduces the risk of unauthorized traffic.  Efficient Use of Hardware: Instead of needing multiple physical network interfaces for each network, VLANs enable me to use a single interface to handle multiple networks, maximizing the efficiency of my existing hardware.Here‚Äôs how I set up VLANs in my configuration:Network IsolationTo enhance the security and organization of my Proxmox servers and NAS, I created four VLANs on my OPNsense firewall to ensure each component of the infrastructure is properly isolated:  VLAN 10 - Proxmox Management          Used for accessing the Proxmox web interface      Handles backup traffic between Proxmox VE and Proxmox Backup Server        VLAN 20 - Proxmox VMs          A dedicated subnet for virtual machines        VLAN 30 - NAS for Personal Use          Designed for managing personal documents and media        VLAN 40 - NAS for Proxmox          Reserved for backups and ISO storage      Used by VMs to access common files      With these VLANs in place, I ensured that each type of traffic is isolated, improving both security and performance. The next step was configuring the servers to access their respective VLANs.Configuring the Physical ServersNext, I configured the Proxmox VE, Proxmox Backup Server (BS), and Orange Pi 5 Pro to use the VLANs. Interestingly, while all of these systems are based on Debian, the configuration process differed for each.Proxmox Backup Server (BS)Proxmox Backup Server was the simplest to configure since it only required a single connection to the NAS for storing backups. The VLAN setup was straightforward, as it primarily handled backup traffic without any complex routing needs.auto loiface lo inet loopback# Management network (VLAN 10, 192.168.177.0/24)auto eno1.10iface eno1.10 inet static    address 192.168.177.129    netmask 255.255.255.0    gateway 192.168.177.1    vlan-raw-device eno1  This configure the backup server to use VLAN 10Orange Pi NASSince the Orange Pi 5 Pro has two network interfaces, I had a few options for configuring the VLANs:  Option 1: Set up VLANs on one network card and dedicate the other for administrative access.  Option 2: Create a redundant network setup using both interfaces for failover and load balancing.  Option 3: Separate the VLANs across both network cards, which is the approach I chose.This setup allows for better network segmentation and isolation between traffic types. Below are the configuration files I used to implement this setup:  10-enP3p49s0.network```[Match]Name=enP3p49s0[Network]VLAN=vlan30&gt; 10-enP4p65s0.network[Match]Name=enP4p65s0[Network]VLAN=vlan40&gt; 20-vlan30.network[Match]Name=vlan30[Network]Address=192.168.198.8/24Gateway=192.168.198.1DNS=8.8.8.8 8.8.4.4&gt; 20-vlan30.netdev[NetDev]Name=vlan30Kind=vlan[VLAN]Id=30&gt; 20-vlan40.network[Match]Name=vlan40[Network]Address=192.168.197.7/24Gateway=192.168.197.1DNS=8.8.8.8 8.8.4.4&gt; 20-vlan40.netdev[NetDev]Name=vlan40Kind=vlan[VLAN]Id=40### Proxmox VESimilar to the Orange Pi 5 setup, the Proxmox VE server also has two network cards. I decided to separate the VLANs, assigning each VLAN to its own network interface for better isolation and performance. Below is the configuration file for this setup:auto loiface lo inet loopbackauto enp11s0iface enp11s0 inet manualauto vmbr0iface vmbr0 inet static        bridge-ports enp11s0        bridge-stp off        bridge-fd 0        bridge-vlan-aware yes        bridge-vids 2-4094auto vmbr0.10iface vmbr0.10 inet static\taddress 192.168.177.7/24\tgateway 192.168.177.1auto ens5iface ens5 inet manualauto vmbr1iface vmbr1 inet static        bridge-ports ens5        bridge-stp off        bridge-fd 0        bridge-vlan-aware yes        bridge-vids 2-4094auto vmbr1.20iface vmbr1.20 inet static        address 192.168.178.8/24        gateway 192.168.178.1```Now that the Samba server and network setup is complete, in the next post, I‚Äôll be covering the firewall rules that control access to the NAS. These rules will ensure that each system can access the NAS as needed while maintaining proper security and isolation."
  },
  
  {
    "title": "Proxmox Network Storages",
    "url": "/posts/Proxmox-Network-Storage/",
    "categories": "Proxmox, Samba",
    "tags": "proxmox, NAS, storage",
    "date": "2024-09-15 19:11:00 -0600",
    





    
    "snippet": "When I imagine the future of my home lab, I envision it filled with professional-grade equipment‚Äîa sleek server rack, servers humming away, and network switches blinking behind a perforated door. H...",
    "content": "When I imagine the future of my home lab, I envision it filled with professional-grade equipment‚Äîa sleek server rack, servers humming away, and network switches blinking behind a perforated door. However, like many enthusiasts, my current setup is a bit more modest, made up of a mix of end-of-life workstations and desktops.But that‚Äôs part of the fun, right? Taking bits and pieces of hardware and transforming them into something practical and useful. That‚Äôs the beauty of a home lab.In this article series, I‚Äôll walk you through how I built a Network Attached Storage (NAS) solution using a DAC (Direct Access Storage) and a Single Board Computer (SBC). While this may not be the most conventional approach, it was certainly a fun and rewarding experience!The SBC NASAt its core, a NAS (Network Attached Storage) is simply a device that shares storage across a network. For my setup, I had an Orange Pi 5 Pro that I‚Äôd been experimenting with, and since it features two network interfaces, it was a perfect candidate for the NAS device to connect to the DAC.One network interface would serve Proxmox, acting as the NAS for my virtual environment, while the other would be dedicated to my personal media storage. I set up the Orange Pi with a headless Debian server and connected it to the DAC, using Samba to share the storage across the network.And just like that, I had something that was almost a fully functional NAS!To Bridge or Not to BridgeThe Orange Pi 5 Pro comes with two network interfaces, and when I first envisioned this project, I considered bridging them. However, bridging the two would essentially connect two subnets that are normally separated by a firewall, bypassing that isolation.This led to an important question: did I really want to bridge these two networks on a Debian SBC? After weighing the pros and cons, I had to carefully consider whether merging these networks would compromise the structure and security of my home lab.Security Concerns with the BridgeAlthough my home lab isn‚Äôt a production-level environment, I try to approach it with the same security considerations. By bridging the networks, I realized a couple of important access concerns:  From a Proxmox and personal device perspective, bridging effectively gave both access to the NAS.  From a NAS perspective, it now had access not only to the Proxmox server and VMs running on it, but also to any personal devices I use to access media.This raised a significant question: was I compromising security by bridging these networks?In my next post, I‚Äôll walk through the steps I took to secure access to the NAS and maintain proper isolation.  /etc/samba/smb.conf  ```[global]    workgroup = sebos    log file = /var/log/samba/smb.log    max log size = 10000    log level = 1    server string = sebos nas %v    security = user    min protocol = SMB2# Include additional configuration files [media]include = /etc/samba/smb.d/media.conf [pve]include = /etc/samba/smb.d/pve.conf&gt; /etc/samba/smb.d/media.conf&gt;comment = home media serverpath = /srv/nas_storage/mediabrowseable = yeswritable = yesguest ok = nocreate mask = 0664directory mask = 0775force user = samba_mediaforce group = samba_admin_mediavalid users = samba_media ```  /etc/samba/smb.d/pve.conf      comment = pve storage    path = /srv/nas_storage/pve    browseable = yes    writable = yes    guest ok = no    create mask = 0664    directory mask = 0775    force user = samba_pve    force group = samba_admin_pve    valid users = samba_pve  "
  },
  
  {
    "title": "Segregation SSH Traffic",
    "url": "/posts/Segregation-SSH-Traffic/",
    "categories": "ssh, cybersecurity",
    "tags": "ssh traffic, segregation, cybersecurity",
    "date": "2024-09-08 14:48:00 -0600",
    





    
    "snippet": "Segregation SSH TrafficControlling access to SSH services is crucial for maintaining a secure environment, and the ability to search logs for troubleshooting and security concerns is just as critic...",
    "content": "Segregation SSH TrafficControlling access to SSH services is crucial for maintaining a secure environment, and the ability to search logs for troubleshooting and security concerns is just as critical.Many tools, like Ansible, rsync, Terraform, and Docker, generate SSH traffic as part of their operations. Segregating this traffic by using different SSH ports for various services can simplify managing:  firewall rules,  monitoring,  and logging.Let‚Äôs explore what it takes to move Ansible‚Äôs SSH traffic to a dedicated port, ensuring smoother and more secure operations.SSH Match StatementTo segregate SSH traffic, we need to use conditional login rules in the SSH configuration file. The Match directive allows you to apply specific settings based on various conditions.You can use the Match command to define settings based on:  Match User: The SSH user‚Äôs username.  Match Group: The SSH user‚Äôs group.  Match Address: The client‚Äôs IP address or subnet.  Match Host: The client‚Äôs hostname (resolved by DNS).  Match LocalAddress: The server‚Äôs local IP address.  Match LocalPort: The port the SSH server is listening on.  Match RemoteAddress: The client‚Äôs remote IP address.  Match RemotePort: The client‚Äôs remote port.In my case, I wanted to split the traffic, so regular SSH user traffic would be on port 22, while Ansible traffic would run on port 2222. To achieve this, I used the Match LocalPort directive.Splitting the SSH Config FileTo better manage and segregate SSH traffic, I split the SSH configuration into three parts:  The main sshd_config file, which contains the common directives shared across both configurations (example provided below).  A separate configuration for regular user traffic, containing additional directives specific to user connections.  A dedicated configuration for Ansible traffic, also with its own set of directives.One key difference between these configurations could be logging. For example, you might set low logging levels for Ansible to generate fewer log entries, while keeping high logging levels for user traffic to enable easier debugging in case of issues.Splitting the SSH Config FileTo better manage and segregate SSH traffic, I split the SSH configuration into three parts:  The main sshd_config file, which contains the common directives shared across both configurations (example provided below).  A separate configuration for regular user traffic, containing additional directives specific to user connections.  A dedicated configuration for Ansible traffic, also with its own set of directives.One key difference between these configurations could be logging. For example, you might set low logging levels for Ansible to generate fewer log entries, while keeping high logging levels for user traffic to enable easier debugging in case of issues.Ansible TrafficI moved the Ansible traffic to port 2222 using the Match LocalPort 2222 directive and configured it as follows:  Granted access only to specific ansible users connecting from the Ansible controller‚Äôs IP address.  Set logging to capture errors only, reducing unnecessary log entries.  Decreased the time between keep-alive checks to ensure consistent connectivity.  Increased the number of concurrent SSH sessions that can be opened at once, optimizing Ansible‚Äôs performance.Next, we‚Äôll take a look at how the configuration differs for regular users.Traffic on Port 22To handle the rest of the traffic, I configured a Match LocalPort 22 directive with the following settings:  Allowed connections only for the ssh-user group, which contains the list of users permitted to access via SSH.  Increased the time between keep-alive checks while reducing the number of unanswered keep-alives allowed.  Set the logging level to INFO for more detailed logging of user activity.  Limited the number of concurrent connections per user to enhance security.By segregating SSH traffic using different ports and custom configurations, you can gain greater control over your server environment. Whether it‚Äôs fine-tuning logging, managing connection limits, or tailoring firewall rules, this approach ensures that your critical tools like Ansible, alongside regular user traffic, run smoothly and securely. Taking the time to implement this type of SSH traffic management can improve both performance and security, while also simplifying troubleshooting and monitoring.As you continue to scale your infrastructure, applying these strategies will help you maintain a more organized and secure system, tailored to meet your specific needs.I‚Äôd love to hear how do you segregation SSH Traffic?Below are the SSH config files  SSH Config File:Ansible Traffic (51-ansible_admin.conf)```Match LocalPort 2222## Limit SSH Access to Specific UsersAllowUsers ansible_admin@192.168.167.17DenyGroups ssh-users## Mitigate hanging or idle connectionsClientAliveInterval 60ClientAliveCountMax 3## Log LevelLogLevel ERROR## Limit Concurrent Sessions increased for AnsibleMaxSessions 10&gt;SSH Config File:User Traffic (55-ssh-user.conf)Match LocalPort 22## Only users in the ssh-users groupAllowGroups ssh-users# Mitigate hanging or idle connectionsClientAliveInterval 300ClientAliveCountMax 0## Log LevelLogLevel INFO## Limit Concurrent SessionsMaxSessions 10 ``` &gt; SSH Config File:sshd_config ``` ## Strong SSH Key Algorithms KexAlgorithms curve25519-sha256@libssh.org,diffie-hellman-group-exchange-sha256 Ciphers aes256-ctr,aes192-ctr,aes128-ctr MACs hmac-sha2-512,hmac-sha2-256PubkeyAcceptedKeyTypes ssh-rsa-cert-v01@openssh.com,ssh-ed25519Disable ssh protocol 1Protocol 2Port 22Port 2222Restrict RootPermitRootLogin noRestrict Auth Key locationAuthorizedKeysFile /home/%u/.ssh/authorized_keysDisable Password LoginsPasswordAuthentication noPermitEmptyPasswords noDisable Unused Authentication MethodsGSSAPIAuthentication noChallengeResponseAuthentication noConnection settingMaxAuthTries 3LoginGraceTime 30sDisable SSH Tunneling and ForwardingAllowAgentForwarding noPermitTunnel noX11Forwarding noAdd Port base configInclude /etc/ssh/sshd_config.d/51-ansible_admin.confInclude /etc/ssh/sshd_config.d/55-ssh-user.conf```"
  },
  
  {
    "title": "SSH Through Firewall",
    "url": "/posts/SSH-Through-Firewall/",
    "categories": "ProxoxVE, Cybersecurity",
    "tags": "SSH, Security, Proxy Jump, Network, Hardening, Best Practices",
    "date": "2024-08-25 09:40:00 -0600",
    





    
    "snippet": "Proxmox Security Series:SSH Through FirewallIn my effort to find fresh ways to improve SSH security beyond the usual tips and tricks, I looked into using the OPNsense firewall‚Äîsomething I already h...",
    "content": "Proxmox Security Series:SSH Through FirewallIn my effort to find fresh ways to improve SSH security beyond the usual tips and tricks, I looked into using the OPNsense firewall‚Äîsomething I already had‚Äîas a gateway for my Proxmox server. The idea was to make this firewall the main entry point for server access. This setup simplifies access but also means that if the firewall has issues, everything does. Although it was an interesting idea, I was initially unsure about how much it would really enhance security. Stay tuned as I dive deeper into this later in the article.SSH Authentication KeysPrior to implementing the firewall adjustments, I generated two SSH keys: one for the connection to the firewall and another for the Proxmox server.## Auth Keys for Proxmox Serverssh-keygen -t ed25519 -f ~/.ssh/pve  ## Auth Keys for OPNsense Firewallssh-keygen -t ed25519 -f ~/.ssh/firewallI admit that using two different keys might seem a bit much, and really, one key is usually enough. But I was curious‚Äîcould using two separate keys actually make things more secure? I saw this as a chance to test out that idea.Firewall Modifications for SSHConfiguring the firewall for SSH was straightforward and could be managed entirely through the web interface. Keep in mind that different firewalls offer varying features, so the following is a high-level description of the adjustments I made:  SSH was enabled on the LAN interface exclusively.  Root user login was disabled to enhance security.  Password-based login was disabled, requiring more secure authentication methods.  A new user account was created, which required administrative privileges for SSH access.  I selected the preferred shell for the user.  The SSH authentication key was uploaded to ensure secure access.These changes successfully enabled SSH access to the firewall, setting the stage for more secure operations.SSH Jump CommandSSH includes a useful -J switch, which allows routing through a jump server.## SSH Jump Commandssh -J &lt;firewall_user_id&gt;@&lt;firewall_ip_address&gt; &lt;pve_user_id&gt;@&lt;pve_ip_address&gt;As you begin to integrate options like authentication keys, the command can become cluttered. This is where the .ssh/config file becomes invaluable. It allows users to assign aliases to SSH servers and specify options for each connection. For environments with multiple servers, the file can become complex. Using include files within the .ssh/config can help manage this complexity, keeping configurations organized and maintainable.Configuring SSH with .ssh/configTo streamline SSH commands using configuration files, you can create specific config files for each server, like so:Config File Name: firewall# Configuration file for firewallHost firewall    HostName 192.168.166.1    User sebos    IdentityFile ~/.ssh/firewallConfig File Name: pve# Configuration file for Proxmox VEInclude ~/.ssh/include.d/firewallHost pve    HostName 192.168.167.127    User richard    ProxyJump firewall    IdentityFile ~/.ssh/pveChanges to: ~/.ssh/configInclude ~/.ssh/include.d/firewallInclude ~/.ssh/include.d/pveWith these configurations in place, connecting to your Proxmox server via the firewall jump host simplifies to a single command:ssh pve  Note:  Storing additional configuration files in the ~/.ssh/include.d/ directory isn‚Äôt mandatory but helps maintain organization and clarity in your SSH setup.Did It Enhance Security?My assessment is affirmative. The setup resulted in some interesting outcomes:  The Proxmox server can no longer be directly accessed via SSH without routing through the jump server‚Äîa predicted but significant tightening of security.  Surprisingly, even from the firewall‚Äôs shell, I was unable to initiate an SSH connection to the Proxmox server.To successfully access the Proxmox server through SSH, the following conditions must be met:  Physical presence within my LAN network.  Possession of both sets of my SSH keys.  Knowledge of the user IDs and server IP addresses, though the .ssh/config file simplifies this aspect.Would I extend this setup to my WiFi or WAN network? Potentially, yes‚Äîif I stored my configuration files and authentication keys within an encrypted directory secured by a password, I would consider it feasible and safe to do so.Here‚Äôs a simpler version for a general audience:Would I create two authentication key files again in the future? Yes, but only if I planned to store them in different places, like one on my local machine and the other on a network share within my own network.Can This Stop All Hackers?No security system is perfect. Highly skilled hackers can find vulnerabilities, including Zero Day exploits, in any system. The best strategy is to minimize potential entry points and add layers of security to discourage most attackers.I‚Äôd love to hear about your methods. How do you secure SSH in your network?"
  },
  
  {
    "title": "Securing Root Access",
    "url": "/posts/Securing-Root-Access/",
    "categories": "ProxoxVE, Cybersecurity",
    "tags": "proxmox, vm, servers, security",
    "date": "2024-08-17 18:35:00 -0600",
    





    
    "snippet": "Continuing our series on securing your Proxmox server, this article focuses on an essential aspect: securing root access. Like many other systems, Proxmox requires an administrator account to perfo...",
    "content": "Continuing our series on securing your Proxmox server, this article focuses on an essential aspect: securing root access. Like many other systems, Proxmox requires an administrator account to perform critical operations, and it uses the Linux root account by default. However, leaving this root access open presents significant security vulnerabilities, such as:  Unrestricted access to the web-based root console  Direct root access via SSHThe good news is that these vulnerabilities are easy to address. Even after implementing these security measures, you‚Äôll still retain the necessary root access to manage your Proxmox environment securely.Install sudoBefore disabling root access, it‚Äôs crucial to create a new administrative user with restricted privileges. To manage administrative tasks without using the root account, we‚Äôll need to install sudo on the Proxmox server. You can do this by accessing the server either through the web console as root or by connecting via SSH. First, update the server‚Äôs packages, then install sudo using the following commands:# Update the server firstapt update &amp;&amp; apt upgrade -y# Install sudoapt install sudoWith sudo installed, we can create an administrative account.Create a New System UserCreating a new user on Linux is straightforward using the adduser command. This user will differ from the standard user account (like ‚Äúbob‚Äù we created in a previous article) as it will have administrative privileges rather than just access to the Proxmox web interface. In this example, we‚Äôll create a new user named ‚ÄúRichard‚Äù to replace the root user for administrative tasks.Steps to Create a New System User      Create the user account:Use the adduser command to create a new user with a specified home directory.    # Create a new user with a home directoryadduser --home /home/richard richard            Create a new group for system administrators:If you don‚Äôt already have a group for administrative users, you can create one. This group can be used to manage permissions more efficiently.    # Create a system admin groupaddgroup system-admin            Grant sudo and group access:Add the new user to the newly created system-admin group.    # Add the new user to the system admin groupgpasswd --add richard system-admin      Next, we will create a custom sudoers file to further secure the server and fine-tune the permissions for our new administrative user.Configure the sudoers File for RichardThe sudoers file allows you to specify which commands a sudo user can execute, providing granular control over administrative permissions. You can create user aliases or assign permissions directly to existing system accounts. Since I often reuse my sudoers files across multiple servers, I prefer to use aliases for better management and scalability.In this section, we‚Äôll create a list of command aliases that define what actions Richard can perform and assign these command aliases to his user group.Steps to Configure the sudoers File      Define user aliases: First, create a user alias for the system-admin group. This makes it easy to manage permissions for all users in this group.    # Sudoers file for adminUser_Alias SYSTEM_ADMIN = %system-admin            Create command aliases: Define command aliases for specific tasks Richard and other system administrators need to perform. This restricts the commands they can run with sudo and enhances security.    # Allow system updatesCmnd_Alias UPDATE_CMDS = /usr/bin/apt update, /usr/bin/apt upgrade# Check error logsCmnd_Alias LOG_CMDS = /usr/bin/tail -f /var/log/*, /usr/bin/journalctl, /bin/grep sshd /var/log/auth.log# Restricted Vim (rvim) to edit files without dropping to a shellCmnd_Alias RVIM = /usr/bin/rvim            Assign command aliases to the user group: Finally, assign these command aliases to the SYSTEM_ADMIN user alias. This allows members of the system_admin group to run the specified commands without a password prompt.    SYSTEM_ADMIN ALL=(ALL) NOPASSWD: UPDATE_CMDS, LOG_CMDS, RVIM      Root access can be restricted now that we have an additional SSH account.Restricting Root Access from SSHCreating an additional user account on the server allows us to restrict root access through SSH while still maintaining SSH access for administrators. To enforce this, we‚Äôll use the ssh-users group to control who can access the server via SSH. While this might seem like overkill for a Proxmox server with just one user-level account, it‚Äôs a best practice for securing all servers that use SSH.Steps to Restrict Root SSH Access      Create an SSH access group: First, create a new group named ssh-users and add the new administrative user, Richard, to this group.    # Create an SSH access groupaddgroup ssh-users# Add the new user to the SSH access groupgpasswd --add richard ssh-users            Edit the SSH configuration file: Next, edit the SSH daemon configuration file (sshd_config) to disable root login and allow only users in the ssh-users group to connect via SSH.    # Disable root loginPermitRootLogin no# Allow only specific groups to access SSHAllowGroups ssh-users        Note: If neither AllowGroups nor AllowUsers is specified, any new user could potentially access the server through SSH. Defining these parameters helps to restrict access to only those users explicitly allowed.      Restart the SSH service: To apply these changes, restart the SSH service.    # Restart SSH servicesystemctl restart sshd        Note:  In a future article, we will cover additional steps to further &gt;secure the SSH daemon beyond just restricting root access.With these SSH security measures in place, we‚Äôve significantly reduced the attack surface by restricting root access and controlling who can access the server. Next, we will focus on restricting access to the Proxmox web shell.Add System Admin to ProxmoxThe richard system account we created earlier does not automatically appear in the Proxmox web interface. To grant the richard account administrative access, we need to manually add it as an Administrator within Proxmox.  Add the richard user to Proxmox  Add administrator the richardBy doing this, Richard will have the necessary permissions to manage all aspects of the Proxmox environment, allowing us to disable the root account safely. Once root is disabled, the server console will require a password for login instead of automatically providing root access.  password is needed on node web consoleBenefits of Replacing Root with RichardBy configuring Richard as the sole user with SSH access and limiting the commands executable with sudo, we‚Äôve effectively secured SSH access. As Richard has Administrator privileges in Proxmox, any tasks that cannot be performed through SSH can still be handled through the Proxmox web interface.Additionally, if the web interface is ever unavailable, you can still access the server directly with physical access. This setup ensures robust security while maintaining the flexibility needed to manage the Proxmox environment effectively."
  },
  
  {
    "title": "Strengthening Your Virtual Environment",
    "url": "/posts/Strengthening-Your-Virtual/",
    "categories": "ProxoxVE, Cybersecurity",
    "tags": "proxmox, vm, servers, security",
    "date": "2024-08-17 18:27:00 -0600",
    





    
    "snippet": "Proxmox:Baby steps to Increase SecurityProxmox is an open-source virtual environment tool for creating and managing virtual machines and containers. Currently a niche solution in a growing market, ...",
    "content": "Proxmox:Baby steps to Increase SecurityProxmox is an open-source virtual environment tool for creating and managing virtual machines and containers. Currently a niche solution in a growing market, Proxmox is  used  by companies and educational institutions as an alternative to VMware ESXi, Proxmox offers robust features at a lower to no cost.  However, since it is built on a Linux system, additional security measures are necessary to protect your environment.One crucial aspect of securing Proxmox is controlling who has access and what actions they can perform, such as starting, stopping, deleting, or modifying virtual machines. In this series of articles, I will guide you through a series of steps to add layers of security to your Proxmox server.We‚Äôll start with the simpler tasks, gradually moving to more advanced security configurations. In this first article, we‚Äôll focus on creating and managing user permissions within Proxmox to ensure that only authorized personnel can access and control your virtual environment.Proxmox UsersTo get started, we‚Äôll create a few users based on their specific needs. These users will be set up with Proxmox VE authentication server logins:      Bob:Bob is the Proxmox Administrator for multiple teams. He is responsible for creating, maintaining, and backing up virtual servers, as well as managing the Proxmox server itself.        Betty:Betty works on external-facing web pages. She needs to monitor the status of the servers and occasionally use the web console for her tasks.        Jim:Jim focuses on internal-facing web pages. Like Betty, he needs to check the status of the servers and occasionally access the web console.        Nancy:Nancy is the Team Manager for the Web Programmers. Betty and Jim report to her when there are issues with the virtual servers. While she has the ability to reboot the VMs, she contacts Bob if more significant action is required.    Note: The permissions discussed in this article are for  accessing Proxmox, not the operating systems of the virtual machines themselves.Proxmox PoolsOne of the powerful features in Proxmox is the ability to organize VMs into Pools, essentially creating groups of VMs based on their function or purpose. In our example, we‚Äôll create two sets of VMs:      ExtWebSer:This pool will contain external web servers.        IntWebSer:This pool will contain internal web servers.  In this setup,      Nancy and Betty::Will be assigned to the ExtWebSer pool        Nancy and JIM::Will be assigned to the IntWebSer pool  Proxmox Privileges and RolesProxmox uses roles to assign a set of predefined privileges to users or groups. These roles are  fixed but can be customized by creating additional roles based on the available privileges. We‚Äôll apply these roles to our users based on their specific needs.      Bob:Bob is assigned the Administrator role, which is a predefined role with comprehensive privileges.        Betty and Jim:Betty and Jim are assigned a new role called WebDev, which includes the privileges Mapping.Audit VM.Audit, VM.Console, and **VM.Monitor. This role is tailored to their needs for monitoring and occasional access.        Nancy:Nancy is assigned a new role called WebAppManager, which includes the privileges Mapping.Audit, VM.Audit, VM.Monitor, and VM.PowerMgmt. This role allows her to oversee the web application servers and perform basic management tasks like rebooting VMs.    Note:  Note: A complete listing of Proxmox privileges can be found here.To ensure that users with the WVWebDev and WebAppManager roles can view these pools in Proxmox, the Pool.Audit permission must be added to their roles.Proxmox PermissionsProxmox pools created the assocations between user, roles and resources, in our case, pools. Premission defind what access a users has to what resource.  Bob:  Has access to / (all)  as Adminstrator  Betty:  Has access to /pool/ExtWebSer as WVWebDev  Jim:  Has access to /pool/IntWebSer as WVWebDev  Nancy:  Has access to /pool/ExtWebSer as WebAppManager  Has access to /pool/IntWebSer as WebAppManagerWhy Do This?In a larger organization, there could be tens or even hundreds of VMs. The teams managing these VMs, along with the critical applications running on them, need appropriate access to perform their duties. By restricting access based on roles and pools, you can secure critical servers, such as those used for HR, strategic planning, and accounting, as well as others ensuring that only authorized personnel have access.What the users see      Betty         Jim         Nancy  In the next article, we will explore how to restrict root access on the Proxmox server itself, adding another layer of security to your environment.I hope you found this article valuable, and I appreciate the time you took to read it. If you have any questions or suggestions, please feel free to reach out. When it comes to securing a virtual host, what steps would you consider taking?"
  }
  
]

